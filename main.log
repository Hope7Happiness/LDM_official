Global seed set to 23
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:2046: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead
  from torch.distributed._sharded_tensor import pre_load_state_dict_hook, state_dict_hook
/srv/mingyang/zhh_ldm_official/ldm/models/autoencoder.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/loggers/test_tube.py:105: LightningDeprecationWarning: The TestTubeLogger is deprecated since v1.5 and will be removed in v1.7. We recommend switching to the `pytorch_lightning.loggers.TensorBoardLogger` as an alternative.
  rank_zero_deprecation(
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:292: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  rank_zero_deprecation(
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:91: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
  rank_zero_warn(
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Global seed set to 23
Global seed set to 23
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:2046: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead
  from torch.distributed._sharded_tensor import pre_load_state_dict_hook, state_dict_hook
/srv/mingyang/zhh_ldm_official/ldm/models/autoencoder.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
Global seed set to 23
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/7
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:2046: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead
  from torch.distributed._sharded_tensor import pre_load_state_dict_hook, state_dict_hook
/srv/mingyang/zhh_ldm_official/ldm/models/autoencoder.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
Global seed set to 23
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/7
Global seed set to 23
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:2046: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead
  from torch.distributed._sharded_tensor import pre_load_state_dict_hook, state_dict_hook
/srv/mingyang/zhh_ldm_official/ldm/models/autoencoder.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
Global seed set to 23
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/7
Global seed set to 23
Global seed set to 23
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:2046: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead
  from torch.distributed._sharded_tensor import pre_load_state_dict_hook, state_dict_hook
/srv/mingyang/zhh_ldm_official/ldm/models/autoencoder.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
Global seed set to 23
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/7
Global seed set to 23
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:2046: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead
  from torch.distributed._sharded_tensor import pre_load_state_dict_hook, state_dict_hook
/srv/mingyang/zhh_ldm_official/ldm/models/autoencoder.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
Global seed set to 23
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/7
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:326: LightningDeprecationWarning: Base `LightningModule.on_train_batch_start` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.
  rank_zero_deprecation(
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.
  rank_zero_deprecation(
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_start` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.
  rank_zero_deprecation(
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:342: LightningDeprecationWarning: Base `Callback.on_train_batch_end` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.
  rank_zero_deprecation(
Global seed set to 23
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/7
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:2046: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead
  from torch.distributed._sharded_tensor import pre_load_state_dict_hook, state_dict_hook
/srv/mingyang/zhh_ldm_official/ldm/models/autoencoder.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
Global seed set to 23
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/7
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 7 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [1,2,3,4,5,6,7]

  | Name              | Type             | Params
-------------------------------------------------------
0 | model             | DiffusionWrapper | 274 M 
1 | model_ema         | LitEma           | 0     
2 | first_stage_model | VQModelInterface | 55.3 M
-------------------------------------------------------
274 M     Trainable params
55.3 M    Non-trainable params
329 M     Total params
1,317.516 Total estimated model params size (MB)
Running on GPUs 0,1,2,3,4,5,6
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 274.06 M params.
Keeping EMAs of 370.
making attention of type 'vanilla' with 512 in_channels
Working with z of shape (1, 3, 64, 64) = 12288 dimensions.
making attention of type 'vanilla' with 512 in_channels
Restored from models/first_stage_models/vq-f4/model.ckpt with 0 missing and 55 unexpected keys (['loss.perceptual_loss.scaling_layer.shift', 'loss.perceptual_loss.scaling_layer.scale', 'loss.perceptual_loss.net.slice1.0.weight', 'loss.perceptual_loss.net.slice1.0.bias', 'loss.perceptual_loss.net.slice1.2.weight', 'loss.perceptual_loss.net.slice1.2.bias', 'loss.perceptual_loss.net.slice2.5.weight', 'loss.perceptual_loss.net.slice2.5.bias', 'loss.perceptual_loss.net.slice2.7.weight', 'loss.perceptual_loss.net.slice2.7.bias', 'loss.perceptual_loss.net.slice3.10.weight', 'loss.perceptual_loss.net.slice3.10.bias', 'loss.perceptual_loss.net.slice3.12.weight', 'loss.perceptual_loss.net.slice3.12.bias', 'loss.perceptual_loss.net.slice3.14.weight', 'loss.perceptual_loss.net.slice3.14.bias', 'loss.perceptual_loss.net.slice4.17.weight', 'loss.perceptual_loss.net.slice4.17.bias', 'loss.perceptual_loss.net.slice4.19.weight', 'loss.perceptual_loss.net.slice4.19.bias', 'loss.perceptual_loss.net.slice4.21.weight', 'loss.perceptual_loss.net.slice4.21.bias', 'loss.perceptual_loss.net.slice5.24.weight', 'loss.perceptual_loss.net.slice5.24.bias', 'loss.perceptual_loss.net.slice5.26.weight', 'loss.perceptual_loss.net.slice5.26.bias', 'loss.perceptual_loss.net.slice5.28.weight', 'loss.perceptual_loss.net.slice5.28.bias', 'loss.perceptual_loss.lin0.model.1.weight', 'loss.perceptual_loss.lin1.model.1.weight', 'loss.perceptual_loss.lin2.model.1.weight', 'loss.perceptual_loss.lin3.model.1.weight', 'loss.perceptual_loss.lin4.model.1.weight', 'loss.discriminator.main.0.weight', 'loss.discriminator.main.0.bias', 'loss.discriminator.main.2.weight', 'loss.discriminator.main.3.weight', 'loss.discriminator.main.3.bias', 'loss.discriminator.main.3.running_mean', 'loss.discriminator.main.3.running_var', 'loss.discriminator.main.3.num_batches_tracked', 'loss.discriminator.main.5.weight', 'loss.discriminator.main.6.weight', 'loss.discriminator.main.6.bias', 'loss.discriminator.main.6.running_mean', 'loss.discriminator.main.6.running_var', 'loss.discriminator.main.6.num_batches_tracked', 'loss.discriminator.main.8.weight', 'loss.discriminator.main.9.weight', 'loss.discriminator.main.9.bias', 'loss.discriminator.main.9.running_mean', 'loss.discriminator.main.9.running_var', 'loss.discriminator.main.9.num_batches_tracked', 'loss.discriminator.main.11.weight', 'loss.discriminator.main.11.bias'])
Training LatentDiffusion as an unconditional model.
Monitoring val/loss_simple_ema as checkpoint metric.
Merged modelckpt-cfg: 
{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2025-03-26T05-14-20_ffhq-ldm-vq-4/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
#### Data #####
train, FFHQTrain, 60000
validation, FFHQValidation, 10000
accumulate_grad_batches = 1
Setting learning rate to 8.40e-05 = 1 (accumulate_grad_batches) * 7 (num_gpus) * 42 (batchsize) * 2.00e-06 (base_lr)
Project config
model:
  base_learning_rate: 2.0e-06
  target: ldm.models.diffusion.ddpm.LatentDiffusion
  params:
    linear_start: 0.0015
    linear_end: 0.0195
    num_timesteps_cond: 1
    log_every_t: 200
    timesteps: 1000
    first_stage_key: image
    image_size: 64
    channels: 3
    monitor: val/loss_simple_ema
    unet_config:
      target: ldm.modules.diffusionmodules.openaimodel.UNetModel
      params:
        image_size: 64
        in_channels: 3
        out_channels: 3
        model_channels: 224
        attention_resolutions:
        - 8
        - 4
        - 2
        num_res_blocks: 2
        channel_mult:
        - 1
        - 2
        - 3
        - 4
        num_head_channels: 32
    first_stage_config:
      target: ldm.models.autoencoder.VQModelInterface
      params:
        embed_dim: 3
        n_embed: 8192
        ckpt_path: models/first_stage_models/vq-f4/model.ckpt
        ddconfig:
          double_z: false
          z_channels: 3
          resolution: 256
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult:
          - 1
          - 2
          - 4
          num_res_blocks: 2
          attn_resolutions: []
          dropout: 0.0
        lossconfig:
          target: torch.nn.Identity
    cond_stage_config: __is_unconditional__
data:
  target: main.DataModuleFromConfig
  params:
    batch_size: 42
    num_workers: 5
    wrap: false
    train:
      target: taming.data.faceshq.FFHQTrain
      params:
        size: 256
    validation:
      target: taming.data.faceshq.FFHQValidation
      params:
        size: 256

Lightning config
callbacks:
  image_logger:
    target: main.ImageLogger
    params:
      batch_frequency: 5000
      max_images: 8
      increase_log_steps: false
trainer:
  benchmark: true
  accelerator: ddp
  gpus: 0,1,2,3,4,5,6

Sanity Checking: 0it [00:00, ?it/s]Running on GPUs 0,1,2,3,4,5,6
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 274.06 M params.
Keeping EMAs of 370.
making attention of type 'vanilla' with 512 in_channels
Working with z of shape (1, 3, 64, 64) = 12288 dimensions.
making attention of type 'vanilla' with 512 in_channels
Restored from models/first_stage_models/vq-f4/model.ckpt with 0 missing and 55 unexpected keys (['loss.perceptual_loss.scaling_layer.shift', 'loss.perceptual_loss.scaling_layer.scale', 'loss.perceptual_loss.net.slice1.0.weight', 'loss.perceptual_loss.net.slice1.0.bias', 'loss.perceptual_loss.net.slice1.2.weight', 'loss.perceptual_loss.net.slice1.2.bias', 'loss.perceptual_loss.net.slice2.5.weight', 'loss.perceptual_loss.net.slice2.5.bias', 'loss.perceptual_loss.net.slice2.7.weight', 'loss.perceptual_loss.net.slice2.7.bias', 'loss.perceptual_loss.net.slice3.10.weight', 'loss.perceptual_loss.net.slice3.10.bias', 'loss.perceptual_loss.net.slice3.12.weight', 'loss.perceptual_loss.net.slice3.12.bias', 'loss.perceptual_loss.net.slice3.14.weight', 'loss.perceptual_loss.net.slice3.14.bias', 'loss.perceptual_loss.net.slice4.17.weight', 'loss.perceptual_loss.net.slice4.17.bias', 'loss.perceptual_loss.net.slice4.19.weight', 'loss.perceptual_loss.net.slice4.19.bias', 'loss.perceptual_loss.net.slice4.21.weight', 'loss.perceptual_loss.net.slice4.21.bias', 'loss.perceptual_loss.net.slice5.24.weight', 'loss.perceptual_loss.net.slice5.24.bias', 'loss.perceptual_loss.net.slice5.26.weight', 'loss.perceptual_loss.net.slice5.26.bias', 'loss.perceptual_loss.net.slice5.28.weight', 'loss.perceptual_loss.net.slice5.28.bias', 'loss.perceptual_loss.lin0.model.1.weight', 'loss.perceptual_loss.lin1.model.1.weight', 'loss.perceptual_loss.lin2.model.1.weight', 'loss.perceptual_loss.lin3.model.1.weight', 'loss.perceptual_loss.lin4.model.1.weight', 'loss.discriminator.main.0.weight', 'loss.discriminator.main.0.bias', 'loss.discriminator.main.2.weight', 'loss.discriminator.main.3.weight', 'loss.discriminator.main.3.bias', 'loss.discriminator.main.3.running_mean', 'loss.discriminator.main.3.running_var', 'loss.discriminator.main.3.num_batches_tracked', 'loss.discriminator.main.5.weight', 'loss.discriminator.main.6.weight', 'loss.discriminator.main.6.bias', 'loss.discriminator.main.6.running_mean', 'loss.discriminator.main.6.running_var', 'loss.discriminator.main.6.num_batches_tracked', 'loss.discriminator.main.8.weight', 'loss.discriminator.main.9.weight', 'loss.discriminator.main.9.bias', 'loss.discriminator.main.9.running_mean', 'loss.discriminator.main.9.running_var', 'loss.discriminator.main.9.num_batches_tracked', 'loss.discriminator.main.11.weight', 'loss.discriminator.main.11.bias'])
Training LatentDiffusion as an unconditional model.
Monitoring val/loss_simple_ema as checkpoint metric.
Merged modelckpt-cfg: 
{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2025-03-26T05-14-38_ffhq-ldm-vq-4/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
#### Data #####
train, FFHQTrain, 60000
validation, FFHQValidation, 10000
accumulate_grad_batches = 1
Setting learning rate to 8.40e-05 = 1 (accumulate_grad_batches) * 7 (num_gpus) * 42 (batchsize) * 2.00e-06 (base_lr)
Running on GPUs 0,1,2,3,4,5,6
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 274.06 M params.
Keeping EMAs of 370.
making attention of type 'vanilla' with 512 in_channels
Working with z of shape (1, 3, 64, 64) = 12288 dimensions.
making attention of type 'vanilla' with 512 in_channels
Restored from models/first_stage_models/vq-f4/model.ckpt with 0 missing and 55 unexpected keys (['loss.perceptual_loss.scaling_layer.shift', 'loss.perceptual_loss.scaling_layer.scale', 'loss.perceptual_loss.net.slice1.0.weight', 'loss.perceptual_loss.net.slice1.0.bias', 'loss.perceptual_loss.net.slice1.2.weight', 'loss.perceptual_loss.net.slice1.2.bias', 'loss.perceptual_loss.net.slice2.5.weight', 'loss.perceptual_loss.net.slice2.5.bias', 'loss.perceptual_loss.net.slice2.7.weight', 'loss.perceptual_loss.net.slice2.7.bias', 'loss.perceptual_loss.net.slice3.10.weight', 'loss.perceptual_loss.net.slice3.10.bias', 'loss.perceptual_loss.net.slice3.12.weight', 'loss.perceptual_loss.net.slice3.12.bias', 'loss.perceptual_loss.net.slice3.14.weight', 'loss.perceptual_loss.net.slice3.14.bias', 'loss.perceptual_loss.net.slice4.17.weight', 'loss.perceptual_loss.net.slice4.17.bias', 'loss.perceptual_loss.net.slice4.19.weight', 'loss.perceptual_loss.net.slice4.19.bias', 'loss.perceptual_loss.net.slice4.21.weight', 'loss.perceptual_loss.net.slice4.21.bias', 'loss.perceptual_loss.net.slice5.24.weight', 'loss.perceptual_loss.net.slice5.24.bias', 'loss.perceptual_loss.net.slice5.26.weight', 'loss.perceptual_loss.net.slice5.26.bias', 'loss.perceptual_loss.net.slice5.28.weight', 'loss.perceptual_loss.net.slice5.28.bias', 'loss.perceptual_loss.lin0.model.1.weight', 'loss.perceptual_loss.lin1.model.1.weight', 'loss.perceptual_loss.lin2.model.1.weight', 'loss.perceptual_loss.lin3.model.1.weight', 'loss.perceptual_loss.lin4.model.1.weight', 'loss.discriminator.main.0.weight', 'loss.discriminator.main.0.bias', 'loss.discriminator.main.2.weight', 'loss.discriminator.main.3.weight', 'loss.discriminator.main.3.bias', 'loss.discriminator.main.3.running_mean', 'loss.discriminator.main.3.running_var', 'loss.discriminator.main.3.num_batches_tracked', 'loss.discriminator.main.5.weight', 'loss.discriminator.main.6.weight', 'loss.discriminator.main.6.bias', 'loss.discriminator.main.6.running_mean', 'loss.discriminator.main.6.running_var', 'loss.discriminator.main.6.num_batches_tracked', 'loss.discriminator.main.8.weight', 'loss.discriminator.main.9.weight', 'loss.discriminator.main.9.bias', 'loss.discriminator.main.9.running_mean', 'loss.discriminator.main.9.running_var', 'loss.discriminator.main.9.num_batches_tracked', 'loss.discriminator.main.11.weight', 'loss.discriminator.main.11.bias'])
Training LatentDiffusion as an unconditional model.
Monitoring val/loss_simple_ema as checkpoint metric.
Merged modelckpt-cfg: 
{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2025-03-26T05-14-42_ffhq-ldm-vq-4/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
#### Data #####
train, FFHQTrain, 60000
validation, FFHQValidation, 10000
accumulate_grad_batches = 1
Setting learning rate to 8.40e-05 = 1 (accumulate_grad_batches) * 7 (num_gpus) * 42 (batchsize) * 2.00e-06 (base_lr)
Running on GPUs 0,1,2,3,4,5,6
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 274.06 M params.
Keeping EMAs of 370.
making attention of type 'vanilla' with 512 in_channels
Working with z of shape (1, 3, 64, 64) = 12288 dimensions.
making attention of type 'vanilla' with 512 in_channels
Restored from models/first_stage_models/vq-f4/model.ckpt with 0 missing and 55 unexpected keys (['loss.perceptual_loss.scaling_layer.shift', 'loss.perceptual_loss.scaling_layer.scale', 'loss.perceptual_loss.net.slice1.0.weight', 'loss.perceptual_loss.net.slice1.0.bias', 'loss.perceptual_loss.net.slice1.2.weight', 'loss.perceptual_loss.net.slice1.2.bias', 'loss.perceptual_loss.net.slice2.5.weight', 'loss.perceptual_loss.net.slice2.5.bias', 'loss.perceptual_loss.net.slice2.7.weight', 'loss.perceptual_loss.net.slice2.7.bias', 'loss.perceptual_loss.net.slice3.10.weight', 'loss.perceptual_loss.net.slice3.10.bias', 'loss.perceptual_loss.net.slice3.12.weight', 'loss.perceptual_loss.net.slice3.12.bias', 'loss.perceptual_loss.net.slice3.14.weight', 'loss.perceptual_loss.net.slice3.14.bias', 'loss.perceptual_loss.net.slice4.17.weight', 'loss.perceptual_loss.net.slice4.17.bias', 'loss.perceptual_loss.net.slice4.19.weight', 'loss.perceptual_loss.net.slice4.19.bias', 'loss.perceptual_loss.net.slice4.21.weight', 'loss.perceptual_loss.net.slice4.21.bias', 'loss.perceptual_loss.net.slice5.24.weight', 'loss.perceptual_loss.net.slice5.24.bias', 'loss.perceptual_loss.net.slice5.26.weight', 'loss.perceptual_loss.net.slice5.26.bias', 'loss.perceptual_loss.net.slice5.28.weight', 'loss.perceptual_loss.net.slice5.28.bias', 'loss.perceptual_loss.lin0.model.1.weight', 'loss.perceptual_loss.lin1.model.1.weight', 'loss.perceptual_loss.lin2.model.1.weight', 'loss.perceptual_loss.lin3.model.1.weight', 'loss.perceptual_loss.lin4.model.1.weight', 'loss.discriminator.main.0.weight', 'loss.discriminator.main.0.bias', 'loss.discriminator.main.2.weight', 'loss.discriminator.main.3.weight', 'loss.discriminator.main.3.bias', 'loss.discriminator.main.3.running_mean', 'loss.discriminator.main.3.running_var', 'loss.discriminator.main.3.num_batches_tracked', 'loss.discriminator.main.5.weight', 'loss.discriminator.main.6.weight', 'loss.discriminator.main.6.bias', 'loss.discriminator.main.6.running_mean', 'loss.discriminator.main.6.running_var', 'loss.discriminator.main.6.num_batches_tracked', 'loss.discriminator.main.8.weight', 'loss.discriminator.main.9.weight', 'loss.discriminator.main.9.bias', 'loss.discriminator.main.9.running_mean', 'loss.discriminator.main.9.running_var', 'loss.discriminator.main.9.num_batches_tracked', 'loss.discriminator.main.11.weight', 'loss.discriminator.main.11.bias'])
Training LatentDiffusion as an unconditional model.
Monitoring val/loss_simple_ema as checkpoint metric.
Merged modelckpt-cfg: 
{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2025-03-26T05-14-34_ffhq-ldm-vq-4/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
#### Data #####
train, FFHQTrain, 60000
validation, FFHQValidation, 10000
accumulate_grad_batches = 1
Setting learning rate to 8.40e-05 = 1 (accumulate_grad_batches) * 7 (num_gpus) * 42 (batchsize) * 2.00e-06 (base_lr)
Running on GPUs 0,1,2,3,4,5,6
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 274.06 M params.
Keeping EMAs of 370.
making attention of type 'vanilla' with 512 in_channels
Working with z of shape (1, 3, 64, 64) = 12288 dimensions.
making attention of type 'vanilla' with 512 in_channels
Restored from models/first_stage_models/vq-f4/model.ckpt with 0 missing and 55 unexpected keys (['loss.perceptual_loss.scaling_layer.shift', 'loss.perceptual_loss.scaling_layer.scale', 'loss.perceptual_loss.net.slice1.0.weight', 'loss.perceptual_loss.net.slice1.0.bias', 'loss.perceptual_loss.net.slice1.2.weight', 'loss.perceptual_loss.net.slice1.2.bias', 'loss.perceptual_loss.net.slice2.5.weight', 'loss.perceptual_loss.net.slice2.5.bias', 'loss.perceptual_loss.net.slice2.7.weight', 'loss.perceptual_loss.net.slice2.7.bias', 'loss.perceptual_loss.net.slice3.10.weight', 'loss.perceptual_loss.net.slice3.10.bias', 'loss.perceptual_loss.net.slice3.12.weight', 'loss.perceptual_loss.net.slice3.12.bias', 'loss.perceptual_loss.net.slice3.14.weight', 'loss.perceptual_loss.net.slice3.14.bias', 'loss.perceptual_loss.net.slice4.17.weight', 'loss.perceptual_loss.net.slice4.17.bias', 'loss.perceptual_loss.net.slice4.19.weight', 'loss.perceptual_loss.net.slice4.19.bias', 'loss.perceptual_loss.net.slice4.21.weight', 'loss.perceptual_loss.net.slice4.21.bias', 'loss.perceptual_loss.net.slice5.24.weight', 'loss.perceptual_loss.net.slice5.24.bias', 'loss.perceptual_loss.net.slice5.26.weight', 'loss.perceptual_loss.net.slice5.26.bias', 'loss.perceptual_loss.net.slice5.28.weight', 'loss.perceptual_loss.net.slice5.28.bias', 'loss.perceptual_loss.lin0.model.1.weight', 'loss.perceptual_loss.lin1.model.1.weight', 'loss.perceptual_loss.lin2.model.1.weight', 'loss.perceptual_loss.lin3.model.1.weight', 'loss.perceptual_loss.lin4.model.1.weight', 'loss.discriminator.main.0.weight', 'loss.discriminator.main.0.bias', 'loss.discriminator.main.2.weight', 'loss.discriminator.main.3.weight', 'loss.discriminator.main.3.bias', 'loss.discriminator.main.3.running_mean', 'loss.discriminator.main.3.running_var', 'loss.discriminator.main.3.num_batches_tracked', 'loss.discriminator.main.5.weight', 'loss.discriminator.main.6.weight', 'loss.discriminator.main.6.bias', 'loss.discriminator.main.6.running_mean', 'loss.discriminator.main.6.running_var', 'loss.discriminator.main.6.num_batches_tracked', 'loss.discriminator.main.8.weight', 'loss.discriminator.main.9.weight', 'loss.discriminator.main.9.bias', 'loss.discriminator.main.9.running_mean', 'loss.discriminator.main.9.running_var', 'loss.discriminator.main.9.num_batches_tracked', 'loss.discriminator.main.11.weight', 'loss.discriminator.main.11.bias'])
Training LatentDiffusion as an unconditional model.
Monitoring val/loss_simple_ema as checkpoint metric.
Merged modelckpt-cfg: 
{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2025-03-26T05-14-29_ffhq-ldm-vq-4/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
#### Data #####
train, FFHQTrain, 60000
validation, FFHQValidation, 10000
accumulate_grad_batches = 1
Setting learning rate to 8.40e-05 = 1 (accumulate_grad_batches) * 7 (num_gpus) * 42 (batchsize) * 2.00e-06 (base_lr)
Running on GPUs 0,1,2,3,4,5,6
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 274.06 M params.
Keeping EMAs of 370.
making attention of type 'vanilla' with 512 in_channels
Working with z of shape (1, 3, 64, 64) = 12288 dimensions.
making attention of type 'vanilla' with 512 in_channels
Restored from models/first_stage_models/vq-f4/model.ckpt with 0 missing and 55 unexpected keys (['loss.perceptual_loss.scaling_layer.shift', 'loss.perceptual_loss.scaling_layer.scale', 'loss.perceptual_loss.net.slice1.0.weight', 'loss.perceptual_loss.net.slice1.0.bias', 'loss.perceptual_loss.net.slice1.2.weight', 'loss.perceptual_loss.net.slice1.2.bias', 'loss.perceptual_loss.net.slice2.5.weight', 'loss.perceptual_loss.net.slice2.5.bias', 'loss.perceptual_loss.net.slice2.7.weight', 'loss.perceptual_loss.net.slice2.7.bias', 'loss.perceptual_loss.net.slice3.10.weight', 'loss.perceptual_loss.net.slice3.10.bias', 'loss.perceptual_loss.net.slice3.12.weight', 'loss.perceptual_loss.net.slice3.12.bias', 'loss.perceptual_loss.net.slice3.14.weight', 'loss.perceptual_loss.net.slice3.14.bias', 'loss.perceptual_loss.net.slice4.17.weight', 'loss.perceptual_loss.net.slice4.17.bias', 'loss.perceptual_loss.net.slice4.19.weight', 'loss.perceptual_loss.net.slice4.19.bias', 'loss.perceptual_loss.net.slice4.21.weight', 'loss.perceptual_loss.net.slice4.21.bias', 'loss.perceptual_loss.net.slice5.24.weight', 'loss.perceptual_loss.net.slice5.24.bias', 'loss.perceptual_loss.net.slice5.26.weight', 'loss.perceptual_loss.net.slice5.26.bias', 'loss.perceptual_loss.net.slice5.28.weight', 'loss.perceptual_loss.net.slice5.28.bias', 'loss.perceptual_loss.lin0.model.1.weight', 'loss.perceptual_loss.lin1.model.1.weight', 'loss.perceptual_loss.lin2.model.1.weight', 'loss.perceptual_loss.lin3.model.1.weight', 'loss.perceptual_loss.lin4.model.1.weight', 'loss.discriminator.main.0.weight', 'loss.discriminator.main.0.bias', 'loss.discriminator.main.2.weight', 'loss.discriminator.main.3.weight', 'loss.discriminator.main.3.bias', 'loss.discriminator.main.3.running_mean', 'loss.discriminator.main.3.running_var', 'loss.discriminator.main.3.num_batches_tracked', 'loss.discriminator.main.5.weight', 'loss.discriminator.main.6.weight', 'loss.discriminator.main.6.bias', 'loss.discriminator.main.6.running_mean', 'loss.discriminator.main.6.running_var', 'loss.discriminator.main.6.num_batches_tracked', 'loss.discriminator.main.8.weight', 'loss.discriminator.main.9.weight', 'loss.discriminator.main.9.bias', 'loss.discriminator.main.9.running_mean', 'loss.discriminator.main.9.running_var', 'loss.discriminator.main.9.num_batches_tracked', 'loss.discriminator.main.11.weight', 'loss.discriminator.main.11.bias'])
Training LatentDiffusion as an unconditional model.
Monitoring val/loss_simple_ema as checkpoint metric.
Merged modelckpt-cfg: 
{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2025-03-26T05-14-26_ffhq-ldm-vq-4/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
#### Data #####
train, FFHQTrain, 60000
validation, FFHQValidation, 10000
accumulate_grad_batches = 1
Setting learning rate to 8.40e-05 = 1 (accumulate_grad_batches) * 7 (num_gpus) * 42 (batchsize) * 2.00e-06 (base_lr)
Running on GPUs 0,1,2,3,4,5,6
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 274.06 M params.
Keeping EMAs of 370.
making attention of type 'vanilla' with 512 in_channels
Working with z of shape (1, 3, 64, 64) = 12288 dimensions.
making attention of type 'vanilla' with 512 in_channels
Restored from models/first_stage_models/vq-f4/model.ckpt with 0 missing and 55 unexpected keys (['loss.perceptual_loss.scaling_layer.shift', 'loss.perceptual_loss.scaling_layer.scale', 'loss.perceptual_loss.net.slice1.0.weight', 'loss.perceptual_loss.net.slice1.0.bias', 'loss.perceptual_loss.net.slice1.2.weight', 'loss.perceptual_loss.net.slice1.2.bias', 'loss.perceptual_loss.net.slice2.5.weight', 'loss.perceptual_loss.net.slice2.5.bias', 'loss.perceptual_loss.net.slice2.7.weight', 'loss.perceptual_loss.net.slice2.7.bias', 'loss.perceptual_loss.net.slice3.10.weight', 'loss.perceptual_loss.net.slice3.10.bias', 'loss.perceptual_loss.net.slice3.12.weight', 'loss.perceptual_loss.net.slice3.12.bias', 'loss.perceptual_loss.net.slice3.14.weight', 'loss.perceptual_loss.net.slice3.14.bias', 'loss.perceptual_loss.net.slice4.17.weight', 'loss.perceptual_loss.net.slice4.17.bias', 'loss.perceptual_loss.net.slice4.19.weight', 'loss.perceptual_loss.net.slice4.19.bias', 'loss.perceptual_loss.net.slice4.21.weight', 'loss.perceptual_loss.net.slice4.21.bias', 'loss.perceptual_loss.net.slice5.24.weight', 'loss.perceptual_loss.net.slice5.24.bias', 'loss.perceptual_loss.net.slice5.26.weight', 'loss.perceptual_loss.net.slice5.26.bias', 'loss.perceptual_loss.net.slice5.28.weight', 'loss.perceptual_loss.net.slice5.28.bias', 'loss.perceptual_loss.lin0.model.1.weight', 'loss.perceptual_loss.lin1.model.1.weight', 'loss.perceptual_loss.lin2.model.1.weight', 'loss.perceptual_loss.lin3.model.1.weight', 'loss.perceptual_loss.lin4.model.1.weight', 'loss.discriminator.main.0.weight', 'loss.discriminator.main.0.bias', 'loss.discriminator.main.2.weight', 'loss.discriminator.main.3.weight', 'loss.discriminator.main.3.bias', 'loss.discriminator.main.3.running_mean', 'loss.discriminator.main.3.running_var', 'loss.discriminator.main.3.num_batches_tracked', 'loss.discriminator.main.5.weight', 'loss.discriminator.main.6.weight', 'loss.discriminator.main.6.bias', 'loss.discriminator.main.6.running_mean', 'loss.discriminator.main.6.running_var', 'loss.discriminator.main.6.num_batches_tracked', 'loss.discriminator.main.8.weight', 'loss.discriminator.main.9.weight', 'loss.discriminator.main.9.bias', 'loss.discriminator.main.9.running_mean', 'loss.discriminator.main.9.running_var', 'loss.discriminator.main.9.num_batches_tracked', 'loss.discriminator.main.11.weight', 'loss.discriminator.main.11.bias'])
Training LatentDiffusion as an unconditional model.
Monitoring val/loss_simple_ema as checkpoint metric.
Merged modelckpt-cfg: 
{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2025-03-26T05-14-41_ffhq-ldm-vq-4/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
#### Data #####
train, FFHQTrain, 60000
validation, FFHQValidation, 10000
accumulate_grad_batches = 1
Setting learning rate to 8.40e-05 = 1 (accumulate_grad_batches) * 7 (num_gpus) * 42 (batchsize) * 2.00e-06 (base_lr)
Sanity Checking DataLoader 0:   0%|          | 0/2 [00:01<?, ?it/s]Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:08<00:08,  8.52s/it]Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:09<00:00,  4.73s/it]                                                                           /srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 42. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
Training: 0it [00:00, ?it/s]Epoch 0:   0%|          | 0/240 [00:00<?, ?it/s]/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2085: LightningDeprecationWarning: `Trainer.root_gpu` is deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.strategy.root_device.index` instead.
  rank_zero_deprecation(
[rank4]:[W326 05:15:15.103644090 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank5]:[W326 05:15:15.110832938 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W326 05:15:15.129726616 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W326 05:15:15.133624554 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W326 05:15:15.213668356 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W326 05:15:15.223663773 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank6]:[W326 05:15:15.272197445 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Epoch 0:   0%|          | 1/240 [00:15<1:01:11, 15.36s/it]Epoch 0:   0%|          | 1/240 [00:15<1:01:12, 15.37s/it, loss=1, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00646, train/loss_step=1.000, global_step=0.000]Epoch 0:   1%|          | 2/240 [00:16<32:06,  8.10s/it, loss=1, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00646, train/loss_step=1.000, global_step=0.000]  Epoch 0:   1%|          | 2/240 [00:16<32:31,  8.20s/it, loss=0.989, v_num=0, train/loss_simple_step=0.976, train/loss_vlb_step=0.00633, train/loss_step=0.976, global_step=1.000]Epoch 0:   1%|▏         | 3/240 [00:17<22:37,  5.73s/it, loss=0.989, v_num=0, train/loss_simple_step=0.976, train/loss_vlb_step=0.00633, train/loss_step=0.976, global_step=1.000]Epoch 0:   1%|▏         | 3/240 [00:17<22:52,  5.79s/it, loss=0.98, v_num=0, train/loss_simple_step=0.961, train/loss_vlb_step=0.00807, train/loss_step=0.961, global_step=2.000] Epoch 0:   2%|▏         | 4/240 [00:18<17:51,  4.54s/it, loss=0.98, v_num=0, train/loss_simple_step=0.961, train/loss_vlb_step=0.00807, train/loss_step=0.961, global_step=2.000]Epoch 0:   2%|▏         | 4/240 [00:18<18:03,  4.59s/it, loss=0.968, v_num=0, train/loss_simple_step=0.932, train/loss_vlb_step=0.010, train/loss_step=0.932, global_step=3.000] Epoch 0:   2%|▏         | 5/240 [00:19<15:00,  3.83s/it, loss=0.968, v_num=0, train/loss_simple_step=0.932, train/loss_vlb_step=0.010, train/loss_step=0.932, global_step=3.000]Epoch 0:   2%|▏         | 5/240 [00:19<15:09,  3.87s/it, loss=0.956, v_num=0, train/loss_simple_step=0.910, train/loss_vlb_step=0.00632, train/loss_step=0.910, global_step=4.000]Epoch 0:   2%|▎         | 6/240 [00:20<13:05,  3.36s/it, loss=0.956, v_num=0, train/loss_simple_step=0.910, train/loss_vlb_step=0.00632, train/loss_step=0.910, global_step=4.000]Epoch 0:   2%|▎         | 6/240 [00:20<13:13,  3.39s/it, loss=0.944, v_num=0, train/loss_simple_step=0.880, train/loss_vlb_step=0.00626, train/loss_step=0.880, global_step=5.000]Epoch 0:   3%|▎         | 7/240 [00:21<11:43,  3.02s/it, loss=0.944, v_num=0, train/loss_simple_step=0.880, train/loss_vlb_step=0.00626, train/loss_step=0.880, global_step=5.000]Epoch 0:   3%|▎         | 7/240 [00:21<11:50,  3.05s/it, loss=0.931, v_num=0, train/loss_simple_step=0.853, train/loss_vlb_step=0.00479, train/loss_step=0.853, global_step=6.000]Epoch 0:   3%|▎         | 8/240 [00:22<10:41,  2.77s/it, loss=0.931, v_num=0, train/loss_simple_step=0.853, train/loss_vlb_step=0.00479, train/loss_step=0.853, global_step=6.000]Epoch 0:   3%|▎         | 8/240 [00:22<10:47,  2.79s/it, loss=0.919, v_num=0, train/loss_simple_step=0.835, train/loss_vlb_step=0.00566, train/loss_step=0.835, global_step=7.000]Epoch 0:   4%|▍         | 9/240 [00:23<09:53,  2.57s/it, loss=0.919, v_num=0, train/loss_simple_step=0.835, train/loss_vlb_step=0.00566, train/loss_step=0.835, global_step=7.000]Epoch 0:   4%|▍         | 9/240 [00:23<09:58,  2.59s/it, loss=0.906, v_num=0, train/loss_simple_step=0.810, train/loss_vlb_step=0.00573, train/loss_step=0.810, global_step=8.000]Epoch 0:   4%|▍         | 10/240 [00:24<09:14,  2.41s/it, loss=0.906, v_num=0, train/loss_simple_step=0.810, train/loss_vlb_step=0.00573, train/loss_step=0.810, global_step=8.000]Epoch 0:   4%|▍         | 10/240 [00:24<09:19,  2.43s/it, loss=0.894, v_num=0, train/loss_simple_step=0.783, train/loss_vlb_step=0.00667, train/loss_step=0.783, global_step=9.000]Epoch 0:   5%|▍         | 11/240 [00:25<08:42,  2.28s/it, loss=0.894, v_num=0, train/loss_simple_step=0.783, train/loss_vlb_step=0.00667, train/loss_step=0.783, global_step=9.000]Epoch 0:   5%|▍         | 11/240 [00:25<08:46,  2.30s/it, loss=0.882, v_num=0, train/loss_simple_step=0.759, train/loss_vlb_step=0.00577, train/loss_step=0.759, global_step=10.00]Epoch 0:   5%|▌         | 12/240 [00:26<08:15,  2.17s/it, loss=0.882, v_num=0, train/loss_simple_step=0.759, train/loss_vlb_step=0.00577, train/loss_step=0.759, global_step=10.00]Epoch 0:   5%|▌         | 12/240 [00:26<08:19,  2.19s/it, loss=0.87, v_num=0, train/loss_simple_step=0.733, train/loss_vlb_step=0.00524, train/loss_step=0.733, global_step=11.00] Epoch 0:   5%|▌         | 13/240 [00:27<07:52,  2.08s/it, loss=0.87, v_num=0, train/loss_simple_step=0.733, train/loss_vlb_step=0.00524, train/loss_step=0.733, global_step=11.00]Epoch 0:   5%|▌         | 13/240 [00:27<07:56,  2.10s/it, loss=0.857, v_num=0, train/loss_simple_step=0.711, train/loss_vlb_step=0.0059, train/loss_step=0.711, global_step=12.00]Epoch 0:   6%|▌         | 14/240 [00:28<07:32,  2.00s/it, loss=0.857, v_num=0, train/loss_simple_step=0.711, train/loss_vlb_step=0.0059, train/loss_step=0.711, global_step=12.00]Epoch 0:   6%|▌         | 14/240 [00:28<07:36,  2.02s/it, loss=0.845, v_num=0, train/loss_simple_step=0.681, train/loss_vlb_step=0.00431, train/loss_step=0.681, global_step=13.00]Epoch 0:   6%|▋         | 15/240 [00:29<07:15,  1.94s/it, loss=0.845, v_num=0, train/loss_simple_step=0.681, train/loss_vlb_step=0.00431, train/loss_step=0.681, global_step=13.00]Epoch 0:   6%|▋         | 15/240 [00:29<07:18,  1.95s/it, loss=0.832, v_num=0, train/loss_simple_step=0.660, train/loss_vlb_step=0.0046, train/loss_step=0.660, global_step=14.00] Epoch 0:   7%|▋         | 16/240 [00:30<07:00,  1.88s/it, loss=0.832, v_num=0, train/loss_simple_step=0.660, train/loss_vlb_step=0.0046, train/loss_step=0.660, global_step=14.00]Epoch 0:   7%|▋         | 16/240 [00:30<07:02,  1.89s/it, loss=0.82, v_num=0, train/loss_simple_step=0.630, train/loss_vlb_step=0.00384, train/loss_step=0.630, global_step=15.00]Epoch 0:   7%|▋         | 17/240 [00:30<06:46,  1.82s/it, loss=0.82, v_num=0, train/loss_simple_step=0.630, train/loss_vlb_step=0.00384, train/loss_step=0.630, global_step=15.00]Epoch 0:   7%|▋         | 17/240 [00:31<06:49,  1.84s/it, loss=0.809, v_num=0, train/loss_simple_step=0.643, train/loss_vlb_step=0.00686, train/loss_step=0.643, global_step=16.00]Epoch 0:   8%|▊         | 18/240 [00:31<06:34,  1.78s/it, loss=0.809, v_num=0, train/loss_simple_step=0.643, train/loss_vlb_step=0.00686, train/loss_step=0.643, global_step=16.00]Epoch 0:   8%|▊         | 18/240 [00:32<06:36,  1.79s/it, loss=0.799, v_num=0, train/loss_simple_step=0.628, train/loss_vlb_step=0.0163, train/loss_step=0.628, global_step=17.00] Epoch 0:   8%|▊         | 19/240 [00:32<06:23,  1.74s/it, loss=0.799, v_num=0, train/loss_simple_step=0.628, train/loss_vlb_step=0.0163, train/loss_step=0.628, global_step=17.00]Epoch 0:   8%|▊         | 19/240 [00:33<06:25,  1.75s/it, loss=0.789, v_num=0, train/loss_simple_step=0.600, train/loss_vlb_step=0.00354, train/loss_step=0.600, global_step=18.00]Epoch 0:   8%|▊         | 20/240 [00:33<06:13,  1.70s/it, loss=0.789, v_num=0, train/loss_simple_step=0.600, train/loss_vlb_step=0.00354, train/loss_step=0.600, global_step=18.00]Epoch 0:   8%|▊         | 20/240 [00:34<06:15,  1.71s/it, loss=0.779, v_num=0, train/loss_simple_step=0.584, train/loss_vlb_step=0.00563, train/loss_step=0.584, global_step=19.00]Epoch 0:   9%|▉         | 21/240 [00:34<06:04,  1.66s/it, loss=0.779, v_num=0, train/loss_simple_step=0.584, train/loss_vlb_step=0.00563, train/loss_step=0.584, global_step=19.00]Epoch 0:   9%|▉         | 21/240 [00:35<06:06,  1.67s/it, loss=0.758, v_num=0, train/loss_simple_step=0.593, train/loss_vlb_step=0.0118, train/loss_step=0.593, global_step=20.00] Epoch 0:   9%|▉         | 22/240 [00:35<05:56,  1.63s/it, loss=0.758, v_num=0, train/loss_simple_step=0.593, train/loss_vlb_step=0.0118, train/loss_step=0.593, global_step=20.00]Epoch 0:   9%|▉         | 22/240 [00:36<05:58,  1.64s/it, loss=0.736, v_num=0, train/loss_simple_step=0.534, train/loss_vlb_step=0.00499, train/loss_step=0.534, global_step=21.00]Epoch 0:  10%|▉         | 23/240 [00:36<05:48,  1.61s/it, loss=0.736, v_num=0, train/loss_simple_step=0.534, train/loss_vlb_step=0.00499, train/loss_step=0.534, global_step=21.00]Epoch 0:  10%|▉         | 23/240 [00:37<05:50,  1.61s/it, loss=0.714, v_num=0, train/loss_simple_step=0.525, train/loss_vlb_step=0.00311, train/loss_step=0.525, global_step=22.00]Epoch 0:  10%|█         | 24/240 [00:37<05:41,  1.58s/it, loss=0.714, v_num=0, train/loss_simple_step=0.525, train/loss_vlb_step=0.00311, train/loss_step=0.525, global_step=22.00]Epoch 0:  10%|█         | 24/240 [00:38<05:43,  1.59s/it, loss=0.692, v_num=0, train/loss_simple_step=0.490, train/loss_vlb_step=0.0164, train/loss_step=0.490, global_step=23.00] Epoch 0:  10%|█         | 25/240 [00:38<05:34,  1.56s/it, loss=0.692, v_num=0, train/loss_simple_step=0.490, train/loss_vlb_step=0.0164, train/loss_step=0.490, global_step=23.00]Epoch 0:  10%|█         | 25/240 [00:39<05:36,  1.56s/it, loss=0.673, v_num=0, train/loss_simple_step=0.537, train/loss_vlb_step=0.00419, train/loss_step=0.537, global_step=24.00]Epoch 0:  11%|█         | 26/240 [00:39<05:28,  1.53s/it, loss=0.673, v_num=0, train/loss_simple_step=0.537, train/loss_vlb_step=0.00419, train/loss_step=0.537, global_step=24.00]Epoch 0:  11%|█         | 26/240 [00:40<05:29,  1.54s/it, loss=0.654, v_num=0, train/loss_simple_step=0.485, train/loss_vlb_step=0.00399, train/loss_step=0.485, global_step=25.00]Epoch 0:  11%|█▏        | 27/240 [00:40<05:22,  1.51s/it, loss=0.654, v_num=0, train/loss_simple_step=0.485, train/loss_vlb_step=0.00399, train/loss_step=0.485, global_step=25.00]Epoch 0:  11%|█▏        | 27/240 [00:41<05:23,  1.52s/it, loss=0.636, v_num=0, train/loss_simple_step=0.490, train/loss_vlb_step=0.00686, train/loss_step=0.490, global_step=26.00]Epoch 0:  12%|█▏        | 28/240 [00:41<05:16,  1.49s/it, loss=0.636, v_num=0, train/loss_simple_step=0.490, train/loss_vlb_step=0.00686, train/loss_step=0.490, global_step=26.00]Epoch 0:  12%|█▏        | 28/240 [00:42<05:18,  1.50s/it, loss=0.617, v_num=0, train/loss_simple_step=0.455, train/loss_vlb_step=0.00347, train/loss_step=0.455, global_step=27.00]Epoch 0:  12%|█▏        | 29/240 [00:42<05:11,  1.48s/it, loss=0.617, v_num=0, train/loss_simple_step=0.455, train/loss_vlb_step=0.00347, train/loss_step=0.455, global_step=27.00]Epoch 0:  12%|█▏        | 29/240 [00:43<05:13,  1.48s/it, loss=0.597, v_num=0, train/loss_simple_step=0.422, train/loss_vlb_step=0.0193, train/loss_step=0.422, global_step=28.00] Epoch 0:  12%|█▎        | 30/240 [00:43<05:06,  1.46s/it, loss=0.597, v_num=0, train/loss_simple_step=0.422, train/loss_vlb_step=0.0193, train/loss_step=0.422, global_step=28.00]Epoch 0:  12%|█▎        | 30/240 [00:44<05:08,  1.47s/it, loss=0.577, v_num=0, train/loss_simple_step=0.385, train/loss_vlb_step=0.00269, train/loss_step=0.385, global_step=29.00]Epoch 0:  13%|█▎        | 31/240 [00:44<05:02,  1.45s/it, loss=0.577, v_num=0, train/loss_simple_step=0.385, train/loss_vlb_step=0.00269, train/loss_step=0.385, global_step=29.00]Epoch 0:  13%|█▎        | 31/240 [00:45<05:03,  1.45s/it, loss=0.56, v_num=0, train/loss_simple_step=0.413, train/loss_vlb_step=0.00472, train/loss_step=0.413, global_step=30.00] Epoch 0:  13%|█▎        | 32/240 [00:45<04:57,  1.43s/it, loss=0.56, v_num=0, train/loss_simple_step=0.413, train/loss_vlb_step=0.00472, train/loss_step=0.413, global_step=30.00]Epoch 0:  13%|█▎        | 32/240 [00:46<04:59,  1.44s/it, loss=0.542, v_num=0, train/loss_simple_step=0.383, train/loss_vlb_step=0.00581, train/loss_step=0.383, global_step=31.00]Epoch 0:  14%|█▍        | 33/240 [00:46<04:53,  1.42s/it, loss=0.542, v_num=0, train/loss_simple_step=0.383, train/loss_vlb_step=0.00581, train/loss_step=0.383, global_step=31.00]Epoch 0:  14%|█▍        | 33/240 [00:47<04:54,  1.42s/it, loss=0.524, v_num=0, train/loss_simple_step=0.350, train/loss_vlb_step=0.00237, train/loss_step=0.350, global_step=32.00]Epoch 0:  14%|█▍        | 34/240 [00:47<04:49,  1.41s/it, loss=0.524, v_num=0, train/loss_simple_step=0.350, train/loss_vlb_step=0.00237, train/loss_step=0.350, global_step=32.00]Epoch 0:  14%|█▍        | 34/240 [00:47<04:50,  1.41s/it, loss=0.509, v_num=0, train/loss_simple_step=0.380, train/loss_vlb_step=0.0173, train/loss_step=0.380, global_step=33.00] Epoch 0:  15%|█▍        | 35/240 [00:48<04:45,  1.39s/it, loss=0.509, v_num=0, train/loss_simple_step=0.380, train/loss_vlb_step=0.0173, train/loss_step=0.380, global_step=33.00]Epoch 0:  15%|█▍        | 35/240 [00:48<04:46,  1.40s/it, loss=0.494, v_num=0, train/loss_simple_step=0.351, train/loss_vlb_step=0.00233, train/loss_step=0.351, global_step=34.00]Epoch 0:  15%|█▌        | 36/240 [00:49<04:41,  1.38s/it, loss=0.494, v_num=0, train/loss_simple_step=0.351, train/loss_vlb_step=0.00233, train/loss_step=0.351, global_step=34.00]Epoch 0:  15%|█▌        | 36/240 [00:49<04:43,  1.39s/it, loss=0.481, v_num=0, train/loss_simple_step=0.364, train/loss_vlb_step=0.00249, train/loss_step=0.364, global_step=35.00]Epoch 0:  15%|█▌        | 37/240 [00:50<04:38,  1.37s/it, loss=0.481, v_num=0, train/loss_simple_step=0.364, train/loss_vlb_step=0.00249, train/loss_step=0.364, global_step=35.00]Epoch 0:  15%|█▌        | 37/240 [00:50<04:39,  1.38s/it, loss=0.466, v_num=0, train/loss_simple_step=0.345, train/loss_vlb_step=0.00343, train/loss_step=0.345, global_step=36.00]Epoch 0:  16%|█▌        | 38/240 [00:51<04:34,  1.36s/it, loss=0.466, v_num=0, train/loss_simple_step=0.345, train/loss_vlb_step=0.00343, train/loss_step=0.345, global_step=36.00]Epoch 0:  16%|█▌        | 38/240 [00:51<04:35,  1.37s/it, loss=0.451, v_num=0, train/loss_simple_step=0.342, train/loss_vlb_step=0.0031, train/loss_step=0.342, global_step=37.00] Epoch 0:  16%|█▋        | 39/240 [00:52<04:31,  1.35s/it, loss=0.451, v_num=0, train/loss_simple_step=0.342, train/loss_vlb_step=0.0031, train/loss_step=0.342, global_step=37.00]Epoch 0:  16%|█▋        | 39/240 [00:52<04:32,  1.36s/it, loss=0.438, v_num=0, train/loss_simple_step=0.336, train/loss_vlb_step=0.00263, train/loss_step=0.336, global_step=38.00]Epoch 0:  17%|█▋        | 40/240 [00:53<04:28,  1.34s/it, loss=0.438, v_num=0, train/loss_simple_step=0.336, train/loss_vlb_step=0.00263, train/loss_step=0.336, global_step=38.00]Epoch 0:  17%|█▋        | 40/240 [00:53<04:29,  1.35s/it, loss=0.427, v_num=0, train/loss_simple_step=0.367, train/loss_vlb_step=0.018, train/loss_step=0.367, global_step=39.00]  Epoch 0:  17%|█▋        | 41/240 [00:54<04:25,  1.33s/it, loss=0.427, v_num=0, train/loss_simple_step=0.367, train/loss_vlb_step=0.018, train/loss_step=0.367, global_step=39.00]Epoch 0:  17%|█▋        | 41/240 [00:54<04:26,  1.34s/it, loss=0.414, v_num=0, train/loss_simple_step=0.328, train/loss_vlb_step=0.0169, train/loss_step=0.328, global_step=40.00]Epoch 0:  18%|█▊        | 42/240 [00:55<04:22,  1.32s/it, loss=0.414, v_num=0, train/loss_simple_step=0.328, train/loss_vlb_step=0.0169, train/loss_step=0.328, global_step=40.00]Epoch 0:  18%|█▊        | 42/240 [00:55<04:23,  1.33s/it, loss=0.402, v_num=0, train/loss_simple_step=0.292, train/loss_vlb_step=0.00187, train/loss_step=0.292, global_step=41.00]Epoch 0:  18%|█▊        | 43/240 [00:56<04:19,  1.32s/it, loss=0.402, v_num=0, train/loss_simple_step=0.292, train/loss_vlb_step=0.00187, train/loss_step=0.292, global_step=41.00]Epoch 0:  18%|█▊        | 43/240 [00:56<04:20,  1.32s/it, loss=0.39, v_num=0, train/loss_simple_step=0.284, train/loss_vlb_step=0.00251, train/loss_step=0.284, global_step=42.00] Epoch 0:  18%|█▊        | 44/240 [00:57<04:16,  1.31s/it, loss=0.39, v_num=0, train/loss_simple_step=0.284, train/loss_vlb_step=0.00251, train/loss_step=0.284, global_step=42.00]Epoch 0:  18%|█▊        | 44/240 [00:57<04:17,  1.31s/it, loss=0.38, v_num=0, train/loss_simple_step=0.293, train/loss_vlb_step=0.00199, train/loss_step=0.293, global_step=43.00]Epoch 0:  19%|█▉        | 45/240 [00:58<04:13,  1.30s/it, loss=0.38, v_num=0, train/loss_simple_step=0.293, train/loss_vlb_step=0.00199, train/loss_step=0.293, global_step=43.00]Epoch 0:  19%|█▉        | 45/240 [00:58<04:14,  1.31s/it, loss=0.37, v_num=0, train/loss_simple_step=0.338, train/loss_vlb_step=0.00361, train/loss_step=0.338, global_step=44.00]Epoch 0:  19%|█▉        | 46/240 [00:59<04:11,  1.30s/it, loss=0.37, v_num=0, train/loss_simple_step=0.338, train/loss_vlb_step=0.00361, train/loss_step=0.338, global_step=44.00]Epoch 0:  19%|█▉        | 46/240 [00:59<04:12,  1.30s/it, loss=0.361, v_num=0, train/loss_simple_step=0.294, train/loss_vlb_step=0.00483, train/loss_step=0.294, global_step=45.00]Epoch 0:  20%|█▉        | 47/240 [01:00<04:08,  1.29s/it, loss=0.361, v_num=0, train/loss_simple_step=0.294, train/loss_vlb_step=0.00483, train/loss_step=0.294, global_step=45.00]Epoch 0:  20%|█▉        | 47/240 [01:00<04:09,  1.29s/it, loss=0.348, v_num=0, train/loss_simple_step=0.236, train/loss_vlb_step=0.00164, train/loss_step=0.236, global_step=46.00]Epoch 0:  20%|██        | 48/240 [01:01<04:06,  1.28s/it, loss=0.348, v_num=0, train/loss_simple_step=0.236, train/loss_vlb_step=0.00164, train/loss_step=0.236, global_step=46.00]Epoch 0:  20%|██        | 48/240 [01:01<04:07,  1.29s/it, loss=0.339, v_num=0, train/loss_simple_step=0.282, train/loss_vlb_step=0.0025, train/loss_step=0.282, global_step=47.00] Epoch 0:  20%|██        | 49/240 [01:02<04:03,  1.28s/it, loss=0.339, v_num=0, train/loss_simple_step=0.282, train/loss_vlb_step=0.0025, train/loss_step=0.282, global_step=47.00]Epoch 0:  20%|██        | 49/240 [01:02<04:04,  1.28s/it, loss=0.33, v_num=0, train/loss_simple_step=0.243, train/loss_vlb_step=0.00158, train/loss_step=0.243, global_step=48.00]Epoch 0:  21%|██        | 50/240 [01:03<04:01,  1.27s/it, loss=0.33, v_num=0, train/loss_simple_step=0.243, train/loss_vlb_step=0.00158, train/loss_step=0.243, global_step=48.00]Epoch 0:  21%|██        | 50/240 [01:03<04:02,  1.27s/it, loss=0.325, v_num=0, train/loss_simple_step=0.272, train/loss_vlb_step=0.00278, train/loss_step=0.272, global_step=49.00]Epoch 0:  21%|██▏       | 51/240 [01:04<03:59,  1.27s/it, loss=0.325, v_num=0, train/loss_simple_step=0.272, train/loss_vlb_step=0.00278, train/loss_step=0.272, global_step=49.00]Epoch 0:  21%|██▏       | 51/240 [01:04<03:59,  1.27s/it, loss=0.317, v_num=0, train/loss_simple_step=0.268, train/loss_vlb_step=0.00233, train/loss_step=0.268, global_step=50.00]Epoch 0:  22%|██▏       | 52/240 [01:05<03:56,  1.26s/it, loss=0.317, v_num=0, train/loss_simple_step=0.268, train/loss_vlb_step=0.00233, train/loss_step=0.268, global_step=50.00]Epoch 0:  22%|██▏       | 52/240 [01:05<03:57,  1.26s/it, loss=0.311, v_num=0, train/loss_simple_step=0.264, train/loss_vlb_step=0.0155, train/loss_step=0.264, global_step=51.00] Epoch 0:  22%|██▏       | 53/240 [01:06<03:54,  1.25s/it, loss=0.311, v_num=0, train/loss_simple_step=0.264, train/loss_vlb_step=0.0155, train/loss_step=0.264, global_step=51.00]Epoch 0:  22%|██▏       | 53/240 [01:06<03:55,  1.26s/it, loss=0.31, v_num=0, train/loss_simple_step=0.325, train/loss_vlb_step=0.00393, train/loss_step=0.325, global_step=52.00]Epoch 0:  22%|██▎       | 54/240 [01:07<03:52,  1.25s/it, loss=0.31, v_num=0, train/loss_simple_step=0.325, train/loss_vlb_step=0.00393, train/loss_step=0.325, global_step=52.00]Epoch 0:  22%|██▎       | 54/240 [01:07<03:53,  1.25s/it, loss=0.303, v_num=0, train/loss_simple_step=0.226, train/loss_vlb_step=0.00168, train/loss_step=0.226, global_step=53.00]Epoch 0:  23%|██▎       | 55/240 [01:08<03:50,  1.24s/it, loss=0.303, v_num=0, train/loss_simple_step=0.226, train/loss_vlb_step=0.00168, train/loss_step=0.226, global_step=53.00]Epoch 0:  23%|██▎       | 55/240 [01:08<03:50,  1.25s/it, loss=0.298, v_num=0, train/loss_simple_step=0.251, train/loss_vlb_step=0.0205, train/loss_step=0.251, global_step=54.00] Epoch 0:  23%|██▎       | 56/240 [01:09<03:48,  1.24s/it, loss=0.298, v_num=0, train/loss_simple_step=0.251, train/loss_vlb_step=0.0205, train/loss_step=0.251, global_step=54.00]Epoch 0:  23%|██▎       | 56/240 [01:09<03:48,  1.24s/it, loss=0.291, v_num=0, train/loss_simple_step=0.226, train/loss_vlb_step=0.00169, train/loss_step=0.226, global_step=55.00]Epoch 0:  24%|██▍       | 57/240 [01:10<03:46,  1.24s/it, loss=0.291, v_num=0, train/loss_simple_step=0.226, train/loss_vlb_step=0.00169, train/loss_step=0.226, global_step=55.00]Epoch 0:  24%|██▍       | 57/240 [01:10<03:46,  1.24s/it, loss=0.288, v_num=0, train/loss_simple_step=0.289, train/loss_vlb_step=0.00938, train/loss_step=0.289, global_step=56.00]Epoch 0:  24%|██▍       | 58/240 [01:11<03:44,  1.23s/it, loss=0.288, v_num=0, train/loss_simple_step=0.289, train/loss_vlb_step=0.00938, train/loss_step=0.289, global_step=56.00]Epoch 0:  24%|██▍       | 58/240 [01:11<03:44,  1.23s/it, loss=0.281, v_num=0, train/loss_simple_step=0.204, train/loss_vlb_step=0.00794, train/loss_step=0.204, global_step=57.00]Epoch 0:  25%|██▍       | 59/240 [01:12<03:42,  1.23s/it, loss=0.281, v_num=0, train/loss_simple_step=0.204, train/loss_vlb_step=0.00794, train/loss_step=0.204, global_step=57.00]Epoch 0:  25%|██▍       | 59/240 [01:12<03:42,  1.23s/it, loss=0.276, v_num=0, train/loss_simple_step=0.240, train/loss_vlb_step=0.002, train/loss_step=0.240, global_step=58.00]  Epoch 0:  25%|██▌       | 60/240 [01:13<03:40,  1.22s/it, loss=0.276, v_num=0, train/loss_simple_step=0.240, train/loss_vlb_step=0.002, train/loss_step=0.240, global_step=58.00]Epoch 0:  25%|██▌       | 60/240 [01:13<03:40,  1.23s/it, loss=0.272, v_num=0, train/loss_simple_step=0.278, train/loss_vlb_step=0.00242, train/loss_step=0.278, global_step=59.00]Epoch 0:  25%|██▌       | 61/240 [01:14<03:38,  1.22s/it, loss=0.272, v_num=0, train/loss_simple_step=0.278, train/loss_vlb_step=0.00242, train/loss_step=0.278, global_step=59.00]Epoch 0:  25%|██▌       | 61/240 [01:14<03:38,  1.22s/it, loss=0.271, v_num=0, train/loss_simple_step=0.312, train/loss_vlb_step=0.0062, train/loss_step=0.312, global_step=60.00] Epoch 0:  26%|██▌       | 62/240 [01:15<03:36,  1.22s/it, loss=0.271, v_num=0, train/loss_simple_step=0.312, train/loss_vlb_step=0.0062, train/loss_step=0.312, global_step=60.00]Epoch 0:  26%|██▌       | 62/240 [01:15<03:36,  1.22s/it, loss=0.266, v_num=0, train/loss_simple_step=0.189, train/loss_vlb_step=0.00635, train/loss_step=0.189, global_step=61.00]Epoch 0:  26%|██▋       | 63/240 [01:16<03:34,  1.21s/it, loss=0.266, v_num=0, train/loss_simple_step=0.189, train/loss_vlb_step=0.00635, train/loss_step=0.189, global_step=61.00]Epoch 0:  26%|██▋       | 63/240 [01:16<03:35,  1.21s/it, loss=0.264, v_num=0, train/loss_simple_step=0.252, train/loss_vlb_step=0.00284, train/loss_step=0.252, global_step=62.00]Epoch 0:  27%|██▋       | 64/240 [01:17<03:32,  1.21s/it, loss=0.264, v_num=0, train/loss_simple_step=0.252, train/loss_vlb_step=0.00284, train/loss_step=0.252, global_step=62.00]Epoch 0:  27%|██▋       | 64/240 [01:17<03:33,  1.21s/it, loss=0.261, v_num=0, train/loss_simple_step=0.235, train/loss_vlb_step=0.0152, train/loss_step=0.235, global_step=63.00] Epoch 0:  27%|██▋       | 65/240 [01:18<03:30,  1.20s/it, loss=0.261, v_num=0, train/loss_simple_step=0.235, train/loss_vlb_step=0.0152, train/loss_step=0.235, global_step=63.00]Epoch 0:  27%|██▋       | 65/240 [01:18<03:31,  1.21s/it, loss=0.255, v_num=0, train/loss_simple_step=0.218, train/loss_vlb_step=0.00194, train/loss_step=0.218, global_step=64.00]Epoch 0:  28%|██▊       | 66/240 [01:19<03:29,  1.20s/it, loss=0.255, v_num=0, train/loss_simple_step=0.218, train/loss_vlb_step=0.00194, train/loss_step=0.218, global_step=64.00]Epoch 0:  28%|██▊       | 66/240 [01:19<03:29,  1.20s/it, loss=0.252, v_num=0, train/loss_simple_step=0.236, train/loss_vlb_step=0.00582, train/loss_step=0.236, global_step=65.00]Epoch 0:  28%|██▊       | 67/240 [01:20<03:27,  1.20s/it, loss=0.252, v_num=0, train/loss_simple_step=0.236, train/loss_vlb_step=0.00582, train/loss_step=0.236, global_step=65.00]Epoch 0:  28%|██▊       | 67/240 [01:20<03:27,  1.20s/it, loss=0.251, v_num=0, train/loss_simple_step=0.210, train/loss_vlb_step=0.00121, train/loss_step=0.210, global_step=66.00]Epoch 0:  28%|██▊       | 68/240 [01:21<03:25,  1.20s/it, loss=0.251, v_num=0, train/loss_simple_step=0.210, train/loss_vlb_step=0.00121, train/loss_step=0.210, global_step=66.00]Epoch 0:  28%|██▊       | 68/240 [01:21<03:26,  1.20s/it, loss=0.249, v_num=0, train/loss_simple_step=0.233, train/loss_vlb_step=0.00537, train/loss_step=0.233, global_step=67.00]Epoch 0:  29%|██▉       | 69/240 [01:22<03:23,  1.19s/it, loss=0.249, v_num=0, train/loss_simple_step=0.233, train/loss_vlb_step=0.00537, train/loss_step=0.233, global_step=67.00]Epoch 0:  29%|██▉       | 69/240 [01:22<03:24,  1.20s/it, loss=0.248, v_num=0, train/loss_simple_step=0.223, train/loss_vlb_step=0.00131, train/loss_step=0.223, global_step=68.00]Epoch 0:  29%|██▉       | 70/240 [01:23<03:22,  1.19s/it, loss=0.248, v_num=0, train/loss_simple_step=0.223, train/loss_vlb_step=0.00131, train/loss_step=0.223, global_step=68.00]Epoch 0:  29%|██▉       | 70/240 [01:23<03:22,  1.19s/it, loss=0.248, v_num=0, train/loss_simple_step=0.277, train/loss_vlb_step=0.00206, train/loss_step=0.277, global_step=69.00]Epoch 0:  30%|██▉       | 71/240 [01:24<03:20,  1.19s/it, loss=0.248, v_num=0, train/loss_simple_step=0.277, train/loss_vlb_step=0.00206, train/loss_step=0.277, global_step=69.00]Epoch 0:  30%|██▉       | 71/240 [01:24<03:21,  1.19s/it, loss=0.244, v_num=0, train/loss_simple_step=0.188, train/loss_vlb_step=0.0013, train/loss_step=0.188, global_step=70.00] Epoch 0:  30%|███       | 72/240 [01:25<03:18,  1.18s/it, loss=0.244, v_num=0, train/loss_simple_step=0.188, train/loss_vlb_step=0.0013, train/loss_step=0.188, global_step=70.00]Epoch 0:  30%|███       | 72/240 [01:25<03:19,  1.19s/it, loss=0.238, v_num=0, train/loss_simple_step=0.156, train/loss_vlb_step=0.000727, train/loss_step=0.156, global_step=71.00]Epoch 0:  30%|███       | 73/240 [01:26<03:17,  1.18s/it, loss=0.238, v_num=0, train/loss_simple_step=0.156, train/loss_vlb_step=0.000727, train/loss_step=0.156, global_step=71.00]Epoch 0:  30%|███       | 73/240 [01:26<03:17,  1.18s/it, loss=0.231, v_num=0, train/loss_simple_step=0.186, train/loss_vlb_step=0.0016, train/loss_step=0.186, global_step=72.00]  Epoch 0:  31%|███       | 74/240 [01:27<03:15,  1.18s/it, loss=0.231, v_num=0, train/loss_simple_step=0.186, train/loss_vlb_step=0.0016, train/loss_step=0.186, global_step=72.00]Epoch 0:  31%|███       | 74/240 [01:27<03:16,  1.18s/it, loss=0.231, v_num=0, train/loss_simple_step=0.210, train/loss_vlb_step=0.00133, train/loss_step=0.210, global_step=73.00]Epoch 0:  31%|███▏      | 75/240 [01:28<03:14,  1.18s/it, loss=0.231, v_num=0, train/loss_simple_step=0.210, train/loss_vlb_step=0.00133, train/loss_step=0.210, global_step=73.00]Epoch 0:  31%|███▏      | 75/240 [01:28<03:14,  1.18s/it, loss=0.229, v_num=0, train/loss_simple_step=0.213, train/loss_vlb_step=0.00211, train/loss_step=0.213, global_step=74.00]Epoch 0:  32%|███▏      | 76/240 [01:29<03:12,  1.17s/it, loss=0.229, v_num=0, train/loss_simple_step=0.213, train/loss_vlb_step=0.00211, train/loss_step=0.213, global_step=74.00]Epoch 0:  32%|███▏      | 76/240 [01:29<03:12,  1.18s/it, loss=0.228, v_num=0, train/loss_simple_step=0.213, train/loss_vlb_step=0.00147, train/loss_step=0.213, global_step=75.00]Epoch 0:  32%|███▏      | 77/240 [01:30<03:10,  1.17s/it, loss=0.228, v_num=0, train/loss_simple_step=0.213, train/loss_vlb_step=0.00147, train/loss_step=0.213, global_step=75.00]Epoch 0:  32%|███▏      | 77/240 [01:30<03:11,  1.17s/it, loss=0.228, v_num=0, train/loss_simple_step=0.297, train/loss_vlb_step=0.00908, train/loss_step=0.297, global_step=76.00]Epoch 0:  32%|███▎      | 78/240 [01:31<03:09,  1.17s/it, loss=0.228, v_num=0, train/loss_simple_step=0.297, train/loss_vlb_step=0.00908, train/loss_step=0.297, global_step=76.00]Epoch 0:  32%|███▎      | 78/240 [01:31<03:09,  1.17s/it, loss=0.228, v_num=0, train/loss_simple_step=0.203, train/loss_vlb_step=0.00234, train/loss_step=0.203, global_step=77.00]Epoch 0:  33%|███▎      | 79/240 [01:32<03:07,  1.17s/it, loss=0.228, v_num=0, train/loss_simple_step=0.203, train/loss_vlb_step=0.00234, train/loss_step=0.203, global_step=77.00]Epoch 0:  33%|███▎      | 79/240 [01:32<03:08,  1.17s/it, loss=0.228, v_num=0, train/loss_simple_step=0.232, train/loss_vlb_step=0.00228, train/loss_step=0.232, global_step=78.00]Epoch 0:  33%|███▎      | 80/240 [01:33<03:06,  1.16s/it, loss=0.228, v_num=0, train/loss_simple_step=0.232, train/loss_vlb_step=0.00228, train/loss_step=0.232, global_step=78.00]Epoch 0:  33%|███▎      | 80/240 [01:33<03:06,  1.17s/it, loss=0.226, v_num=0, train/loss_simple_step=0.230, train/loss_vlb_step=0.00211, train/loss_step=0.230, global_step=79.00]Epoch 0:  34%|███▍      | 81/240 [01:34<03:04,  1.16s/it, loss=0.226, v_num=0, train/loss_simple_step=0.230, train/loss_vlb_step=0.00211, train/loss_step=0.230, global_step=79.00]Epoch 0:  34%|███▍      | 81/240 [01:34<03:05,  1.16s/it, loss=0.223, v_num=0, train/loss_simple_step=0.250, train/loss_vlb_step=0.00355, train/loss_step=0.250, global_step=80.00]Epoch 0:  34%|███▍      | 82/240 [01:35<03:03,  1.16s/it, loss=0.223, v_num=0, train/loss_simple_step=0.250, train/loss_vlb_step=0.00355, train/loss_step=0.250, global_step=80.00]Epoch 0:  34%|███▍      | 82/240 [01:35<03:03,  1.16s/it, loss=0.224, v_num=0, train/loss_simple_step=0.223, train/loss_vlb_step=0.00243, train/loss_step=0.223, global_step=81.00]Epoch 0:  35%|███▍      | 83/240 [01:36<03:01,  1.16s/it, loss=0.224, v_num=0, train/loss_simple_step=0.223, train/loss_vlb_step=0.00243, train/loss_step=0.223, global_step=81.00]Epoch 0:  35%|███▍      | 83/240 [01:36<03:02,  1.16s/it, loss=0.221, v_num=0, train/loss_simple_step=0.181, train/loss_vlb_step=0.00101, train/loss_step=0.181, global_step=82.00]Epoch 0:  35%|███▌      | 84/240 [01:37<03:00,  1.16s/it, loss=0.221, v_num=0, train/loss_simple_step=0.181, train/loss_vlb_step=0.00101, train/loss_step=0.181, global_step=82.00]Epoch 0:  35%|███▌      | 84/240 [01:37<03:00,  1.16s/it, loss=0.219, v_num=0, train/loss_simple_step=0.201, train/loss_vlb_step=0.00106, train/loss_step=0.201, global_step=83.00]Epoch 0:  35%|███▌      | 85/240 [01:38<02:58,  1.15s/it, loss=0.219, v_num=0, train/loss_simple_step=0.201, train/loss_vlb_step=0.00106, train/loss_step=0.201, global_step=83.00]Epoch 0:  35%|███▌      | 85/240 [01:38<02:59,  1.16s/it, loss=0.221, v_num=0, train/loss_simple_step=0.268, train/loss_vlb_step=0.015, train/loss_step=0.268, global_step=84.00]  Epoch 0:  36%|███▌      | 86/240 [01:39<02:57,  1.15s/it, loss=0.221, v_num=0, train/loss_simple_step=0.268, train/loss_vlb_step=0.015, train/loss_step=0.268, global_step=84.00]Epoch 0:  36%|███▌      | 86/240 [01:39<02:57,  1.15s/it, loss=0.22, v_num=0, train/loss_simple_step=0.201, train/loss_vlb_step=0.00232, train/loss_step=0.201, global_step=85.00]Epoch 0:  36%|███▋      | 87/240 [01:40<02:55,  1.15s/it, loss=0.22, v_num=0, train/loss_simple_step=0.201, train/loss_vlb_step=0.00232, train/loss_step=0.201, global_step=85.00]Epoch 0:  36%|███▋      | 87/240 [01:40<02:56,  1.15s/it, loss=0.219, v_num=0, train/loss_simple_step=0.186, train/loss_vlb_step=0.00163, train/loss_step=0.186, global_step=86.00]Epoch 0:  37%|███▋      | 88/240 [01:41<02:54,  1.15s/it, loss=0.219, v_num=0, train/loss_simple_step=0.186, train/loss_vlb_step=0.00163, train/loss_step=0.186, global_step=86.00]Epoch 0:  37%|███▋      | 88/240 [01:41<02:54,  1.15s/it, loss=0.219, v_num=0, train/loss_simple_step=0.236, train/loss_vlb_step=0.0146, train/loss_step=0.236, global_step=87.00] Epoch 0:  37%|███▋      | 89/240 [01:42<02:53,  1.15s/it, loss=0.219, v_num=0, train/loss_simple_step=0.236, train/loss_vlb_step=0.0146, train/loss_step=0.236, global_step=87.00]Epoch 0:  37%|███▋      | 89/240 [01:42<02:53,  1.15s/it, loss=0.219, v_num=0, train/loss_simple_step=0.230, train/loss_vlb_step=0.00802, train/loss_step=0.230, global_step=88.00]Epoch 0:  38%|███▊      | 90/240 [01:43<02:51,  1.15s/it, loss=0.219, v_num=0, train/loss_simple_step=0.230, train/loss_vlb_step=0.00802, train/loss_step=0.230, global_step=88.00]Epoch 0:  38%|███▊      | 90/240 [01:43<02:52,  1.15s/it, loss=0.214, v_num=0, train/loss_simple_step=0.169, train/loss_vlb_step=0.000818, train/loss_step=0.169, global_step=89.00]Epoch 0:  38%|███▊      | 91/240 [01:44<02:50,  1.14s/it, loss=0.214, v_num=0, train/loss_simple_step=0.169, train/loss_vlb_step=0.000818, train/loss_step=0.169, global_step=89.00]Epoch 0:  38%|███▊      | 91/240 [01:44<02:50,  1.15s/it, loss=0.213, v_num=0, train/loss_simple_step=0.172, train/loss_vlb_step=0.001, train/loss_step=0.172, global_step=90.00]   Epoch 0:  38%|███▊      | 92/240 [01:45<02:49,  1.14s/it, loss=0.213, v_num=0, train/loss_simple_step=0.172, train/loss_vlb_step=0.001, train/loss_step=0.172, global_step=90.00]Epoch 0:  38%|███▊      | 92/240 [01:45<02:49,  1.14s/it, loss=0.216, v_num=0, train/loss_simple_step=0.210, train/loss_vlb_step=0.00143, train/loss_step=0.210, global_step=91.00]Epoch 0:  39%|███▉      | 93/240 [01:46<02:47,  1.14s/it, loss=0.216, v_num=0, train/loss_simple_step=0.210, train/loss_vlb_step=0.00143, train/loss_step=0.210, global_step=91.00]Epoch 0:  39%|███▉      | 93/240 [01:46<02:48,  1.14s/it, loss=0.216, v_num=0, train/loss_simple_step=0.190, train/loss_vlb_step=0.00113, train/loss_step=0.190, global_step=92.00]Epoch 0:  39%|███▉      | 94/240 [01:47<02:46,  1.14s/it, loss=0.216, v_num=0, train/loss_simple_step=0.190, train/loss_vlb_step=0.00113, train/loss_step=0.190, global_step=92.00]Epoch 0:  39%|███▉      | 94/240 [01:47<02:46,  1.14s/it, loss=0.213, v_num=0, train/loss_simple_step=0.159, train/loss_vlb_step=0.000861, train/loss_step=0.159, global_step=93.00]Epoch 0:  40%|███▉      | 95/240 [01:48<02:45,  1.14s/it, loss=0.213, v_num=0, train/loss_simple_step=0.159, train/loss_vlb_step=0.000861, train/loss_step=0.159, global_step=93.00]Epoch 0:  40%|███▉      | 95/240 [01:48<02:45,  1.14s/it, loss=0.212, v_num=0, train/loss_simple_step=0.192, train/loss_vlb_step=0.00148, train/loss_step=0.192, global_step=94.00] Epoch 0:  40%|████      | 96/240 [01:49<02:43,  1.14s/it, loss=0.212, v_num=0, train/loss_simple_step=0.192, train/loss_vlb_step=0.00148, train/loss_step=0.192, global_step=94.00]Epoch 0:  40%|████      | 96/240 [01:49<02:44,  1.14s/it, loss=0.212, v_num=0, train/loss_simple_step=0.204, train/loss_vlb_step=0.00346, train/loss_step=0.204, global_step=95.00]Epoch 0:  40%|████      | 97/240 [01:50<02:42,  1.14s/it, loss=0.212, v_num=0, train/loss_simple_step=0.204, train/loss_vlb_step=0.00346, train/loss_step=0.204, global_step=95.00]Epoch 0:  40%|████      | 97/240 [01:50<02:42,  1.14s/it, loss=0.208, v_num=0, train/loss_simple_step=0.214, train/loss_vlb_step=0.00133, train/loss_step=0.214, global_step=96.00]Epoch 0:  41%|████      | 98/240 [01:51<02:41,  1.13s/it, loss=0.208, v_num=0, train/loss_simple_step=0.214, train/loss_vlb_step=0.00133, train/loss_step=0.214, global_step=96.00]Epoch 0:  41%|████      | 98/240 [01:51<02:41,  1.14s/it, loss=0.206, v_num=0, train/loss_simple_step=0.164, train/loss_vlb_step=0.00177, train/loss_step=0.164, global_step=97.00]Epoch 0:  41%|████▏     | 99/240 [01:52<02:39,  1.13s/it, loss=0.206, v_num=0, train/loss_simple_step=0.164, train/loss_vlb_step=0.00177, train/loss_step=0.164, global_step=97.00]Epoch 0:  41%|████▏     | 99/240 [01:52<02:39,  1.13s/it, loss=0.207, v_num=0, train/loss_simple_step=0.267, train/loss_vlb_step=0.00171, train/loss_step=0.267, global_step=98.00]Epoch 0:  42%|████▏     | 100/240 [01:53<02:38,  1.13s/it, loss=0.207, v_num=0, train/loss_simple_step=0.267, train/loss_vlb_step=0.00171, train/loss_step=0.267, global_step=98.00]Epoch 0:  42%|████▏     | 100/240 [01:53<02:38,  1.13s/it, loss=0.205, v_num=0, train/loss_simple_step=0.179, train/loss_vlb_step=0.00382, train/loss_step=0.179, global_step=99.00]Epoch 0:  42%|████▏     | 101/240 [01:54<02:37,  1.13s/it, loss=0.205, v_num=0, train/loss_simple_step=0.179, train/loss_vlb_step=0.00382, train/loss_step=0.179, global_step=99.00]Epoch 0:  42%|████▏     | 101/240 [01:54<02:37,  1.13s/it, loss=0.201, v_num=0, train/loss_simple_step=0.170, train/loss_vlb_step=0.00132, train/loss_step=0.170, global_step=100.0]Epoch 0:  42%|████▎     | 102/240 [01:55<02:35,  1.13s/it, loss=0.201, v_num=0, train/loss_simple_step=0.170, train/loss_vlb_step=0.00132, train/loss_step=0.170, global_step=100.0]Epoch 0:  42%|████▎     | 102/240 [01:55<02:36,  1.13s/it, loss=0.202, v_num=0, train/loss_simple_step=0.250, train/loss_vlb_step=0.00177, train/loss_step=0.250, global_step=101.0]Epoch 0:  43%|████▎     | 103/240 [01:56<02:34,  1.13s/it, loss=0.202, v_num=0, train/loss_simple_step=0.250, train/loss_vlb_step=0.00177, train/loss_step=0.250, global_step=101.0]Epoch 0:  43%|████▎     | 103/240 [01:56<02:34,  1.13s/it, loss=0.204, v_num=0, train/loss_simple_step=0.221, train/loss_vlb_step=0.0134, train/loss_step=0.221, global_step=102.0] Epoch 0:  43%|████▎     | 104/240 [01:57<02:33,  1.13s/it, loss=0.204, v_num=0, train/loss_simple_step=0.221, train/loss_vlb_step=0.0134, train/loss_step=0.221, global_step=102.0]Epoch 0:  43%|████▎     | 104/240 [01:57<02:33,  1.13s/it, loss=0.204, v_num=0, train/loss_simple_step=0.204, train/loss_vlb_step=0.00261, train/loss_step=0.204, global_step=103.0]Epoch 0:  44%|████▍     | 105/240 [01:58<02:31,  1.12s/it, loss=0.204, v_num=0, train/loss_simple_step=0.204, train/loss_vlb_step=0.00261, train/loss_step=0.204, global_step=103.0]Epoch 0:  44%|████▍     | 105/240 [01:58<02:32,  1.13s/it, loss=0.203, v_num=0, train/loss_simple_step=0.242, train/loss_vlb_step=0.00141, train/loss_step=0.242, global_step=104.0]Epoch 0:  44%|████▍     | 106/240 [01:59<02:30,  1.12s/it, loss=0.203, v_num=0, train/loss_simple_step=0.242, train/loss_vlb_step=0.00141, train/loss_step=0.242, global_step=104.0]Epoch 0:  44%|████▍     | 106/240 [01:59<02:30,  1.13s/it, loss=0.204, v_num=0, train/loss_simple_step=0.214, train/loss_vlb_step=0.00187, train/loss_step=0.214, global_step=105.0]Epoch 0:  45%|████▍     | 107/240 [02:00<02:29,  1.12s/it, loss=0.204, v_num=0, train/loss_simple_step=0.214, train/loss_vlb_step=0.00187, train/loss_step=0.214, global_step=105.0]Epoch 0:  45%|████▍     | 107/240 [02:00<02:29,  1.12s/it, loss=0.203, v_num=0, train/loss_simple_step=0.178, train/loss_vlb_step=0.00231, train/loss_step=0.178, global_step=106.0]Epoch 0:  45%|████▌     | 108/240 [02:01<02:27,  1.12s/it, loss=0.203, v_num=0, train/loss_simple_step=0.178, train/loss_vlb_step=0.00231, train/loss_step=0.178, global_step=106.0]Epoch 0:  45%|████▌     | 108/240 [02:01<02:28,  1.12s/it, loss=0.204, v_num=0, train/loss_simple_step=0.244, train/loss_vlb_step=0.00399, train/loss_step=0.244, global_step=107.0]Epoch 0:  45%|████▌     | 109/240 [02:02<02:26,  1.12s/it, loss=0.204, v_num=0, train/loss_simple_step=0.244, train/loss_vlb_step=0.00399, train/loss_step=0.244, global_step=107.0]Epoch 0:  45%|████▌     | 109/240 [02:02<02:26,  1.12s/it, loss=0.197, v_num=0, train/loss_simple_step=0.0991, train/loss_vlb_step=0.000513, train/loss_step=0.0991, global_step=108.0]Epoch 0:  46%|████▌     | 110/240 [02:03<02:25,  1.12s/it, loss=0.197, v_num=0, train/loss_simple_step=0.0991, train/loss_vlb_step=0.000513, train/loss_step=0.0991, global_step=108.0]Epoch 0:  46%|████▌     | 110/240 [02:03<02:25,  1.12s/it, loss=0.199, v_num=0, train/loss_simple_step=0.203, train/loss_vlb_step=0.00207, train/loss_step=0.203, global_step=109.0]   Epoch 0:  46%|████▋     | 111/240 [02:03<02:24,  1.12s/it, loss=0.199, v_num=0, train/loss_simple_step=0.203, train/loss_vlb_step=0.00207, train/loss_step=0.203, global_step=109.0]Epoch 0:  46%|████▋     | 111/240 [02:04<02:24,  1.12s/it, loss=0.202, v_num=0, train/loss_simple_step=0.228, train/loss_vlb_step=0.00191, train/loss_step=0.228, global_step=110.0]Epoch 0:  47%|████▋     | 112/240 [02:04<02:22,  1.12s/it, loss=0.202, v_num=0, train/loss_simple_step=0.228, train/loss_vlb_step=0.00191, train/loss_step=0.228, global_step=110.0]Epoch 0:  47%|████▋     | 112/240 [02:05<02:23,  1.12s/it, loss=0.203, v_num=0, train/loss_simple_step=0.240, train/loss_vlb_step=0.00155, train/loss_step=0.240, global_step=111.0]Epoch 0:  47%|████▋     | 113/240 [02:05<02:21,  1.11s/it, loss=0.203, v_num=0, train/loss_simple_step=0.240, train/loss_vlb_step=0.00155, train/loss_step=0.240, global_step=111.0]Epoch 0:  47%|████▋     | 113/240 [02:06<02:21,  1.12s/it, loss=0.207, v_num=0, train/loss_simple_step=0.274, train/loss_vlb_step=0.00269, train/loss_step=0.274, global_step=112.0]Epoch 0:  48%|████▊     | 114/240 [02:06<02:20,  1.11s/it, loss=0.207, v_num=0, train/loss_simple_step=0.274, train/loss_vlb_step=0.00269, train/loss_step=0.274, global_step=112.0]Epoch 0:  48%|████▊     | 114/240 [02:07<02:20,  1.12s/it, loss=0.209, v_num=0, train/loss_simple_step=0.201, train/loss_vlb_step=0.00307, train/loss_step=0.201, global_step=113.0]Epoch 0:  48%|████▊     | 115/240 [02:07<02:19,  1.11s/it, loss=0.209, v_num=0, train/loss_simple_step=0.201, train/loss_vlb_step=0.00307, train/loss_step=0.201, global_step=113.0]Epoch 0:  48%|████▊     | 115/240 [02:08<02:19,  1.11s/it, loss=0.21, v_num=0, train/loss_simple_step=0.203, train/loss_vlb_step=0.002, train/loss_step=0.203, global_step=114.0]   Epoch 0:  48%|████▊     | 116/240 [02:08<02:17,  1.11s/it, loss=0.21, v_num=0, train/loss_simple_step=0.203, train/loss_vlb_step=0.002, train/loss_step=0.203, global_step=114.0]Epoch 0:  48%|████▊     | 116/240 [02:09<02:18,  1.11s/it, loss=0.21, v_num=0, train/loss_simple_step=0.203, train/loss_vlb_step=0.00123, train/loss_step=0.203, global_step=115.0]Epoch 0:  49%|████▉     | 117/240 [02:09<02:16,  1.11s/it, loss=0.21, v_num=0, train/loss_simple_step=0.203, train/loss_vlb_step=0.00123, train/loss_step=0.203, global_step=115.0]Epoch 0:  49%|████▉     | 117/240 [02:10<02:16,  1.11s/it, loss=0.207, v_num=0, train/loss_simple_step=0.160, train/loss_vlb_step=0.00191, train/loss_step=0.160, global_step=116.0]Epoch 0:  49%|████▉     | 118/240 [02:10<02:15,  1.11s/it, loss=0.207, v_num=0, train/loss_simple_step=0.160, train/loss_vlb_step=0.00191, train/loss_step=0.160, global_step=116.0]Epoch 0:  49%|████▉     | 118/240 [02:11<02:15,  1.11s/it, loss=0.211, v_num=0, train/loss_simple_step=0.246, train/loss_vlb_step=0.00238, train/loss_step=0.246, global_step=117.0]Epoch 0:  50%|████▉     | 119/240 [02:11<02:14,  1.11s/it, loss=0.211, v_num=0, train/loss_simple_step=0.246, train/loss_vlb_step=0.00238, train/loss_step=0.246, global_step=117.0]Epoch 0:  50%|████▉     | 119/240 [02:12<02:14,  1.11s/it, loss=0.208, v_num=0, train/loss_simple_step=0.190, train/loss_vlb_step=0.00245, train/loss_step=0.190, global_step=118.0]Epoch 0:  50%|█████     | 120/240 [02:12<02:12,  1.11s/it, loss=0.208, v_num=0, train/loss_simple_step=0.190, train/loss_vlb_step=0.00245, train/loss_step=0.190, global_step=118.0]Epoch 0:  50%|█████     | 120/240 [02:13<02:13,  1.11s/it, loss=0.205, v_num=0, train/loss_simple_step=0.135, train/loss_vlb_step=0.00163, train/loss_step=0.135, global_step=119.0]Epoch 0:  50%|█████     | 121/240 [02:13<02:11,  1.11s/it, loss=0.205, v_num=0, train/loss_simple_step=0.135, train/loss_vlb_step=0.00163, train/loss_step=0.135, global_step=119.0]Epoch 0:  50%|█████     | 121/240 [02:14<02:11,  1.11s/it, loss=0.208, v_num=0, train/loss_simple_step=0.215, train/loss_vlb_step=0.00213, train/loss_step=0.215, global_step=120.0]Epoch 0:  51%|█████     | 122/240 [02:14<02:10,  1.11s/it, loss=0.208, v_num=0, train/loss_simple_step=0.215, train/loss_vlb_step=0.00213, train/loss_step=0.215, global_step=120.0]Epoch 0:  51%|█████     | 122/240 [02:15<02:10,  1.11s/it, loss=0.206, v_num=0, train/loss_simple_step=0.223, train/loss_vlb_step=0.00446, train/loss_step=0.223, global_step=121.0]Epoch 0:  51%|█████▏    | 123/240 [02:15<02:09,  1.10s/it, loss=0.206, v_num=0, train/loss_simple_step=0.223, train/loss_vlb_step=0.00446, train/loss_step=0.223, global_step=121.0]Epoch 0:  51%|█████▏    | 123/240 [02:16<02:09,  1.11s/it, loss=0.203, v_num=0, train/loss_simple_step=0.164, train/loss_vlb_step=0.00185, train/loss_step=0.164, global_step=122.0]Epoch 0:  52%|█████▏    | 124/240 [02:16<02:08,  1.10s/it, loss=0.203, v_num=0, train/loss_simple_step=0.164, train/loss_vlb_step=0.00185, train/loss_step=0.164, global_step=122.0]Epoch 0:  52%|█████▏    | 124/240 [02:17<02:08,  1.11s/it, loss=0.205, v_num=0, train/loss_simple_step=0.231, train/loss_vlb_step=0.00222, train/loss_step=0.231, global_step=123.0]Epoch 0:  52%|█████▏    | 125/240 [02:17<02:06,  1.10s/it, loss=0.205, v_num=0, train/loss_simple_step=0.231, train/loss_vlb_step=0.00222, train/loss_step=0.231, global_step=123.0]Epoch 0:  52%|█████▏    | 125/240 [02:18<02:07,  1.10s/it, loss=0.201, v_num=0, train/loss_simple_step=0.168, train/loss_vlb_step=0.0021, train/loss_step=0.168, global_step=124.0] Epoch 0:  52%|█████▎    | 126/240 [02:18<02:05,  1.10s/it, loss=0.201, v_num=0, train/loss_simple_step=0.168, train/loss_vlb_step=0.0021, train/loss_step=0.168, global_step=124.0]Epoch 0:  52%|█████▎    | 126/240 [02:19<02:05,  1.10s/it, loss=0.205, v_num=0, train/loss_simple_step=0.294, train/loss_vlb_step=0.00448, train/loss_step=0.294, global_step=125.0]Epoch 0:  53%|█████▎    | 127/240 [02:19<02:04,  1.10s/it, loss=0.205, v_num=0, train/loss_simple_step=0.294, train/loss_vlb_step=0.00448, train/loss_step=0.294, global_step=125.0]Epoch 0:  53%|█████▎    | 127/240 [02:20<02:04,  1.10s/it, loss=0.206, v_num=0, train/loss_simple_step=0.189, train/loss_vlb_step=0.0013, train/loss_step=0.189, global_step=126.0] Epoch 0:  53%|█████▎    | 128/240 [02:20<02:03,  1.10s/it, loss=0.206, v_num=0, train/loss_simple_step=0.189, train/loss_vlb_step=0.0013, train/loss_step=0.189, global_step=126.0]Epoch 0:  53%|█████▎    | 128/240 [02:21<02:03,  1.10s/it, loss=0.204, v_num=0, train/loss_simple_step=0.215, train/loss_vlb_step=0.0016, train/loss_step=0.215, global_step=127.0]Epoch 0:  54%|█████▍    | 129/240 [02:21<02:02,  1.10s/it, loss=0.204, v_num=0, train/loss_simple_step=0.215, train/loss_vlb_step=0.0016, train/loss_step=0.215, global_step=127.0]Epoch 0:  54%|█████▍    | 129/240 [02:22<02:02,  1.10s/it, loss=0.207, v_num=0, train/loss_simple_step=0.162, train/loss_vlb_step=0.00131, train/loss_step=0.162, global_step=128.0]Epoch 0:  54%|█████▍    | 130/240 [02:22<02:00,  1.10s/it, loss=0.207, v_num=0, train/loss_simple_step=0.162, train/loss_vlb_step=0.00131, train/loss_step=0.162, global_step=128.0]Epoch 0:  54%|█████▍    | 130/240 [02:23<02:01,  1.10s/it, loss=0.206, v_num=0, train/loss_simple_step=0.176, train/loss_vlb_step=0.00116, train/loss_step=0.176, global_step=129.0]Epoch 0:  55%|█████▍    | 131/240 [02:23<01:59,  1.10s/it, loss=0.206, v_num=0, train/loss_simple_step=0.176, train/loss_vlb_step=0.00116, train/loss_step=0.176, global_step=129.0]Epoch 0:  55%|█████▍    | 131/240 [02:24<01:59,  1.10s/it, loss=0.202, v_num=0, train/loss_simple_step=0.155, train/loss_vlb_step=0.00108, train/loss_step=0.155, global_step=130.0]Epoch 0:  55%|█████▌    | 132/240 [02:24<01:58,  1.10s/it, loss=0.202, v_num=0, train/loss_simple_step=0.155, train/loss_vlb_step=0.00108, train/loss_step=0.155, global_step=130.0]Epoch 0:  55%|█████▌    | 132/240 [02:25<01:58,  1.10s/it, loss=0.201, v_num=0, train/loss_simple_step=0.209, train/loss_vlb_step=0.00141, train/loss_step=0.209, global_step=131.0]Epoch 0:  55%|█████▌    | 133/240 [02:25<01:57,  1.10s/it, loss=0.201, v_num=0, train/loss_simple_step=0.209, train/loss_vlb_step=0.00141, train/loss_step=0.209, global_step=131.0]Epoch 0:  55%|█████▌    | 133/240 [02:26<01:57,  1.10s/it, loss=0.199, v_num=0, train/loss_simple_step=0.247, train/loss_vlb_step=0.0027, train/loss_step=0.247, global_step=132.0] Epoch 0:  56%|█████▌    | 134/240 [02:26<01:56,  1.10s/it, loss=0.199, v_num=0, train/loss_simple_step=0.247, train/loss_vlb_step=0.0027, train/loss_step=0.247, global_step=132.0]Epoch 0:  56%|█████▌    | 134/240 [02:26<01:56,  1.10s/it, loss=0.2, v_num=0, train/loss_simple_step=0.212, train/loss_vlb_step=0.00187, train/loss_step=0.212, global_step=133.0] Epoch 0:  56%|█████▋    | 135/240 [02:27<01:54,  1.09s/it, loss=0.2, v_num=0, train/loss_simple_step=0.212, train/loss_vlb_step=0.00187, train/loss_step=0.212, global_step=133.0]Epoch 0:  56%|█████▋    | 135/240 [02:27<01:55,  1.10s/it, loss=0.196, v_num=0, train/loss_simple_step=0.134, train/loss_vlb_step=0.000748, train/loss_step=0.134, global_step=134.0]Epoch 0:  57%|█████▋    | 136/240 [02:28<01:53,  1.09s/it, loss=0.196, v_num=0, train/loss_simple_step=0.134, train/loss_vlb_step=0.000748, train/loss_step=0.134, global_step=134.0]Epoch 0:  57%|█████▋    | 136/240 [02:28<01:53,  1.10s/it, loss=0.195, v_num=0, train/loss_simple_step=0.178, train/loss_vlb_step=0.00191, train/loss_step=0.178, global_step=135.0] Epoch 0:  57%|█████▋    | 137/240 [02:29<01:52,  1.09s/it, loss=0.195, v_num=0, train/loss_simple_step=0.178, train/loss_vlb_step=0.00191, train/loss_step=0.178, global_step=135.0]Epoch 0:  57%|█████▋    | 137/240 [02:29<01:52,  1.09s/it, loss=0.196, v_num=0, train/loss_simple_step=0.188, train/loss_vlb_step=0.00115, train/loss_step=0.188, global_step=136.0]Epoch 0:  57%|█████▊    | 138/240 [02:30<01:51,  1.09s/it, loss=0.196, v_num=0, train/loss_simple_step=0.188, train/loss_vlb_step=0.00115, train/loss_step=0.188, global_step=136.0]Epoch 0:  57%|█████▊    | 138/240 [02:30<01:51,  1.09s/it, loss=0.198, v_num=0, train/loss_simple_step=0.269, train/loss_vlb_step=0.0157, train/loss_step=0.269, global_step=137.0] Epoch 0:  58%|█████▊    | 139/240 [02:31<01:50,  1.09s/it, loss=0.198, v_num=0, train/loss_simple_step=0.269, train/loss_vlb_step=0.0157, train/loss_step=0.269, global_step=137.0]Epoch 0:  58%|█████▊    | 139/240 [02:31<01:50,  1.09s/it, loss=0.2, v_num=0, train/loss_simple_step=0.233, train/loss_vlb_step=0.00926, train/loss_step=0.233, global_step=138.0] Epoch 0:  58%|█████▊    | 140/240 [02:32<01:49,  1.09s/it, loss=0.2, v_num=0, train/loss_simple_step=0.233, train/loss_vlb_step=0.00926, train/loss_step=0.233, global_step=138.0]Epoch 0:  58%|█████▊    | 140/240 [02:32<01:49,  1.09s/it, loss=0.204, v_num=0, train/loss_simple_step=0.216, train/loss_vlb_step=0.00122, train/loss_step=0.216, global_step=139.0]Epoch 0:  59%|█████▉    | 141/240 [02:33<01:47,  1.09s/it, loss=0.204, v_num=0, train/loss_simple_step=0.216, train/loss_vlb_step=0.00122, train/loss_step=0.216, global_step=139.0]Epoch 0:  59%|█████▉    | 141/240 [02:33<01:48,  1.09s/it, loss=0.204, v_num=0, train/loss_simple_step=0.226, train/loss_vlb_step=0.00209, train/loss_step=0.226, global_step=140.0]Epoch 0:  59%|█████▉    | 142/240 [02:34<01:46,  1.09s/it, loss=0.204, v_num=0, train/loss_simple_step=0.226, train/loss_vlb_step=0.00209, train/loss_step=0.226, global_step=140.0]Epoch 0:  59%|█████▉    | 142/240 [02:34<01:46,  1.09s/it, loss=0.202, v_num=0, train/loss_simple_step=0.182, train/loss_vlb_step=0.00131, train/loss_step=0.182, global_step=141.0]Epoch 0:  60%|█████▉    | 143/240 [02:35<01:45,  1.09s/it, loss=0.202, v_num=0, train/loss_simple_step=0.182, train/loss_vlb_step=0.00131, train/loss_step=0.182, global_step=141.0]Epoch 0:  60%|█████▉    | 143/240 [02:35<01:45,  1.09s/it, loss=0.206, v_num=0, train/loss_simple_step=0.228, train/loss_vlb_step=0.00194, train/loss_step=0.228, global_step=142.0]Epoch 0:  60%|██████    | 144/240 [02:36<01:44,  1.09s/it, loss=0.206, v_num=0, train/loss_simple_step=0.228, train/loss_vlb_step=0.00194, train/loss_step=0.228, global_step=142.0]Epoch 0:  60%|██████    | 144/240 [02:36<01:44,  1.09s/it, loss=0.205, v_num=0, train/loss_simple_step=0.217, train/loss_vlb_step=0.014, train/loss_step=0.217, global_step=143.0]  Epoch 0:  60%|██████    | 145/240 [02:37<01:43,  1.09s/it, loss=0.205, v_num=0, train/loss_simple_step=0.217, train/loss_vlb_step=0.014, train/loss_step=0.217, global_step=143.0]Epoch 0:  60%|██████    | 145/240 [02:37<01:43,  1.09s/it, loss=0.208, v_num=0, train/loss_simple_step=0.225, train/loss_vlb_step=0.00154, train/loss_step=0.225, global_step=144.0]Epoch 0:  61%|██████    | 146/240 [02:38<01:42,  1.09s/it, loss=0.208, v_num=0, train/loss_simple_step=0.225, train/loss_vlb_step=0.00154, train/loss_step=0.225, global_step=144.0]Epoch 0:  61%|██████    | 146/240 [02:38<01:42,  1.09s/it, loss=0.206, v_num=0, train/loss_simple_step=0.260, train/loss_vlb_step=0.00237, train/loss_step=0.260, global_step=145.0]Epoch 0:  61%|██████▏   | 147/240 [02:39<01:41,  1.09s/it, loss=0.206, v_num=0, train/loss_simple_step=0.260, train/loss_vlb_step=0.00237, train/loss_step=0.260, global_step=145.0]Epoch 0:  61%|██████▏   | 147/240 [02:39<01:41,  1.09s/it, loss=0.206, v_num=0, train/loss_simple_step=0.193, train/loss_vlb_step=0.00182, train/loss_step=0.193, global_step=146.0]Epoch 0:  62%|██████▏   | 148/240 [02:40<01:39,  1.09s/it, loss=0.206, v_num=0, train/loss_simple_step=0.193, train/loss_vlb_step=0.00182, train/loss_step=0.193, global_step=146.0]Epoch 0:  62%|██████▏   | 148/240 [02:40<01:39,  1.09s/it, loss=0.205, v_num=0, train/loss_simple_step=0.201, train/loss_vlb_step=0.00694, train/loss_step=0.201, global_step=147.0]Epoch 0:  62%|██████▏   | 149/240 [02:41<01:38,  1.08s/it, loss=0.205, v_num=0, train/loss_simple_step=0.201, train/loss_vlb_step=0.00694, train/loss_step=0.201, global_step=147.0]Epoch 0:  62%|██████▏   | 149/240 [02:41<01:38,  1.09s/it, loss=0.205, v_num=0, train/loss_simple_step=0.151, train/loss_vlb_step=0.00108, train/loss_step=0.151, global_step=148.0]Epoch 0:  62%|██████▎   | 150/240 [02:42<01:37,  1.08s/it, loss=0.205, v_num=0, train/loss_simple_step=0.151, train/loss_vlb_step=0.00108, train/loss_step=0.151, global_step=148.0]Epoch 0:  62%|██████▎   | 150/240 [02:42<01:37,  1.09s/it, loss=0.207, v_num=0, train/loss_simple_step=0.226, train/loss_vlb_step=0.005, train/loss_step=0.226, global_step=149.0]  Epoch 0:  63%|██████▎   | 151/240 [02:43<01:36,  1.08s/it, loss=0.207, v_num=0, train/loss_simple_step=0.226, train/loss_vlb_step=0.005, train/loss_step=0.226, global_step=149.0]Epoch 0:  63%|██████▎   | 151/240 [02:43<01:36,  1.08s/it, loss=0.209, v_num=0, train/loss_simple_step=0.185, train/loss_vlb_step=0.00177, train/loss_step=0.185, global_step=150.0]Epoch 0:  63%|██████▎   | 152/240 [02:44<01:35,  1.08s/it, loss=0.209, v_num=0, train/loss_simple_step=0.185, train/loss_vlb_step=0.00177, train/loss_step=0.185, global_step=150.0]Epoch 0:  63%|██████▎   | 152/240 [02:44<01:35,  1.08s/it, loss=0.209, v_num=0, train/loss_simple_step=0.212, train/loss_vlb_step=0.00187, train/loss_step=0.212, global_step=151.0]Epoch 0:  64%|██████▍   | 153/240 [02:45<01:34,  1.08s/it, loss=0.209, v_num=0, train/loss_simple_step=0.212, train/loss_vlb_step=0.00187, train/loss_step=0.212, global_step=151.0]Epoch 0:  64%|██████▍   | 153/240 [02:45<01:34,  1.08s/it, loss=0.207, v_num=0, train/loss_simple_step=0.214, train/loss_vlb_step=0.00278, train/loss_step=0.214, global_step=152.0]Epoch 0:  64%|██████▍   | 154/240 [02:46<01:33,  1.08s/it, loss=0.207, v_num=0, train/loss_simple_step=0.214, train/loss_vlb_step=0.00278, train/loss_step=0.214, global_step=152.0]Epoch 0:  64%|██████▍   | 154/240 [02:46<01:33,  1.08s/it, loss=0.21, v_num=0, train/loss_simple_step=0.259, train/loss_vlb_step=0.00399, train/loss_step=0.259, global_step=153.0] Epoch 0:  65%|██████▍   | 155/240 [02:47<01:31,  1.08s/it, loss=0.21, v_num=0, train/loss_simple_step=0.259, train/loss_vlb_step=0.00399, train/loss_step=0.259, global_step=153.0]Epoch 0:  65%|██████▍   | 155/240 [02:47<01:32,  1.08s/it, loss=0.217, v_num=0, train/loss_simple_step=0.271, train/loss_vlb_step=0.0141, train/loss_step=0.271, global_step=154.0]Epoch 0:  65%|██████▌   | 156/240 [02:48<01:30,  1.08s/it, loss=0.217, v_num=0, train/loss_simple_step=0.271, train/loss_vlb_step=0.0141, train/loss_step=0.271, global_step=154.0]Epoch 0:  65%|██████▌   | 156/240 [02:48<01:30,  1.08s/it, loss=0.214, v_num=0, train/loss_simple_step=0.130, train/loss_vlb_step=0.000906, train/loss_step=0.130, global_step=155.0]Epoch 0:  65%|██████▌   | 157/240 [02:49<01:29,  1.08s/it, loss=0.214, v_num=0, train/loss_simple_step=0.130, train/loss_vlb_step=0.000906, train/loss_step=0.130, global_step=155.0]Epoch 0:  65%|██████▌   | 157/240 [02:49<01:29,  1.08s/it, loss=0.212, v_num=0, train/loss_simple_step=0.147, train/loss_vlb_step=0.000755, train/loss_step=0.147, global_step=156.0]Epoch 0:  66%|██████▌   | 158/240 [02:50<01:28,  1.08s/it, loss=0.212, v_num=0, train/loss_simple_step=0.147, train/loss_vlb_step=0.000755, train/loss_step=0.147, global_step=156.0]Epoch 0:  66%|██████▌   | 158/240 [02:50<01:28,  1.08s/it, loss=0.21, v_num=0, train/loss_simple_step=0.227, train/loss_vlb_step=0.00268, train/loss_step=0.227, global_step=157.0]  Epoch 0:  66%|██████▋   | 159/240 [02:51<01:27,  1.08s/it, loss=0.21, v_num=0, train/loss_simple_step=0.227, train/loss_vlb_step=0.00268, train/loss_step=0.227, global_step=157.0]Epoch 0:  66%|██████▋   | 159/240 [02:51<01:27,  1.08s/it, loss=0.208, v_num=0, train/loss_simple_step=0.191, train/loss_vlb_step=0.00109, train/loss_step=0.191, global_step=158.0]Epoch 0:  67%|██████▋   | 160/240 [02:52<01:26,  1.08s/it, loss=0.208, v_num=0, train/loss_simple_step=0.191, train/loss_vlb_step=0.00109, train/loss_step=0.191, global_step=158.0]Epoch 0:  67%|██████▋   | 160/240 [02:52<01:26,  1.08s/it, loss=0.206, v_num=0, train/loss_simple_step=0.176, train/loss_vlb_step=0.000975, train/loss_step=0.176, global_step=159.0]Epoch 0:  67%|██████▋   | 161/240 [02:53<01:25,  1.08s/it, loss=0.206, v_num=0, train/loss_simple_step=0.176, train/loss_vlb_step=0.000975, train/loss_step=0.176, global_step=159.0]Epoch 0:  67%|██████▋   | 161/240 [02:53<01:25,  1.08s/it, loss=0.202, v_num=0, train/loss_simple_step=0.156, train/loss_vlb_step=0.000805, train/loss_step=0.156, global_step=160.0]Epoch 0:  68%|██████▊   | 162/240 [02:54<01:24,  1.08s/it, loss=0.202, v_num=0, train/loss_simple_step=0.156, train/loss_vlb_step=0.000805, train/loss_step=0.156, global_step=160.0]Epoch 0:  68%|██████▊   | 162/240 [02:54<01:24,  1.08s/it, loss=0.205, v_num=0, train/loss_simple_step=0.241, train/loss_vlb_step=0.0021, train/loss_step=0.241, global_step=161.0]  Epoch 0:  68%|██████▊   | 163/240 [02:55<01:22,  1.08s/it, loss=0.205, v_num=0, train/loss_simple_step=0.241, train/loss_vlb_step=0.0021, train/loss_step=0.241, global_step=161.0]Epoch 0:  68%|██████▊   | 163/240 [02:55<01:22,  1.08s/it, loss=0.206, v_num=0, train/loss_simple_step=0.233, train/loss_vlb_step=0.00327, train/loss_step=0.233, global_step=162.0]Epoch 0:  68%|██████▊   | 164/240 [02:56<01:21,  1.08s/it, loss=0.206, v_num=0, train/loss_simple_step=0.233, train/loss_vlb_step=0.00327, train/loss_step=0.233, global_step=162.0]Epoch 0:  68%|██████▊   | 164/240 [02:56<01:21,  1.08s/it, loss=0.205, v_num=0, train/loss_simple_step=0.213, train/loss_vlb_step=0.0015, train/loss_step=0.213, global_step=163.0] Epoch 0:  69%|██████▉   | 165/240 [02:57<01:20,  1.08s/it, loss=0.205, v_num=0, train/loss_simple_step=0.213, train/loss_vlb_step=0.0015, train/loss_step=0.213, global_step=163.0]Epoch 0:  69%|██████▉   | 165/240 [02:57<01:20,  1.08s/it, loss=0.206, v_num=0, train/loss_simple_step=0.231, train/loss_vlb_step=0.00205, train/loss_step=0.231, global_step=164.0]Epoch 0:  69%|██████▉   | 166/240 [02:58<01:19,  1.07s/it, loss=0.206, v_num=0, train/loss_simple_step=0.231, train/loss_vlb_step=0.00205, train/loss_step=0.231, global_step=164.0]Epoch 0:  69%|██████▉   | 166/240 [02:58<01:19,  1.08s/it, loss=0.201, v_num=0, train/loss_simple_step=0.162, train/loss_vlb_step=0.00179, train/loss_step=0.162, global_step=165.0]Epoch 0:  70%|██████▉   | 167/240 [02:59<01:18,  1.07s/it, loss=0.201, v_num=0, train/loss_simple_step=0.162, train/loss_vlb_step=0.00179, train/loss_step=0.162, global_step=165.0]Epoch 0:  70%|██████▉   | 167/240 [02:59<01:18,  1.08s/it, loss=0.202, v_num=0, train/loss_simple_step=0.219, train/loss_vlb_step=0.00187, train/loss_step=0.219, global_step=166.0]Epoch 0:  70%|███████   | 168/240 [03:00<01:17,  1.07s/it, loss=0.202, v_num=0, train/loss_simple_step=0.219, train/loss_vlb_step=0.00187, train/loss_step=0.219, global_step=166.0]Epoch 0:  70%|███████   | 168/240 [03:00<01:17,  1.08s/it, loss=0.204, v_num=0, train/loss_simple_step=0.239, train/loss_vlb_step=0.0176, train/loss_step=0.239, global_step=167.0] Epoch 0:  70%|███████   | 169/240 [03:01<01:16,  1.07s/it, loss=0.204, v_num=0, train/loss_simple_step=0.239, train/loss_vlb_step=0.0176, train/loss_step=0.239, global_step=167.0]Epoch 0:  70%|███████   | 169/240 [03:01<01:16,  1.07s/it, loss=0.205, v_num=0, train/loss_simple_step=0.178, train/loss_vlb_step=0.00152, train/loss_step=0.178, global_step=168.0]Epoch 0:  71%|███████   | 170/240 [03:02<01:15,  1.07s/it, loss=0.205, v_num=0, train/loss_simple_step=0.178, train/loss_vlb_step=0.00152, train/loss_step=0.178, global_step=168.0]Epoch 0:  71%|███████   | 170/240 [03:02<01:15,  1.07s/it, loss=0.204, v_num=0, train/loss_simple_step=0.196, train/loss_vlb_step=0.00326, train/loss_step=0.196, global_step=169.0]Epoch 0:  71%|███████▏  | 171/240 [03:03<01:14,  1.07s/it, loss=0.204, v_num=0, train/loss_simple_step=0.196, train/loss_vlb_step=0.00326, train/loss_step=0.196, global_step=169.0]Epoch 0:  71%|███████▏  | 171/240 [03:03<01:14,  1.07s/it, loss=0.204, v_num=0, train/loss_simple_step=0.183, train/loss_vlb_step=0.000939, train/loss_step=0.183, global_step=170.0]Epoch 0:  72%|███████▏  | 172/240 [03:04<01:12,  1.07s/it, loss=0.204, v_num=0, train/loss_simple_step=0.183, train/loss_vlb_step=0.000939, train/loss_step=0.183, global_step=170.0]Epoch 0:  72%|███████▏  | 172/240 [03:04<01:12,  1.07s/it, loss=0.205, v_num=0, train/loss_simple_step=0.227, train/loss_vlb_step=0.00449, train/loss_step=0.227, global_step=171.0] Epoch 0:  72%|███████▏  | 173/240 [03:05<01:11,  1.07s/it, loss=0.205, v_num=0, train/loss_simple_step=0.227, train/loss_vlb_step=0.00449, train/loss_step=0.227, global_step=171.0]Epoch 0:  72%|███████▏  | 173/240 [03:05<01:11,  1.07s/it, loss=0.205, v_num=0, train/loss_simple_step=0.215, train/loss_vlb_step=0.00252, train/loss_step=0.215, global_step=172.0]Epoch 0:  72%|███████▎  | 174/240 [03:06<01:10,  1.07s/it, loss=0.205, v_num=0, train/loss_simple_step=0.215, train/loss_vlb_step=0.00252, train/loss_step=0.215, global_step=172.0]Epoch 0:  72%|███████▎  | 174/240 [03:06<01:10,  1.07s/it, loss=0.204, v_num=0, train/loss_simple_step=0.251, train/loss_vlb_step=0.00824, train/loss_step=0.251, global_step=173.0]Epoch 0:  73%|███████▎  | 175/240 [03:07<01:09,  1.07s/it, loss=0.204, v_num=0, train/loss_simple_step=0.251, train/loss_vlb_step=0.00824, train/loss_step=0.251, global_step=173.0]Epoch 0:  73%|███████▎  | 175/240 [03:07<01:09,  1.07s/it, loss=0.201, v_num=0, train/loss_simple_step=0.213, train/loss_vlb_step=0.00172, train/loss_step=0.213, global_step=174.0]Epoch 0:  73%|███████▎  | 176/240 [03:08<01:08,  1.07s/it, loss=0.201, v_num=0, train/loss_simple_step=0.213, train/loss_vlb_step=0.00172, train/loss_step=0.213, global_step=174.0]Epoch 0:  73%|███████▎  | 176/240 [03:08<01:08,  1.07s/it, loss=0.202, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000702, train/loss_step=0.144, global_step=175.0]Epoch 0:  74%|███████▍  | 177/240 [03:09<01:07,  1.07s/it, loss=0.202, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000702, train/loss_step=0.144, global_step=175.0]Epoch 0:  74%|███████▍  | 177/240 [03:09<01:07,  1.07s/it, loss=0.207, v_num=0, train/loss_simple_step=0.246, train/loss_vlb_step=0.00526, train/loss_step=0.246, global_step=176.0] Epoch 0:  74%|███████▍  | 178/240 [03:10<01:06,  1.07s/it, loss=0.207, v_num=0, train/loss_simple_step=0.246, train/loss_vlb_step=0.00526, train/loss_step=0.246, global_step=176.0]Epoch 0:  74%|███████▍  | 178/240 [03:10<01:06,  1.07s/it, loss=0.207, v_num=0, train/loss_simple_step=0.235, train/loss_vlb_step=0.0016, train/loss_step=0.235, global_step=177.0] Epoch 0:  75%|███████▍  | 179/240 [03:11<01:05,  1.07s/it, loss=0.207, v_num=0, train/loss_simple_step=0.235, train/loss_vlb_step=0.0016, train/loss_step=0.235, global_step=177.0]Epoch 0:  75%|███████▍  | 179/240 [03:11<01:05,  1.07s/it, loss=0.209, v_num=0, train/loss_simple_step=0.227, train/loss_vlb_step=0.00323, train/loss_step=0.227, global_step=178.0]Epoch 0:  75%|███████▌  | 180/240 [03:12<01:04,  1.07s/it, loss=0.209, v_num=0, train/loss_simple_step=0.227, train/loss_vlb_step=0.00323, train/loss_step=0.227, global_step=178.0]Epoch 0:  75%|███████▌  | 180/240 [03:12<01:04,  1.07s/it, loss=0.209, v_num=0, train/loss_simple_step=0.169, train/loss_vlb_step=0.00194, train/loss_step=0.169, global_step=179.0]Epoch 0:  75%|███████▌  | 181/240 [03:13<01:02,  1.07s/it, loss=0.209, v_num=0, train/loss_simple_step=0.169, train/loss_vlb_step=0.00194, train/loss_step=0.169, global_step=179.0]Epoch 0:  75%|███████▌  | 181/240 [03:13<01:03,  1.07s/it, loss=0.215, v_num=0, train/loss_simple_step=0.285, train/loss_vlb_step=0.00626, train/loss_step=0.285, global_step=180.0]Epoch 0:  76%|███████▌  | 182/240 [03:14<01:01,  1.07s/it, loss=0.215, v_num=0, train/loss_simple_step=0.285, train/loss_vlb_step=0.00626, train/loss_step=0.285, global_step=180.0]Epoch 0:  76%|███████▌  | 182/240 [03:14<01:01,  1.07s/it, loss=0.217, v_num=0, train/loss_simple_step=0.279, train/loss_vlb_step=0.0112, train/loss_step=0.279, global_step=181.0] Epoch 0:  76%|███████▋  | 183/240 [03:15<01:00,  1.07s/it, loss=0.217, v_num=0, train/loss_simple_step=0.279, train/loss_vlb_step=0.0112, train/loss_step=0.279, global_step=181.0]Epoch 0:  76%|███████▋  | 183/240 [03:15<01:00,  1.07s/it, loss=0.216, v_num=0, train/loss_simple_step=0.200, train/loss_vlb_step=0.0024, train/loss_step=0.200, global_step=182.0]Epoch 0:  77%|███████▋  | 184/240 [03:16<00:59,  1.07s/it, loss=0.216, v_num=0, train/loss_simple_step=0.200, train/loss_vlb_step=0.0024, train/loss_step=0.200, global_step=182.0]Epoch 0:  77%|███████▋  | 184/240 [03:16<00:59,  1.07s/it, loss=0.212, v_num=0, train/loss_simple_step=0.150, train/loss_vlb_step=0.00106, train/loss_step=0.150, global_step=183.0]Epoch 0:  77%|███████▋  | 185/240 [03:17<00:58,  1.07s/it, loss=0.212, v_num=0, train/loss_simple_step=0.150, train/loss_vlb_step=0.00106, train/loss_step=0.150, global_step=183.0]Epoch 0:  77%|███████▋  | 185/240 [03:17<00:58,  1.07s/it, loss=0.21, v_num=0, train/loss_simple_step=0.173, train/loss_vlb_step=0.000904, train/loss_step=0.173, global_step=184.0]Epoch 0:  78%|███████▊  | 186/240 [03:18<00:57,  1.07s/it, loss=0.21, v_num=0, train/loss_simple_step=0.173, train/loss_vlb_step=0.000904, train/loss_step=0.173, global_step=184.0]Epoch 0:  78%|███████▊  | 186/240 [03:18<00:57,  1.07s/it, loss=0.216, v_num=0, train/loss_simple_step=0.284, train/loss_vlb_step=0.016, train/loss_step=0.284, global_step=185.0]  Epoch 0:  78%|███████▊  | 187/240 [03:19<00:56,  1.07s/it, loss=0.216, v_num=0, train/loss_simple_step=0.284, train/loss_vlb_step=0.016, train/loss_step=0.284, global_step=185.0]Epoch 0:  78%|███████▊  | 187/240 [03:19<00:56,  1.07s/it, loss=0.215, v_num=0, train/loss_simple_step=0.205, train/loss_vlb_step=0.00255, train/loss_step=0.205, global_step=186.0]Epoch 0:  78%|███████▊  | 188/240 [03:20<00:55,  1.06s/it, loss=0.215, v_num=0, train/loss_simple_step=0.205, train/loss_vlb_step=0.00255, train/loss_step=0.205, global_step=186.0]Epoch 0:  78%|███████▊  | 188/240 [03:20<00:55,  1.07s/it, loss=0.214, v_num=0, train/loss_simple_step=0.228, train/loss_vlb_step=0.00556, train/loss_step=0.228, global_step=187.0]Epoch 0:  79%|███████▉  | 189/240 [03:21<00:54,  1.06s/it, loss=0.214, v_num=0, train/loss_simple_step=0.228, train/loss_vlb_step=0.00556, train/loss_step=0.228, global_step=187.0]Epoch 0:  79%|███████▉  | 189/240 [03:21<00:54,  1.07s/it, loss=0.217, v_num=0, train/loss_simple_step=0.230, train/loss_vlb_step=0.00172, train/loss_step=0.230, global_step=188.0]Epoch 0:  79%|███████▉  | 190/240 [03:22<00:53,  1.06s/it, loss=0.217, v_num=0, train/loss_simple_step=0.230, train/loss_vlb_step=0.00172, train/loss_step=0.230, global_step=188.0]Epoch 0:  79%|███████▉  | 190/240 [03:22<00:53,  1.07s/it, loss=0.218, v_num=0, train/loss_simple_step=0.217, train/loss_vlb_step=0.0136, train/loss_step=0.217, global_step=189.0] Epoch 0:  80%|███████▉  | 191/240 [03:23<00:52,  1.06s/it, loss=0.218, v_num=0, train/loss_simple_step=0.217, train/loss_vlb_step=0.0136, train/loss_step=0.217, global_step=189.0]Epoch 0:  80%|███████▉  | 191/240 [03:23<00:52,  1.06s/it, loss=0.216, v_num=0, train/loss_simple_step=0.148, train/loss_vlb_step=0.00102, train/loss_step=0.148, global_step=190.0]Epoch 0:  80%|████████  | 192/240 [03:24<00:51,  1.06s/it, loss=0.216, v_num=0, train/loss_simple_step=0.148, train/loss_vlb_step=0.00102, train/loss_step=0.148, global_step=190.0]Epoch 0:  80%|████████  | 192/240 [03:24<00:51,  1.06s/it, loss=0.216, v_num=0, train/loss_simple_step=0.227, train/loss_vlb_step=0.00245, train/loss_step=0.227, global_step=191.0]Epoch 0:  80%|████████  | 193/240 [03:25<00:49,  1.06s/it, loss=0.216, v_num=0, train/loss_simple_step=0.227, train/loss_vlb_step=0.00245, train/loss_step=0.227, global_step=191.0]Epoch 0:  80%|████████  | 193/240 [03:25<00:50,  1.06s/it, loss=0.213, v_num=0, train/loss_simple_step=0.160, train/loss_vlb_step=0.000824, train/loss_step=0.160, global_step=192.0]Epoch 0:  81%|████████  | 194/240 [03:26<00:48,  1.06s/it, loss=0.213, v_num=0, train/loss_simple_step=0.160, train/loss_vlb_step=0.000824, train/loss_step=0.160, global_step=192.0]Epoch 0:  81%|████████  | 194/240 [03:26<00:48,  1.06s/it, loss=0.209, v_num=0, train/loss_simple_step=0.170, train/loss_vlb_step=0.000986, train/loss_step=0.170, global_step=193.0]Epoch 0:  81%|████████▏ | 195/240 [03:27<00:47,  1.06s/it, loss=0.209, v_num=0, train/loss_simple_step=0.170, train/loss_vlb_step=0.000986, train/loss_step=0.170, global_step=193.0]Epoch 0:  81%|████████▏ | 195/240 [03:27<00:47,  1.06s/it, loss=0.208, v_num=0, train/loss_simple_step=0.179, train/loss_vlb_step=0.002, train/loss_step=0.179, global_step=194.0]   Epoch 0:  82%|████████▏ | 196/240 [03:28<00:46,  1.06s/it, loss=0.208, v_num=0, train/loss_simple_step=0.179, train/loss_vlb_step=0.002, train/loss_step=0.179, global_step=194.0]Epoch 0:  82%|████████▏ | 196/240 [03:28<00:46,  1.06s/it, loss=0.212, v_num=0, train/loss_simple_step=0.239, train/loss_vlb_step=0.0135, train/loss_step=0.239, global_step=195.0]Epoch 0:  82%|████████▏ | 197/240 [03:29<00:45,  1.06s/it, loss=0.212, v_num=0, train/loss_simple_step=0.239, train/loss_vlb_step=0.0135, train/loss_step=0.239, global_step=195.0]Epoch 0:  82%|████████▏ | 197/240 [03:29<00:45,  1.06s/it, loss=0.21, v_num=0, train/loss_simple_step=0.190, train/loss_vlb_step=0.00112, train/loss_step=0.190, global_step=196.0]Epoch 0:  82%|████████▎ | 198/240 [03:30<00:44,  1.06s/it, loss=0.21, v_num=0, train/loss_simple_step=0.190, train/loss_vlb_step=0.00112, train/loss_step=0.190, global_step=196.0]Epoch 0:  82%|████████▎ | 198/240 [03:30<00:44,  1.06s/it, loss=0.207, v_num=0, train/loss_simple_step=0.178, train/loss_vlb_step=0.00225, train/loss_step=0.178, global_step=197.0]Epoch 0:  83%|████████▎ | 199/240 [03:31<00:43,  1.06s/it, loss=0.207, v_num=0, train/loss_simple_step=0.178, train/loss_vlb_step=0.00225, train/loss_step=0.178, global_step=197.0]Epoch 0:  83%|████████▎ | 199/240 [03:31<00:43,  1.06s/it, loss=0.204, v_num=0, train/loss_simple_step=0.180, train/loss_vlb_step=0.00119, train/loss_step=0.180, global_step=198.0]Epoch 0:  83%|████████▎ | 200/240 [03:32<00:42,  1.06s/it, loss=0.204, v_num=0, train/loss_simple_step=0.180, train/loss_vlb_step=0.00119, train/loss_step=0.180, global_step=198.0]Epoch 0:  83%|████████▎ | 200/240 [03:32<00:42,  1.06s/it, loss=0.207, v_num=0, train/loss_simple_step=0.212, train/loss_vlb_step=0.00178, train/loss_step=0.212, global_step=199.0]Epoch 0:  84%|████████▍ | 201/240 [03:33<00:41,  1.06s/it, loss=0.207, v_num=0, train/loss_simple_step=0.212, train/loss_vlb_step=0.00178, train/loss_step=0.212, global_step=199.0]Epoch 0:  84%|████████▍ | 201/240 [03:33<00:41,  1.06s/it, loss=0.199, v_num=0, train/loss_simple_step=0.126, train/loss_vlb_step=0.0143, train/loss_step=0.126, global_step=200.0] Epoch 0:  84%|████████▍ | 202/240 [03:34<00:40,  1.06s/it, loss=0.199, v_num=0, train/loss_simple_step=0.126, train/loss_vlb_step=0.0143, train/loss_step=0.126, global_step=200.0]Epoch 0:  84%|████████▍ | 202/240 [03:34<00:40,  1.06s/it, loss=0.197, v_num=0, train/loss_simple_step=0.241, train/loss_vlb_step=0.00437, train/loss_step=0.241, global_step=201.0]Epoch 0:  85%|████████▍ | 203/240 [03:35<00:39,  1.06s/it, loss=0.197, v_num=0, train/loss_simple_step=0.241, train/loss_vlb_step=0.00437, train/loss_step=0.241, global_step=201.0]Epoch 0:  85%|████████▍ | 203/240 [03:35<00:39,  1.06s/it, loss=0.196, v_num=0, train/loss_simple_step=0.191, train/loss_vlb_step=0.00151, train/loss_step=0.191, global_step=202.0]Epoch 0:  85%|████████▌ | 204/240 [03:36<00:38,  1.06s/it, loss=0.196, v_num=0, train/loss_simple_step=0.191, train/loss_vlb_step=0.00151, train/loss_step=0.191, global_step=202.0]Epoch 0:  85%|████████▌ | 204/240 [03:36<00:38,  1.06s/it, loss=0.199, v_num=0, train/loss_simple_step=0.207, train/loss_vlb_step=0.00613, train/loss_step=0.207, global_step=203.0]Epoch 0:  85%|████████▌ | 205/240 [03:50<00:39,  1.12s/it, loss=0.199, v_num=0, train/loss_simple_step=0.207, train/loss_vlb_step=0.00613, train/loss_step=0.207, global_step=203.0]Epoch 0:  85%|████████▌ | 205/240 [03:50<00:39,  1.12s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:229: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(

Validation: 0it [00:00, ?it/s][A
Validation DataLoader 0:   0%|          | 0/35 [00:01<?, ?it/s][A
Validation DataLoader 0:   3%|▎         | 1/35 [00:02<01:34,  2.79s/it][AEpoch 0:  86%|████████▌ | 206/240 [03:53<00:38,  1.13s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:   6%|▌         | 2/35 [00:03<01:00,  1.85s/it][AEpoch 0:  86%|████████▋ | 207/240 [03:54<00:37,  1.13s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:   9%|▊         | 3/35 [00:04<00:49,  1.54s/it][AEpoch 0:  87%|████████▋ | 208/240 [03:55<00:36,  1.13s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  11%|█▏        | 4/35 [00:05<00:42,  1.39s/it][AEpoch 0:  87%|████████▋ | 209/240 [03:56<00:35,  1.13s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  14%|█▍        | 5/35 [00:06<00:38,  1.29s/it][AEpoch 0:  88%|████████▊ | 210/240 [03:56<00:33,  1.13s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  17%|█▋        | 6/35 [00:07<00:35,  1.22s/it][AEpoch 0:  88%|████████▊ | 211/240 [03:57<00:32,  1.13s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  20%|██        | 7/35 [00:08<00:32,  1.18s/it][AEpoch 0:  88%|████████▊ | 212/240 [03:58<00:31,  1.13s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  23%|██▎       | 8/35 [00:09<00:30,  1.14s/it][AEpoch 0:  89%|████████▉ | 213/240 [03:59<00:30,  1.13s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  26%|██▌       | 9/35 [00:09<00:28,  1.11s/it][AEpoch 0:  89%|████████▉ | 214/240 [04:00<00:29,  1.12s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  29%|██▊       | 10/35 [00:10<00:27,  1.09s/it][AEpoch 0:  90%|████████▉ | 215/240 [04:01<00:28,  1.12s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  31%|███▏      | 11/35 [00:11<00:25,  1.07s/it][AEpoch 0:  90%|█████████ | 216/240 [04:02<00:26,  1.12s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  34%|███▍      | 12/35 [00:12<00:24,  1.06s/it][AEpoch 0:  90%|█████████ | 217/240 [04:03<00:25,  1.12s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  37%|███▋      | 13/35 [00:13<00:23,  1.05s/it][AEpoch 0:  91%|█████████ | 218/240 [04:04<00:24,  1.12s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  40%|████      | 14/35 [00:14<00:21,  1.04s/it][AEpoch 0:  91%|█████████▏| 219/240 [04:05<00:23,  1.12s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  43%|████▎     | 15/35 [00:15<00:20,  1.03s/it][AEpoch 0:  92%|█████████▏| 220/240 [04:06<00:22,  1.12s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  46%|████▌     | 16/35 [00:16<00:19,  1.02s/it][AEpoch 0:  92%|█████████▏| 221/240 [04:06<00:21,  1.12s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  49%|████▊     | 17/35 [00:17<00:18,  1.02s/it][AEpoch 0:  92%|█████████▎| 222/240 [04:07<00:20,  1.12s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  51%|█████▏    | 18/35 [00:18<00:17,  1.01s/it][AEpoch 0:  93%|█████████▎| 223/240 [04:08<00:18,  1.12s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  54%|█████▍    | 19/35 [00:19<00:16,  1.00s/it][AEpoch 0:  93%|█████████▎| 224/240 [04:09<00:17,  1.11s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  57%|█████▋    | 20/35 [00:19<00:14,  1.00it/s][AEpoch 0:  94%|█████████▍| 225/240 [04:10<00:16,  1.11s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  60%|██████    | 21/35 [00:20<00:13,  1.00it/s][AEpoch 0:  94%|█████████▍| 226/240 [04:11<00:15,  1.11s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  63%|██████▎   | 22/35 [00:21<00:12,  1.01it/s][AEpoch 0:  95%|█████████▍| 227/240 [04:12<00:14,  1.11s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  66%|██████▌   | 23/35 [00:22<00:11,  1.01it/s][AEpoch 0:  95%|█████████▌| 228/240 [04:13<00:13,  1.11s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  69%|██████▊   | 24/35 [00:23<00:10,  1.01it/s][AEpoch 0:  95%|█████████▌| 229/240 [04:14<00:12,  1.11s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  71%|███████▏  | 25/35 [00:24<00:09,  1.02it/s][AEpoch 0:  96%|█████████▌| 230/240 [04:15<00:11,  1.11s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  74%|███████▍  | 26/35 [00:25<00:08,  1.02it/s][AEpoch 0:  96%|█████████▋| 231/240 [04:15<00:09,  1.11s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  77%|███████▋  | 27/35 [00:26<00:07,  1.03it/s][AEpoch 0:  97%|█████████▋| 232/240 [04:16<00:08,  1.11s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  80%|████████  | 28/35 [00:27<00:06,  1.03it/s][AEpoch 0:  97%|█████████▋| 233/240 [04:17<00:07,  1.11s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  83%|████████▎ | 29/35 [00:28<00:05,  1.03it/s][AEpoch 0:  98%|█████████▊| 234/240 [04:18<00:06,  1.11s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  86%|████████▌ | 30/35 [00:29<00:04,  1.03it/s][AEpoch 0:  98%|█████████▊| 235/240 [04:19<00:05,  1.10s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  89%|████████▊ | 31/35 [00:30<00:03,  1.03it/s][AEpoch 0:  98%|█████████▊| 236/240 [04:20<00:04,  1.10s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  91%|█████████▏| 32/35 [00:30<00:02,  1.03it/s][AEpoch 0:  99%|█████████▉| 237/240 [04:21<00:03,  1.10s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  94%|█████████▍| 33/35 [00:31<00:01,  1.04it/s][AEpoch 0:  99%|█████████▉| 238/240 [04:22<00:02,  1.10s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0:  97%|█████████▋| 34/35 [00:32<00:00,  1.04it/s][AEpoch 0: 100%|█████████▉| 239/240 [04:23<00:01,  1.10s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
Validation DataLoader 0: 100%|██████████| 35/35 [00:34<00:00,  1.00it/s][AEpoch 0: 100%|██████████| 240/240 [04:25<00:00,  1.11s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]Epoch 0: 100%|██████████| 240/240 [04:26<00:00,  1.11s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0]
                                                                        [A/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2011: LightningDeprecationWarning: `Trainer.training_type_plugin` is deprecated in v1.6 and will be removed in v1.8. Use `Trainer.strategy` instead.
  rank_zero_deprecation(
Average Epoch time: 266.82 seconds
Average Peak memory 35073.31MiB
Epoch 0: 100%|██████████| 240/240 [04:26<00:00,  1.11s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 0, global step 205: 'val/loss_simple_ema' reached 0.20283 (best 0.20283), saving model to 'logs/2025-03-26T05-14-20_ffhq-ldm-vq-4/checkpoints/epoch=000000.ckpt' as top 3
Epoch 1: 100%|██████████| 240/240 [04:41<00:00,  1.17s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   0%|          | 1/240 [04:44<18:52:15, 284.25s/it, loss=0.198, v_num=0, train/loss_simple_step=0.144, train/loss_vlb_step=0.000602, train/loss_step=0.144, global_step=204.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   0%|          | 1/240 [04:44<18:52:56, 284.42s/it, loss=0.196, v_num=0, train/loss_simple_step=0.254, train/loss_vlb_step=0.00181, train/loss_step=0.254, global_step=205.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:   1%|          | 2/240 [04:45<9:25:41, 142.61s/it, loss=0.196, v_num=0, train/loss_simple_step=0.254, train/loss_vlb_step=0.00181, train/loss_step=0.254, global_step=205.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:   1%|          | 2/240 [04:45<9:26:02, 142.70s/it, loss=0.196, v_num=0, train/loss_simple_step=0.208, train/loss_vlb_step=0.00127, train/loss_step=0.208, global_step=206.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   1%|▏         | 3/240 [04:46<6:16:50, 95.40s/it, loss=0.196, v_num=0, train/loss_simple_step=0.208, train/loss_vlb_step=0.00127, train/loss_step=0.208, global_step=206.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:   1%|▏         | 3/240 [04:46<6:17:04, 95.46s/it, loss=0.194, v_num=0, train/loss_simple_step=0.189, train/loss_vlb_step=0.000963, train/loss_step=0.189, global_step=207.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   2%|▏         | 4/240 [04:47<4:42:24, 71.80s/it, loss=0.194, v_num=0, train/loss_simple_step=0.189, train/loss_vlb_step=0.000963, train/loss_step=0.189, global_step=207.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   2%|▏         | 4/240 [04:47<4:42:36, 71.85s/it, loss=0.192, v_num=0, train/loss_simple_step=0.181, train/loss_vlb_step=0.00123, train/loss_step=0.181, global_step=208.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:   2%|▏         | 5/240 [04:48<3:45:45, 57.64s/it, loss=0.192, v_num=0, train/loss_simple_step=0.181, train/loss_vlb_step=0.00123, train/loss_step=0.181, global_step=208.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   2%|▏         | 5/240 [04:48<3:45:54, 57.68s/it, loss=0.189, v_num=0, train/loss_simple_step=0.161, train/loss_vlb_step=0.00128, train/loss_step=0.161, global_step=209.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   2%|▎         | 6/240 [04:49<3:07:59, 48.20s/it, loss=0.189, v_num=0, train/loss_simple_step=0.161, train/loss_vlb_step=0.00128, train/loss_step=0.161, global_step=209.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   2%|▎         | 6/240 [04:49<3:08:06, 48.23s/it, loss=0.191, v_num=0, train/loss_simple_step=0.186, train/loss_vlb_step=0.00157, train/loss_step=0.186, global_step=210.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   3%|▎         | 7/240 [04:50<2:41:00, 41.46s/it, loss=0.191, v_num=0, train/loss_simple_step=0.186, train/loss_vlb_step=0.00157, train/loss_step=0.186, global_step=210.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   3%|▎         | 7/240 [04:50<2:41:06, 41.49s/it, loss=0.188, v_num=0, train/loss_simple_step=0.171, train/loss_vlb_step=0.0134, train/loss_step=0.171, global_step=211.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:   3%|▎         | 8/240 [04:51<2:20:45, 36.40s/it, loss=0.188, v_num=0, train/loss_simple_step=0.171, train/loss_vlb_step=0.0134, train/loss_step=0.171, global_step=211.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   3%|▎         | 8/240 [04:51<2:20:51, 36.43s/it, loss=0.188, v_num=0, train/loss_simple_step=0.148, train/loss_vlb_step=0.00124, train/loss_step=0.148, global_step=212.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   4%|▍         | 9/240 [04:52<2:05:00, 32.47s/it, loss=0.188, v_num=0, train/loss_simple_step=0.148, train/loss_vlb_step=0.00124, train/loss_step=0.148, global_step=212.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   4%|▍         | 9/240 [04:52<2:05:05, 32.49s/it, loss=0.191, v_num=0, train/loss_simple_step=0.227, train/loss_vlb_step=0.00136, train/loss_step=0.227, global_step=213.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   4%|▍         | 10/240 [04:53<1:52:24, 29.32s/it, loss=0.191, v_num=0, train/loss_simple_step=0.227, train/loss_vlb_step=0.00136, train/loss_step=0.227, global_step=213.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   4%|▍         | 10/240 [04:53<1:52:28, 29.34s/it, loss=0.191, v_num=0, train/loss_simple_step=0.185, train/loss_vlb_step=0.00124, train/loss_step=0.185, global_step=214.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   5%|▍         | 11/240 [04:54<1:42:04, 26.75s/it, loss=0.191, v_num=0, train/loss_simple_step=0.185, train/loss_vlb_step=0.00124, train/loss_step=0.185, global_step=214.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   5%|▍         | 11/240 [04:54<1:42:09, 26.76s/it, loss=0.192, v_num=0, train/loss_simple_step=0.258, train/loss_vlb_step=0.00406, train/loss_step=0.258, global_step=215.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   5%|▌         | 12/240 [04:55<1:33:28, 24.60s/it, loss=0.192, v_num=0, train/loss_simple_step=0.258, train/loss_vlb_step=0.00406, train/loss_step=0.258, global_step=215.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   5%|▌         | 12/240 [04:55<1:33:32, 24.62s/it, loss=0.191, v_num=0, train/loss_simple_step=0.175, train/loss_vlb_step=0.0037, train/loss_step=0.175, global_step=216.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:   5%|▌         | 13/240 [04:56<1:26:11, 22.78s/it, loss=0.191, v_num=0, train/loss_simple_step=0.175, train/loss_vlb_step=0.0037, train/loss_step=0.175, global_step=216.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   5%|▌         | 13/240 [04:56<1:26:15, 22.80s/it, loss=0.193, v_num=0, train/loss_simple_step=0.225, train/loss_vlb_step=0.00146, train/loss_step=0.225, global_step=217.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   6%|▌         | 14/240 [04:57<1:19:57, 21.23s/it, loss=0.193, v_num=0, train/loss_simple_step=0.225, train/loss_vlb_step=0.00146, train/loss_step=0.225, global_step=217.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   6%|▌         | 14/240 [04:57<1:20:00, 21.24s/it, loss=0.195, v_num=0, train/loss_simple_step=0.211, train/loss_vlb_step=0.00317, train/loss_step=0.211, global_step=218.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   6%|▋         | 15/240 [04:58<1:14:32, 19.88s/it, loss=0.195, v_num=0, train/loss_simple_step=0.211, train/loss_vlb_step=0.00317, train/loss_step=0.211, global_step=218.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   6%|▋         | 15/240 [04:58<1:14:35, 19.89s/it, loss=0.196, v_num=0, train/loss_simple_step=0.229, train/loss_vlb_step=0.00128, train/loss_step=0.229, global_step=219.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   7%|▋         | 16/240 [04:59<1:09:48, 18.70s/it, loss=0.196, v_num=0, train/loss_simple_step=0.229, train/loss_vlb_step=0.00128, train/loss_step=0.229, global_step=219.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   7%|▋         | 16/240 [04:59<1:09:51, 18.71s/it, loss=0.2, v_num=0, train/loss_simple_step=0.215, train/loss_vlb_step=0.00248, train/loss_step=0.215, global_step=220.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]  Epoch 1:   7%|▋         | 17/240 [05:00<1:05:37, 17.66s/it, loss=0.2, v_num=0, train/loss_simple_step=0.215, train/loss_vlb_step=0.00248, train/loss_step=0.215, global_step=220.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   7%|▋         | 17/240 [05:00<1:05:40, 17.67s/it, loss=0.198, v_num=0, train/loss_simple_step=0.201, train/loss_vlb_step=0.0015, train/loss_step=0.201, global_step=221.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   8%|▊         | 18/240 [05:01<1:01:54, 16.73s/it, loss=0.198, v_num=0, train/loss_simple_step=0.201, train/loss_vlb_step=0.0015, train/loss_step=0.201, global_step=221.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   8%|▊         | 18/240 [05:01<1:01:56, 16.74s/it, loss=0.197, v_num=0, train/loss_simple_step=0.160, train/loss_vlb_step=0.000879, train/loss_step=0.160, global_step=222.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   8%|▊         | 19/240 [05:02<58:34, 15.90s/it, loss=0.197, v_num=0, train/loss_simple_step=0.160, train/loss_vlb_step=0.000879, train/loss_step=0.160, global_step=222.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]  Epoch 1:   8%|▊         | 19/240 [05:02<58:36, 15.91s/it, loss=0.194, v_num=0, train/loss_simple_step=0.158, train/loss_vlb_step=0.000946, train/loss_step=0.158, global_step=223.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   8%|▊         | 20/240 [05:03<55:34, 15.16s/it, loss=0.194, v_num=0, train/loss_simple_step=0.158, train/loss_vlb_step=0.000946, train/loss_step=0.158, global_step=223.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   8%|▊         | 20/240 [05:03<55:36, 15.17s/it, loss=0.196, v_num=0, train/loss_simple_step=0.177, train/loss_vlb_step=0.00151, train/loss_step=0.177, global_step=224.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:   9%|▉         | 21/240 [05:04<52:51, 14.48s/it, loss=0.196, v_num=0, train/loss_simple_step=0.177, train/loss_vlb_step=0.00151, train/loss_step=0.177, global_step=224.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   9%|▉         | 21/240 [05:04<52:53, 14.49s/it, loss=0.194, v_num=0, train/loss_simple_step=0.206, train/loss_vlb_step=0.00407, train/loss_step=0.206, global_step=225.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   9%|▉         | 22/240 [05:05<50:23, 13.87s/it, loss=0.194, v_num=0, train/loss_simple_step=0.206, train/loss_vlb_step=0.00407, train/loss_step=0.206, global_step=225.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:   9%|▉         | 22/240 [05:05<50:25, 13.88s/it, loss=0.195, v_num=0, train/loss_simple_step=0.228, train/loss_vlb_step=0.00154, train/loss_step=0.228, global_step=226.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  10%|▉         | 23/240 [05:06<48:08, 13.31s/it, loss=0.195, v_num=0, train/loss_simple_step=0.228, train/loss_vlb_step=0.00154, train/loss_step=0.228, global_step=226.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  10%|▉         | 23/240 [05:06<48:10, 13.32s/it, loss=0.196, v_num=0, train/loss_simple_step=0.208, train/loss_vlb_step=0.014, train/loss_step=0.208, global_step=227.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]  Epoch 1:  10%|█         | 24/240 [05:07<46:04, 12.80s/it, loss=0.196, v_num=0, train/loss_simple_step=0.208, train/loss_vlb_step=0.014, train/loss_step=0.208, global_step=227.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  10%|█         | 24/240 [05:07<46:05, 12.80s/it, loss=0.196, v_num=0, train/loss_simple_step=0.188, train/loss_vlb_step=0.00142, train/loss_step=0.188, global_step=228.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  10%|█         | 25/240 [05:08<44:09, 12.32s/it, loss=0.196, v_num=0, train/loss_simple_step=0.188, train/loss_vlb_step=0.00142, train/loss_step=0.188, global_step=228.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  10%|█         | 25/240 [05:08<44:11, 12.33s/it, loss=0.198, v_num=0, train/loss_simple_step=0.195, train/loss_vlb_step=0.0136, train/loss_step=0.195, global_step=229.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:  11%|█         | 26/240 [05:09<42:23, 11.89s/it, loss=0.198, v_num=0, train/loss_simple_step=0.195, train/loss_vlb_step=0.0136, train/loss_step=0.195, global_step=229.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  11%|█         | 26/240 [05:09<42:25, 11.90s/it, loss=0.2, v_num=0, train/loss_simple_step=0.225, train/loss_vlb_step=0.00156, train/loss_step=0.225, global_step=230.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:  11%|█▏        | 27/240 [05:10<40:46, 11.48s/it, loss=0.2, v_num=0, train/loss_simple_step=0.225, train/loss_vlb_step=0.00156, train/loss_step=0.225, global_step=230.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  11%|█▏        | 27/240 [05:10<40:47, 11.49s/it, loss=0.202, v_num=0, train/loss_simple_step=0.221, train/loss_vlb_step=0.00227, train/loss_step=0.221, global_step=231.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  12%|█▏        | 28/240 [05:11<39:14, 11.11s/it, loss=0.202, v_num=0, train/loss_simple_step=0.221, train/loss_vlb_step=0.00227, train/loss_step=0.221, global_step=231.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  12%|█▏        | 28/240 [05:11<39:16, 11.12s/it, loss=0.204, v_num=0, train/loss_simple_step=0.191, train/loss_vlb_step=0.00152, train/loss_step=0.191, global_step=232.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  12%|█▏        | 29/240 [05:12<37:50, 10.76s/it, loss=0.204, v_num=0, train/loss_simple_step=0.191, train/loss_vlb_step=0.00152, train/loss_step=0.191, global_step=232.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  12%|█▏        | 29/240 [05:12<37:51, 10.77s/it, loss=0.204, v_num=0, train/loss_simple_step=0.230, train/loss_vlb_step=0.00202, train/loss_step=0.230, global_step=233.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  12%|█▎        | 30/240 [05:13<36:31, 10.43s/it, loss=0.204, v_num=0, train/loss_simple_step=0.230, train/loss_vlb_step=0.00202, train/loss_step=0.230, global_step=233.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  12%|█▎        | 30/240 [05:13<36:32, 10.44s/it, loss=0.202, v_num=0, train/loss_simple_step=0.145, train/loss_vlb_step=0.000962, train/loss_step=0.145, global_step=234.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  13%|█▎        | 31/240 [05:14<35:17, 10.13s/it, loss=0.202, v_num=0, train/loss_simple_step=0.145, train/loss_vlb_step=0.000962, train/loss_step=0.145, global_step=234.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  13%|█▎        | 31/240 [05:14<35:18, 10.14s/it, loss=0.197, v_num=0, train/loss_simple_step=0.161, train/loss_vlb_step=0.000968, train/loss_step=0.161, global_step=235.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  13%|█▎        | 32/240 [05:15<34:07,  9.84s/it, loss=0.197, v_num=0, train/loss_simple_step=0.161, train/loss_vlb_step=0.000968, train/loss_step=0.161, global_step=235.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  13%|█▎        | 32/240 [05:15<34:08,  9.85s/it, loss=0.196, v_num=0, train/loss_simple_step=0.152, train/loss_vlb_step=0.00214, train/loss_step=0.152, global_step=236.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:  14%|█▍        | 33/240 [05:15<33:02,  9.58s/it, loss=0.196, v_num=0, train/loss_simple_step=0.152, train/loss_vlb_step=0.00214, train/loss_step=0.152, global_step=236.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  14%|█▍        | 33/240 [05:16<33:03,  9.58s/it, loss=0.194, v_num=0, train/loss_simple_step=0.179, train/loss_vlb_step=0.00175, train/loss_step=0.179, global_step=237.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  14%|█▍        | 34/240 [05:16<32:00,  9.32s/it, loss=0.194, v_num=0, train/loss_simple_step=0.179, train/loss_vlb_step=0.00175, train/loss_step=0.179, global_step=237.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  14%|█▍        | 34/240 [05:17<32:01,  9.33s/it, loss=0.194, v_num=0, train/loss_simple_step=0.206, train/loss_vlb_step=0.00286, train/loss_step=0.206, global_step=238.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  15%|█▍        | 35/240 [05:17<31:02,  9.09s/it, loss=0.194, v_num=0, train/loss_simple_step=0.206, train/loss_vlb_step=0.00286, train/loss_step=0.206, global_step=238.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  15%|█▍        | 35/240 [05:18<31:03,  9.09s/it, loss=0.195, v_num=0, train/loss_simple_step=0.251, train/loss_vlb_step=0.00378, train/loss_step=0.251, global_step=239.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  15%|█▌        | 36/240 [05:18<30:07,  8.86s/it, loss=0.195, v_num=0, train/loss_simple_step=0.251, train/loss_vlb_step=0.00378, train/loss_step=0.251, global_step=239.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  15%|█▌        | 36/240 [05:19<30:08,  8.87s/it, loss=0.192, v_num=0, train/loss_simple_step=0.167, train/loss_vlb_step=0.0009, train/loss_step=0.167, global_step=240.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:  15%|█▌        | 37/240 [05:19<29:15,  8.65s/it, loss=0.192, v_num=0, train/loss_simple_step=0.167, train/loss_vlb_step=0.0009, train/loss_step=0.167, global_step=240.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  15%|█▌        | 37/240 [05:20<29:16,  8.65s/it, loss=0.193, v_num=0, train/loss_simple_step=0.214, train/loss_vlb_step=0.0024, train/loss_step=0.214, global_step=241.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  16%|█▌        | 38/240 [05:20<28:26,  8.45s/it, loss=0.193, v_num=0, train/loss_simple_step=0.214, train/loss_vlb_step=0.0024, train/loss_step=0.214, global_step=241.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  16%|█▌        | 38/240 [05:21<28:27,  8.45s/it, loss=0.197, v_num=0, train/loss_simple_step=0.239, train/loss_vlb_step=0.0185, train/loss_step=0.239, global_step=242.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  16%|█▋        | 39/240 [05:21<27:39,  8.25s/it, loss=0.197, v_num=0, train/loss_simple_step=0.239, train/loss_vlb_step=0.0185, train/loss_step=0.239, global_step=242.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  16%|█▋        | 39/240 [05:22<27:40,  8.26s/it, loss=0.202, v_num=0, train/loss_simple_step=0.267, train/loss_vlb_step=0.00412, train/loss_step=0.267, global_step=243.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  17%|█▋        | 40/240 [05:22<26:54,  8.07s/it, loss=0.202, v_num=0, train/loss_simple_step=0.267, train/loss_vlb_step=0.00412, train/loss_step=0.267, global_step=243.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  17%|█▋        | 40/240 [05:23<26:55,  8.08s/it, loss=0.204, v_num=0, train/loss_simple_step=0.209, train/loss_vlb_step=0.00224, train/loss_step=0.209, global_step=244.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  17%|█▋        | 41/240 [05:23<26:12,  7.90s/it, loss=0.204, v_num=0, train/loss_simple_step=0.209, train/loss_vlb_step=0.00224, train/loss_step=0.209, global_step=244.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  17%|█▋        | 41/240 [05:24<26:13,  7.91s/it, loss=0.203, v_num=0, train/loss_simple_step=0.192, train/loss_vlb_step=0.00315, train/loss_step=0.192, global_step=245.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  18%|█▊        | 42/240 [05:24<25:31,  7.74s/it, loss=0.203, v_num=0, train/loss_simple_step=0.192, train/loss_vlb_step=0.00315, train/loss_step=0.192, global_step=245.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  18%|█▊        | 42/240 [05:25<25:32,  7.74s/it, loss=0.199, v_num=0, train/loss_simple_step=0.143, train/loss_vlb_step=0.00108, train/loss_step=0.143, global_step=246.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  18%|█▊        | 43/240 [05:25<24:53,  7.58s/it, loss=0.199, v_num=0, train/loss_simple_step=0.143, train/loss_vlb_step=0.00108, train/loss_step=0.143, global_step=246.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  18%|█▊        | 43/240 [05:26<24:54,  7.58s/it, loss=0.196, v_num=0, train/loss_simple_step=0.156, train/loss_vlb_step=0.00107, train/loss_step=0.156, global_step=247.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  18%|█▊        | 44/240 [05:26<24:16,  7.43s/it, loss=0.196, v_num=0, train/loss_simple_step=0.156, train/loss_vlb_step=0.00107, train/loss_step=0.156, global_step=247.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  18%|█▊        | 44/240 [05:27<24:17,  7.43s/it, loss=0.194, v_num=0, train/loss_simple_step=0.135, train/loss_vlb_step=0.000752, train/loss_step=0.135, global_step=248.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  19%|█▉        | 45/240 [05:27<23:40,  7.29s/it, loss=0.194, v_num=0, train/loss_simple_step=0.135, train/loss_vlb_step=0.000752, train/loss_step=0.135, global_step=248.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  19%|█▉        | 45/240 [05:28<23:41,  7.29s/it, loss=0.192, v_num=0, train/loss_simple_step=0.157, train/loss_vlb_step=0.00134, train/loss_step=0.157, global_step=249.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:  19%|█▉        | 46/240 [05:28<23:06,  7.15s/it, loss=0.192, v_num=0, train/loss_simple_step=0.157, train/loss_vlb_step=0.00134, train/loss_step=0.157, global_step=249.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  19%|█▉        | 46/240 [05:29<23:07,  7.15s/it, loss=0.191, v_num=0, train/loss_simple_step=0.211, train/loss_vlb_step=0.00238, train/loss_step=0.211, global_step=250.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  20%|█▉        | 47/240 [05:29<22:34,  7.02s/it, loss=0.191, v_num=0, train/loss_simple_step=0.211, train/loss_vlb_step=0.00238, train/loss_step=0.211, global_step=250.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  20%|█▉        | 47/240 [05:30<22:35,  7.02s/it, loss=0.192, v_num=0, train/loss_simple_step=0.232, train/loss_vlb_step=0.0014, train/loss_step=0.232, global_step=251.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:  20%|██        | 48/240 [05:30<22:03,  6.89s/it, loss=0.192, v_num=0, train/loss_simple_step=0.232, train/loss_vlb_step=0.0014, train/loss_step=0.232, global_step=251.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  20%|██        | 48/240 [05:31<22:04,  6.90s/it, loss=0.192, v_num=0, train/loss_simple_step=0.191, train/loss_vlb_step=0.00131, train/loss_step=0.191, global_step=252.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  20%|██        | 49/240 [05:31<21:33,  6.77s/it, loss=0.192, v_num=0, train/loss_simple_step=0.191, train/loss_vlb_step=0.00131, train/loss_step=0.191, global_step=252.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  20%|██        | 49/240 [05:32<21:34,  6.78s/it, loss=0.19, v_num=0, train/loss_simple_step=0.197, train/loss_vlb_step=0.00233, train/loss_step=0.197, global_step=253.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:  21%|██        | 50/240 [05:32<21:04,  6.66s/it, loss=0.19, v_num=0, train/loss_simple_step=0.197, train/loss_vlb_step=0.00233, train/loss_step=0.197, global_step=253.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  21%|██        | 50/240 [05:33<21:05,  6.66s/it, loss=0.192, v_num=0, train/loss_simple_step=0.191, train/loss_vlb_step=0.00185, train/loss_step=0.191, global_step=254.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  21%|██▏       | 51/240 [05:33<20:37,  6.55s/it, loss=0.192, v_num=0, train/loss_simple_step=0.191, train/loss_vlb_step=0.00185, train/loss_step=0.191, global_step=254.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  21%|██▏       | 51/240 [05:34<20:37,  6.55s/it, loss=0.194, v_num=0, train/loss_simple_step=0.187, train/loss_vlb_step=0.00331, train/loss_step=0.187, global_step=255.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  22%|██▏       | 52/240 [05:34<20:10,  6.44s/it, loss=0.194, v_num=0, train/loss_simple_step=0.187, train/loss_vlb_step=0.00331, train/loss_step=0.187, global_step=255.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  22%|██▏       | 52/240 [05:35<20:11,  6.44s/it, loss=0.195, v_num=0, train/loss_simple_step=0.172, train/loss_vlb_step=0.00186, train/loss_step=0.172, global_step=256.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  22%|██▏       | 53/240 [05:35<19:44,  6.34s/it, loss=0.195, v_num=0, train/loss_simple_step=0.172, train/loss_vlb_step=0.00186, train/loss_step=0.172, global_step=256.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  22%|██▏       | 53/240 [05:36<19:45,  6.34s/it, loss=0.195, v_num=0, train/loss_simple_step=0.186, train/loss_vlb_step=0.00273, train/loss_step=0.186, global_step=257.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  22%|██▎       | 54/240 [05:36<19:20,  6.24s/it, loss=0.195, v_num=0, train/loss_simple_step=0.186, train/loss_vlb_step=0.00273, train/loss_step=0.186, global_step=257.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  22%|██▎       | 54/240 [05:37<19:20,  6.24s/it, loss=0.192, v_num=0, train/loss_simple_step=0.139, train/loss_vlb_step=0.000866, train/loss_step=0.139, global_step=258.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  23%|██▎       | 55/240 [05:37<18:56,  6.14s/it, loss=0.192, v_num=0, train/loss_simple_step=0.139, train/loss_vlb_step=0.000866, train/loss_step=0.139, global_step=258.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  23%|██▎       | 55/240 [05:38<18:56,  6.15s/it, loss=0.19, v_num=0, train/loss_simple_step=0.219, train/loss_vlb_step=0.00171, train/loss_step=0.219, global_step=259.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]  Epoch 1:  23%|██▎       | 56/240 [05:38<18:33,  6.05s/it, loss=0.19, v_num=0, train/loss_simple_step=0.219, train/loss_vlb_step=0.00171, train/loss_step=0.219, global_step=259.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  23%|██▎       | 56/240 [05:38<18:33,  6.05s/it, loss=0.191, v_num=0, train/loss_simple_step=0.193, train/loss_vlb_step=0.00177, train/loss_step=0.193, global_step=260.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  24%|██▍       | 57/240 [05:39<18:10,  5.96s/it, loss=0.191, v_num=0, train/loss_simple_step=0.193, train/loss_vlb_step=0.00177, train/loss_step=0.193, global_step=260.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  24%|██▍       | 57/240 [05:39<18:11,  5.96s/it, loss=0.19, v_num=0, train/loss_simple_step=0.192, train/loss_vlb_step=0.00259, train/loss_step=0.192, global_step=261.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:  24%|██▍       | 58/240 [05:40<17:49,  5.88s/it, loss=0.19, v_num=0, train/loss_simple_step=0.192, train/loss_vlb_step=0.00259, train/loss_step=0.192, global_step=261.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  24%|██▍       | 58/240 [05:40<17:49,  5.88s/it, loss=0.19, v_num=0, train/loss_simple_step=0.226, train/loss_vlb_step=0.00355, train/loss_step=0.226, global_step=262.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  25%|██▍       | 59/240 [05:41<17:28,  5.79s/it, loss=0.19, v_num=0, train/loss_simple_step=0.226, train/loss_vlb_step=0.00355, train/loss_step=0.226, global_step=262.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  25%|██▍       | 59/240 [05:41<17:29,  5.80s/it, loss=0.188, v_num=0, train/loss_simple_step=0.229, train/loss_vlb_step=0.00816, train/loss_step=0.229, global_step=263.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  25%|██▌       | 60/240 [05:42<17:08,  5.71s/it, loss=0.188, v_num=0, train/loss_simple_step=0.229, train/loss_vlb_step=0.00816, train/loss_step=0.229, global_step=263.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  25%|██▌       | 60/240 [05:42<17:08,  5.72s/it, loss=0.19, v_num=0, train/loss_simple_step=0.246, train/loss_vlb_step=0.00247, train/loss_step=0.246, global_step=264.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:  25%|██▌       | 61/240 [05:43<16:48,  5.63s/it, loss=0.19, v_num=0, train/loss_simple_step=0.246, train/loss_vlb_step=0.00247, train/loss_step=0.246, global_step=264.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  25%|██▌       | 61/240 [05:43<16:49,  5.64s/it, loss=0.19, v_num=0, train/loss_simple_step=0.202, train/loss_vlb_step=0.00127, train/loss_step=0.202, global_step=265.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  26%|██▌       | 62/240 [05:44<16:29,  5.56s/it, loss=0.19, v_num=0, train/loss_simple_step=0.202, train/loss_vlb_step=0.00127, train/loss_step=0.202, global_step=265.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  26%|██▌       | 62/240 [05:44<16:30,  5.56s/it, loss=0.192, v_num=0, train/loss_simple_step=0.175, train/loss_vlb_step=0.00234, train/loss_step=0.175, global_step=266.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  26%|██▋       | 63/240 [05:45<16:11,  5.49s/it, loss=0.192, v_num=0, train/loss_simple_step=0.175, train/loss_vlb_step=0.00234, train/loss_step=0.175, global_step=266.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  26%|██▋       | 63/240 [05:45<16:11,  5.49s/it, loss=0.192, v_num=0, train/loss_simple_step=0.167, train/loss_vlb_step=0.00126, train/loss_step=0.167, global_step=267.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  27%|██▋       | 64/240 [05:46<15:53,  5.42s/it, loss=0.192, v_num=0, train/loss_simple_step=0.167, train/loss_vlb_step=0.00126, train/loss_step=0.167, global_step=267.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  27%|██▋       | 64/240 [05:46<15:53,  5.42s/it, loss=0.198, v_num=0, train/loss_simple_step=0.247, train/loss_vlb_step=0.00187, train/loss_step=0.247, global_step=268.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  27%|██▋       | 65/240 [05:47<15:35,  5.35s/it, loss=0.198, v_num=0, train/loss_simple_step=0.247, train/loss_vlb_step=0.00187, train/loss_step=0.247, global_step=268.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  27%|██▋       | 65/240 [05:47<15:36,  5.35s/it, loss=0.201, v_num=0, train/loss_simple_step=0.220, train/loss_vlb_step=0.00166, train/loss_step=0.220, global_step=269.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  28%|██▊       | 66/240 [05:48<15:19,  5.28s/it, loss=0.201, v_num=0, train/loss_simple_step=0.220, train/loss_vlb_step=0.00166, train/loss_step=0.220, global_step=269.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  28%|██▊       | 66/240 [05:48<15:19,  5.29s/it, loss=0.201, v_num=0, train/loss_simple_step=0.211, train/loss_vlb_step=0.00215, train/loss_step=0.211, global_step=270.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  28%|██▊       | 67/240 [05:49<15:02,  5.22s/it, loss=0.201, v_num=0, train/loss_simple_step=0.211, train/loss_vlb_step=0.00215, train/loss_step=0.211, global_step=270.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  28%|██▊       | 67/240 [05:49<15:03,  5.22s/it, loss=0.198, v_num=0, train/loss_simple_step=0.171, train/loss_vlb_step=0.00119, train/loss_step=0.171, global_step=271.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  28%|██▊       | 68/240 [05:50<14:46,  5.16s/it, loss=0.198, v_num=0, train/loss_simple_step=0.171, train/loss_vlb_step=0.00119, train/loss_step=0.171, global_step=271.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  28%|██▊       | 68/240 [05:50<14:47,  5.16s/it, loss=0.199, v_num=0, train/loss_simple_step=0.206, train/loss_vlb_step=0.00575, train/loss_step=0.206, global_step=272.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  29%|██▉       | 69/240 [05:51<14:31,  5.10s/it, loss=0.199, v_num=0, train/loss_simple_step=0.206, train/loss_vlb_step=0.00575, train/loss_step=0.206, global_step=272.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  29%|██▉       | 69/240 [05:51<14:31,  5.10s/it, loss=0.199, v_num=0, train/loss_simple_step=0.206, train/loss_vlb_step=0.00229, train/loss_step=0.206, global_step=273.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  29%|██▉       | 70/240 [05:52<14:16,  5.04s/it, loss=0.199, v_num=0, train/loss_simple_step=0.206, train/loss_vlb_step=0.00229, train/loss_step=0.206, global_step=273.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  29%|██▉       | 70/240 [05:52<14:16,  5.04s/it, loss=0.2, v_num=0, train/loss_simple_step=0.200, train/loss_vlb_step=0.00422, train/loss_step=0.200, global_step=274.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]  Epoch 1:  30%|██▉       | 71/240 [05:53<14:01,  4.98s/it, loss=0.2, v_num=0, train/loss_simple_step=0.200, train/loss_vlb_step=0.00422, train/loss_step=0.200, global_step=274.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  30%|██▉       | 71/240 [05:53<14:02,  4.98s/it, loss=0.202, v_num=0, train/loss_simple_step=0.225, train/loss_vlb_step=0.00561, train/loss_step=0.225, global_step=275.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  30%|███       | 72/240 [05:54<13:47,  4.93s/it, loss=0.202, v_num=0, train/loss_simple_step=0.225, train/loss_vlb_step=0.00561, train/loss_step=0.225, global_step=275.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  30%|███       | 72/240 [05:54<13:47,  4.93s/it, loss=0.202, v_num=0, train/loss_simple_step=0.172, train/loss_vlb_step=0.00163, train/loss_step=0.172, global_step=276.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  30%|███       | 73/240 [05:55<13:33,  4.87s/it, loss=0.202, v_num=0, train/loss_simple_step=0.172, train/loss_vlb_step=0.00163, train/loss_step=0.172, global_step=276.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  30%|███       | 73/240 [05:55<13:33,  4.87s/it, loss=0.204, v_num=0, train/loss_simple_step=0.239, train/loss_vlb_step=0.0017, train/loss_step=0.239, global_step=277.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:  31%|███       | 74/240 [05:56<13:19,  4.82s/it, loss=0.204, v_num=0, train/loss_simple_step=0.239, train/loss_vlb_step=0.0017, train/loss_step=0.239, global_step=277.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  31%|███       | 74/240 [05:56<13:20,  4.82s/it, loss=0.209, v_num=0, train/loss_simple_step=0.226, train/loss_vlb_step=0.0102, train/loss_step=0.226, global_step=278.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  31%|███▏      | 75/240 [05:57<13:06,  4.77s/it, loss=0.209, v_num=0, train/loss_simple_step=0.226, train/loss_vlb_step=0.0102, train/loss_step=0.226, global_step=278.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  31%|███▏      | 75/240 [05:57<13:06,  4.77s/it, loss=0.208, v_num=0, train/loss_simple_step=0.202, train/loss_vlb_step=0.0136, train/loss_step=0.202, global_step=279.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  32%|███▏      | 76/240 [05:58<12:53,  4.72s/it, loss=0.208, v_num=0, train/loss_simple_step=0.202, train/loss_vlb_step=0.0136, train/loss_step=0.202, global_step=279.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  32%|███▏      | 76/240 [05:58<12:53,  4.72s/it, loss=0.209, v_num=0, train/loss_simple_step=0.216, train/loss_vlb_step=0.00305, train/loss_step=0.216, global_step=280.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  32%|███▏      | 77/240 [05:59<12:40,  4.67s/it, loss=0.209, v_num=0, train/loss_simple_step=0.216, train/loss_vlb_step=0.00305, train/loss_step=0.216, global_step=280.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  32%|███▏      | 77/240 [05:59<12:41,  4.67s/it, loss=0.208, v_num=0, train/loss_simple_step=0.180, train/loss_vlb_step=0.00128, train/loss_step=0.180, global_step=281.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  32%|███▎      | 78/240 [06:00<12:28,  4.62s/it, loss=0.208, v_num=0, train/loss_simple_step=0.180, train/loss_vlb_step=0.00128, train/loss_step=0.180, global_step=281.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  32%|███▎      | 78/240 [06:00<12:29,  4.62s/it, loss=0.207, v_num=0, train/loss_simple_step=0.196, train/loss_vlb_step=0.00157, train/loss_step=0.196, global_step=282.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  33%|███▎      | 79/240 [06:01<12:16,  4.58s/it, loss=0.207, v_num=0, train/loss_simple_step=0.196, train/loss_vlb_step=0.00157, train/loss_step=0.196, global_step=282.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  33%|███▎      | 79/240 [06:01<12:17,  4.58s/it, loss=0.206, v_num=0, train/loss_simple_step=0.220, train/loss_vlb_step=0.00191, train/loss_step=0.220, global_step=283.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  33%|███▎      | 80/240 [06:02<12:05,  4.53s/it, loss=0.206, v_num=0, train/loss_simple_step=0.220, train/loss_vlb_step=0.00191, train/loss_step=0.220, global_step=283.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  33%|███▎      | 80/240 [06:02<12:05,  4.53s/it, loss=0.203, v_num=0, train/loss_simple_step=0.186, train/loss_vlb_step=0.0034, train/loss_step=0.186, global_step=284.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:  34%|███▍      | 81/240 [06:03<11:53,  4.49s/it, loss=0.203, v_num=0, train/loss_simple_step=0.186, train/loss_vlb_step=0.0034, train/loss_step=0.186, global_step=284.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  34%|███▍      | 81/240 [06:03<11:54,  4.49s/it, loss=0.2, v_num=0, train/loss_simple_step=0.141, train/loss_vlb_step=0.000723, train/loss_step=0.141, global_step=285.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  34%|███▍      | 82/240 [06:04<11:42,  4.45s/it, loss=0.2, v_num=0, train/loss_simple_step=0.141, train/loss_vlb_step=0.000723, train/loss_step=0.141, global_step=285.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  34%|███▍      | 82/240 [06:04<11:42,  4.45s/it, loss=0.199, v_num=0, train/loss_simple_step=0.153, train/loss_vlb_step=0.000698, train/loss_step=0.153, global_step=286.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  35%|███▍      | 83/240 [06:05<11:31,  4.40s/it, loss=0.199, v_num=0, train/loss_simple_step=0.153, train/loss_vlb_step=0.000698, train/loss_step=0.153, global_step=286.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  35%|███▍      | 83/240 [06:05<11:31,  4.41s/it, loss=0.199, v_num=0, train/loss_simple_step=0.160, train/loss_vlb_step=0.00121, train/loss_step=0.160, global_step=287.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:  35%|███▌      | 84/240 [06:06<11:20,  4.36s/it, loss=0.199, v_num=0, train/loss_simple_step=0.160, train/loss_vlb_step=0.00121, train/loss_step=0.160, global_step=287.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  35%|███▌      | 84/240 [06:06<11:21,  4.37s/it, loss=0.197, v_num=0, train/loss_simple_step=0.215, train/loss_vlb_step=0.00156, train/loss_step=0.215, global_step=288.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  35%|███▌      | 85/240 [06:07<11:10,  4.32s/it, loss=0.197, v_num=0, train/loss_simple_step=0.215, train/loss_vlb_step=0.00156, train/loss_step=0.215, global_step=288.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  35%|███▌      | 85/240 [06:07<11:10,  4.33s/it, loss=0.193, v_num=0, train/loss_simple_step=0.129, train/loss_vlb_step=0.000614, train/loss_step=0.129, global_step=289.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  36%|███▌      | 86/240 [06:08<11:00,  4.29s/it, loss=0.193, v_num=0, train/loss_simple_step=0.129, train/loss_vlb_step=0.000614, train/loss_step=0.129, global_step=289.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  36%|███▌      | 86/240 [06:08<11:00,  4.29s/it, loss=0.191, v_num=0, train/loss_simple_step=0.169, train/loss_vlb_step=0.00226, train/loss_step=0.169, global_step=290.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:  36%|███▋      | 87/240 [06:09<10:50,  4.25s/it, loss=0.191, v_num=0, train/loss_simple_step=0.169, train/loss_vlb_step=0.00226, train/loss_step=0.169, global_step=290.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  36%|███▋      | 87/240 [06:09<10:50,  4.25s/it, loss=0.193, v_num=0, train/loss_simple_step=0.215, train/loss_vlb_step=0.00304, train/loss_step=0.215, global_step=291.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  37%|███▋      | 88/240 [06:10<10:40,  4.21s/it, loss=0.193, v_num=0, train/loss_simple_step=0.215, train/loss_vlb_step=0.00304, train/loss_step=0.215, global_step=291.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  37%|███▋      | 88/240 [06:10<10:40,  4.21s/it, loss=0.192, v_num=0, train/loss_simple_step=0.180, train/loss_vlb_step=0.00134, train/loss_step=0.180, global_step=292.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  37%|███▋      | 89/240 [06:11<10:30,  4.18s/it, loss=0.192, v_num=0, train/loss_simple_step=0.180, train/loss_vlb_step=0.00134, train/loss_step=0.180, global_step=292.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  37%|███▋      | 89/240 [06:11<10:30,  4.18s/it, loss=0.19, v_num=0, train/loss_simple_step=0.177, train/loss_vlb_step=0.00133, train/loss_step=0.177, global_step=293.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:  38%|███▊      | 90/240 [06:12<10:21,  4.14s/it, loss=0.19, v_num=0, train/loss_simple_step=0.177, train/loss_vlb_step=0.00133, train/loss_step=0.177, global_step=293.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  38%|███▊      | 90/240 [06:12<10:21,  4.14s/it, loss=0.187, v_num=0, train/loss_simple_step=0.132, train/loss_vlb_step=0.000664, train/loss_step=0.132, global_step=294.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  38%|███▊      | 91/240 [06:13<10:11,  4.11s/it, loss=0.187, v_num=0, train/loss_simple_step=0.132, train/loss_vlb_step=0.000664, train/loss_step=0.132, global_step=294.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  38%|███▊      | 91/240 [06:13<10:12,  4.11s/it, loss=0.185, v_num=0, train/loss_simple_step=0.181, train/loss_vlb_step=0.00115, train/loss_step=0.181, global_step=295.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:  38%|███▊      | 92/240 [06:14<10:02,  4.07s/it, loss=0.185, v_num=0, train/loss_simple_step=0.181, train/loss_vlb_step=0.00115, train/loss_step=0.181, global_step=295.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  38%|███▊      | 92/240 [06:14<10:03,  4.07s/it, loss=0.182, v_num=0, train/loss_simple_step=0.131, train/loss_vlb_step=0.00064, train/loss_step=0.131, global_step=296.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  39%|███▉      | 93/240 [06:15<09:53,  4.04s/it, loss=0.182, v_num=0, train/loss_simple_step=0.131, train/loss_vlb_step=0.00064, train/loss_step=0.131, global_step=296.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  39%|███▉      | 93/240 [06:15<09:54,  4.04s/it, loss=0.176, v_num=0, train/loss_simple_step=0.113, train/loss_vlb_step=0.00122, train/loss_step=0.113, global_step=297.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  39%|███▉      | 94/240 [06:16<09:45,  4.01s/it, loss=0.176, v_num=0, train/loss_simple_step=0.113, train/loss_vlb_step=0.00122, train/loss_step=0.113, global_step=297.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  39%|███▉      | 94/240 [06:16<09:45,  4.01s/it, loss=0.175, v_num=0, train/loss_simple_step=0.206, train/loss_vlb_step=0.0016, train/loss_step=0.206, global_step=298.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:  40%|███▉      | 95/240 [06:17<09:36,  3.98s/it, loss=0.175, v_num=0, train/loss_simple_step=0.206, train/loss_vlb_step=0.0016, train/loss_step=0.206, global_step=298.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  40%|███▉      | 95/240 [06:17<09:36,  3.98s/it, loss=0.175, v_num=0, train/loss_simple_step=0.197, train/loss_vlb_step=0.00171, train/loss_step=0.197, global_step=299.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  40%|████      | 96/240 [06:18<09:28,  3.95s/it, loss=0.175, v_num=0, train/loss_simple_step=0.197, train/loss_vlb_step=0.00171, train/loss_step=0.197, global_step=299.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  40%|████      | 96/240 [06:18<09:28,  3.95s/it, loss=0.177, v_num=0, train/loss_simple_step=0.256, train/loss_vlb_step=0.00309, train/loss_step=0.256, global_step=300.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  40%|████      | 97/240 [06:19<09:19,  3.92s/it, loss=0.177, v_num=0, train/loss_simple_step=0.256, train/loss_vlb_step=0.00309, train/loss_step=0.256, global_step=300.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  40%|████      | 97/240 [06:19<09:20,  3.92s/it, loss=0.176, v_num=0, train/loss_simple_step=0.171, train/loss_vlb_step=0.00278, train/loss_step=0.171, global_step=301.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  41%|████      | 98/240 [06:20<09:11,  3.89s/it, loss=0.176, v_num=0, train/loss_simple_step=0.171, train/loss_vlb_step=0.00278, train/loss_step=0.171, global_step=301.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  41%|████      | 98/240 [06:20<09:11,  3.89s/it, loss=0.177, v_num=0, train/loss_simple_step=0.197, train/loss_vlb_step=0.00241, train/loss_step=0.197, global_step=302.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  41%|████▏     | 99/240 [06:21<09:03,  3.86s/it, loss=0.177, v_num=0, train/loss_simple_step=0.197, train/loss_vlb_step=0.00241, train/loss_step=0.197, global_step=302.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  41%|████▏     | 99/240 [06:21<09:03,  3.86s/it, loss=0.174, v_num=0, train/loss_simple_step=0.166, train/loss_vlb_step=0.00263, train/loss_step=0.166, global_step=303.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  42%|████▏     | 100/240 [06:22<08:55,  3.83s/it, loss=0.174, v_num=0, train/loss_simple_step=0.166, train/loss_vlb_step=0.00263, train/loss_step=0.166, global_step=303.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  42%|████▏     | 100/240 [06:22<08:56,  3.83s/it, loss=0.182, v_num=0, train/loss_simple_step=0.356, train/loss_vlb_step=0.0166, train/loss_step=0.356, global_step=304.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:  42%|████▏     | 101/240 [06:23<08:48,  3.80s/it, loss=0.182, v_num=0, train/loss_simple_step=0.356, train/loss_vlb_step=0.0166, train/loss_step=0.356, global_step=304.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  42%|████▏     | 101/240 [06:23<08:48,  3.80s/it, loss=0.186, v_num=0, train/loss_simple_step=0.207, train/loss_vlb_step=0.00197, train/loss_step=0.207, global_step=305.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  42%|████▎     | 102/240 [06:24<08:40,  3.77s/it, loss=0.186, v_num=0, train/loss_simple_step=0.207, train/loss_vlb_step=0.00197, train/loss_step=0.207, global_step=305.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  42%|████▎     | 102/240 [06:24<08:40,  3.77s/it, loss=0.186, v_num=0, train/loss_simple_step=0.161, train/loss_vlb_step=0.00142, train/loss_step=0.161, global_step=306.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  43%|████▎     | 103/240 [06:25<08:33,  3.75s/it, loss=0.186, v_num=0, train/loss_simple_step=0.161, train/loss_vlb_step=0.00142, train/loss_step=0.161, global_step=306.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  43%|████▎     | 103/240 [06:25<08:33,  3.75s/it, loss=0.19, v_num=0, train/loss_simple_step=0.235, train/loss_vlb_step=0.00185, train/loss_step=0.235, global_step=307.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:  43%|████▎     | 104/240 [06:26<08:25,  3.72s/it, loss=0.19, v_num=0, train/loss_simple_step=0.235, train/loss_vlb_step=0.00185, train/loss_step=0.235, global_step=307.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  43%|████▎     | 104/240 [06:27<08:26,  3.72s/it, loss=0.189, v_num=0, train/loss_simple_step=0.202, train/loss_vlb_step=0.00178, train/loss_step=0.202, global_step=308.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  44%|████▍     | 105/240 [06:27<08:18,  3.69s/it, loss=0.189, v_num=0, train/loss_simple_step=0.202, train/loss_vlb_step=0.00178, train/loss_step=0.202, global_step=308.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  44%|████▍     | 105/240 [06:28<08:18,  3.70s/it, loss=0.192, v_num=0, train/loss_simple_step=0.195, train/loss_vlb_step=0.0031, train/loss_step=0.195, global_step=309.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:  44%|████▍     | 106/240 [06:28<08:11,  3.67s/it, loss=0.192, v_num=0, train/loss_simple_step=0.195, train/loss_vlb_step=0.0031, train/loss_step=0.195, global_step=309.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  44%|████▍     | 106/240 [06:29<08:11,  3.67s/it, loss=0.192, v_num=0, train/loss_simple_step=0.155, train/loss_vlb_step=0.000754, train/loss_step=0.155, global_step=310.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  45%|████▍     | 107/240 [06:29<08:04,  3.64s/it, loss=0.192, v_num=0, train/loss_simple_step=0.155, train/loss_vlb_step=0.000754, train/loss_step=0.155, global_step=310.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  45%|████▍     | 107/240 [06:30<08:04,  3.65s/it, loss=0.19, v_num=0, train/loss_simple_step=0.176, train/loss_vlb_step=0.0015, train/loss_step=0.176, global_step=311.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]   Epoch 1:  45%|████▌     | 108/240 [06:30<07:57,  3.62s/it, loss=0.19, v_num=0, train/loss_simple_step=0.176, train/loss_vlb_step=0.0015, train/loss_step=0.176, global_step=311.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  45%|████▌     | 108/240 [06:31<07:57,  3.62s/it, loss=0.191, v_num=0, train/loss_simple_step=0.208, train/loss_vlb_step=0.00126, train/loss_step=0.208, global_step=312.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  45%|████▌     | 109/240 [06:31<07:50,  3.60s/it, loss=0.191, v_num=0, train/loss_simple_step=0.208, train/loss_vlb_step=0.00126, train/loss_step=0.208, global_step=312.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  45%|████▌     | 109/240 [06:32<07:51,  3.60s/it, loss=0.193, v_num=0, train/loss_simple_step=0.218, train/loss_vlb_step=0.0021, train/loss_step=0.218, global_step=313.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292] Epoch 1:  46%|████▌     | 110/240 [06:33<07:44,  3.57s/it, loss=0.193, v_num=0, train/loss_simple_step=0.218, train/loss_vlb_step=0.0021, train/loss_step=0.218, global_step=313.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]Epoch 1:  46%|████▌     | 110/240 [06:33<07:44,  3.57s/it, loss=0.196, v_num=0, train/loss_simple_step=0.189, train/loss_vlb_step=0.00176, train/loss_step=0.189, global_step=314.0, train/loss_simple_epoch=0.292, train/loss_vlb_epoch=0.0043, train/loss_epoch=0.292]
Global seed set to 23
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:2046: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead
  from torch.distributed._sharded_tensor import pre_load_state_dict_hook, state_dict_hook
/srv/mingyang/zhh_ldm_official/ldm/models/autoencoder.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/loggers/test_tube.py:105: LightningDeprecationWarning: The TestTubeLogger is deprecated since v1.5 and will be removed in v1.7. We recommend switching to the `pytorch_lightning.loggers.TensorBoardLogger` as an alternative.
  rank_zero_deprecation(
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:292: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  rank_zero_deprecation(
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:91: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
  rank_zero_warn(
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Global seed set to 23
Global seed set to 23
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:2046: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead
  from torch.distributed._sharded_tensor import pre_load_state_dict_hook, state_dict_hook
/srv/mingyang/zhh_ldm_official/ldm/models/autoencoder.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
Global seed set to 23
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/7
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:2046: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead
  from torch.distributed._sharded_tensor import pre_load_state_dict_hook, state_dict_hook
/srv/mingyang/zhh_ldm_official/ldm/models/autoencoder.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
Global seed set to 23
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/7
Global seed set to 23
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:2046: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead
  from torch.distributed._sharded_tensor import pre_load_state_dict_hook, state_dict_hook
/srv/mingyang/zhh_ldm_official/ldm/models/autoencoder.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
Global seed set to 23
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/7
Global seed set to 23
Global seed set to 23
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:2046: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead
  from torch.distributed._sharded_tensor import pre_load_state_dict_hook, state_dict_hook
/srv/mingyang/zhh_ldm_official/ldm/models/autoencoder.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
Global seed set to 23
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/7
Global seed set to 23
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:2046: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead
  from torch.distributed._sharded_tensor import pre_load_state_dict_hook, state_dict_hook
/srv/mingyang/zhh_ldm_official/ldm/models/autoencoder.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
Global seed set to 23
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/7
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:326: LightningDeprecationWarning: Base `LightningModule.on_train_batch_start` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.
  rank_zero_deprecation(
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.
  rank_zero_deprecation(
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_start` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.
  rank_zero_deprecation(
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:342: LightningDeprecationWarning: Base `Callback.on_train_batch_end` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.
  rank_zero_deprecation(
Global seed set to 23
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/7
/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:2046: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead
  from torch.distributed._sharded_tensor import pre_load_state_dict_hook, state_dict_hook
/srv/mingyang/zhh_ldm_official/ldm/models/autoencoder.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
Global seed set to 23
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/7
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 7 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [1,2,3,4,5,6,7]

  | Name              | Type             | Params
-------------------------------------------------------
0 | model             | DiffusionWrapper | 274 M 
1 | model_ema         | LitEma           | 0     
2 | first_stage_model | VQModelInterface | 55.3 M
-------------------------------------------------------
274 M     Trainable params
55.3 M    Non-trainable params
329 M     Total params
1,317.516 Total estimated model params size (MB)
Running on GPUs 0,1,2,3,4,5,6
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 274.06 M params.
Keeping EMAs of 370.
making attention of type 'vanilla' with 512 in_channels
Working with z of shape (1, 3, 64, 64) = 12288 dimensions.
making attention of type 'vanilla' with 512 in_channels
Restored from models/first_stage_models/vq-f4/model.ckpt with 0 missing and 55 unexpected keys (['loss.perceptual_loss.scaling_layer.shift', 'loss.perceptual_loss.scaling_layer.scale', 'loss.perceptual_loss.net.slice1.0.weight', 'loss.perceptual_loss.net.slice1.0.bias', 'loss.perceptual_loss.net.slice1.2.weight', 'loss.perceptual_loss.net.slice1.2.bias', 'loss.perceptual_loss.net.slice2.5.weight', 'loss.perceptual_loss.net.slice2.5.bias', 'loss.perceptual_loss.net.slice2.7.weight', 'loss.perceptual_loss.net.slice2.7.bias', 'loss.perceptual_loss.net.slice3.10.weight', 'loss.perceptual_loss.net.slice3.10.bias', 'loss.perceptual_loss.net.slice3.12.weight', 'loss.perceptual_loss.net.slice3.12.bias', 'loss.perceptual_loss.net.slice3.14.weight', 'loss.perceptual_loss.net.slice3.14.bias', 'loss.perceptual_loss.net.slice4.17.weight', 'loss.perceptual_loss.net.slice4.17.bias', 'loss.perceptual_loss.net.slice4.19.weight', 'loss.perceptual_loss.net.slice4.19.bias', 'loss.perceptual_loss.net.slice4.21.weight', 'loss.perceptual_loss.net.slice4.21.bias', 'loss.perceptual_loss.net.slice5.24.weight', 'loss.perceptual_loss.net.slice5.24.bias', 'loss.perceptual_loss.net.slice5.26.weight', 'loss.perceptual_loss.net.slice5.26.bias', 'loss.perceptual_loss.net.slice5.28.weight', 'loss.perceptual_loss.net.slice5.28.bias', 'loss.perceptual_loss.lin0.model.1.weight', 'loss.perceptual_loss.lin1.model.1.weight', 'loss.perceptual_loss.lin2.model.1.weight', 'loss.perceptual_loss.lin3.model.1.weight', 'loss.perceptual_loss.lin4.model.1.weight', 'loss.discriminator.main.0.weight', 'loss.discriminator.main.0.bias', 'loss.discriminator.main.2.weight', 'loss.discriminator.main.3.weight', 'loss.discriminator.main.3.bias', 'loss.discriminator.main.3.running_mean', 'loss.discriminator.main.3.running_var', 'loss.discriminator.main.3.num_batches_tracked', 'loss.discriminator.main.5.weight', 'loss.discriminator.main.6.weight', 'loss.discriminator.main.6.bias', 'loss.discriminator.main.6.running_mean', 'loss.discriminator.main.6.running_var', 'loss.discriminator.main.6.num_batches_tracked', 'loss.discriminator.main.8.weight', 'loss.discriminator.main.9.weight', 'loss.discriminator.main.9.bias', 'loss.discriminator.main.9.running_mean', 'loss.discriminator.main.9.running_var', 'loss.discriminator.main.9.num_batches_tracked', 'loss.discriminator.main.11.weight', 'loss.discriminator.main.11.bias'])
Training LatentDiffusion as an unconditional model.
Monitoring val/loss_simple_ema as checkpoint metric.
Merged modelckpt-cfg: 
{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2025-03-26T05-08-36_ffhq-ldm-vq-4/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
#### Data #####
train, FFHQTrain, 60000
validation, FFHQValidation, 10000
accumulate_grad_batches = 1
Setting learning rate to 5.88e-04 = 1 (accumulate_grad_batches) * 7 (num_gpus) * 42 (batchsize) * 2.00e-06 (base_lr)
Project config
model:
  base_learning_rate: 2.0e-06
  target: ldm.models.diffusion.ddpm.LatentDiffusion
  params:
    linear_start: 0.0015
    linear_end: 0.0195
    num_timesteps_cond: 1
    log_every_t: 200
    timesteps: 1000
    first_stage_key: image
    image_size: 64
    channels: 3
    monitor: val/loss_simple_ema
    unet_config:
      target: ldm.modules.diffusionmodules.openaimodel.UNetModel
      params:
        image_size: 64
        in_channels: 3
        out_channels: 3
        model_channels: 224
        attention_resolutions:
        - 8
        - 4
        - 2
        num_res_blocks: 2
        channel_mult:
        - 1
        - 2
        - 3
        - 4
        num_head_channels: 32
    first_stage_config:
      target: ldm.models.autoencoder.VQModelInterface
      params:
        embed_dim: 3
        n_embed: 8192
        ckpt_path: models/first_stage_models/vq-f4/model.ckpt
        ddconfig:
          double_z: false
          z_channels: 3
          resolution: 256
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult:
          - 1
          - 2
          - 4
          num_res_blocks: 2
          attn_resolutions: []
          dropout: 0.0
        lossconfig:
          target: torch.nn.Identity
    cond_stage_config: __is_unconditional__
data:
  target: main.DataModuleFromConfig
  params:
    batch_size: 42
    num_workers: 5
    wrap: false
    train:
      target: taming.data.faceshq.FFHQTrain
      params:
        size: 256
    validation:
      target: taming.data.faceshq.FFHQValidation
      params:
        size: 256

Lightning config
callbacks:
  image_logger:
    target: main.ImageLogger
    params:
      batch_frequency: 5000
      max_images: 8
      increase_log_steps: false
trainer:
  benchmark: true
  accelerator: ddp
  gpus: 0,1,2,3,4,5,6

Sanity Checking: 0it [00:00, ?it/s]Running on GPUs 0,1,2,3,4,5,6
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 274.06 M params.
Keeping EMAs of 370.
making attention of type 'vanilla' with 512 in_channels
Working with z of shape (1, 3, 64, 64) = 12288 dimensions.
making attention of type 'vanilla' with 512 in_channels
Restored from models/first_stage_models/vq-f4/model.ckpt with 0 missing and 55 unexpected keys (['loss.perceptual_loss.scaling_layer.shift', 'loss.perceptual_loss.scaling_layer.scale', 'loss.perceptual_loss.net.slice1.0.weight', 'loss.perceptual_loss.net.slice1.0.bias', 'loss.perceptual_loss.net.slice1.2.weight', 'loss.perceptual_loss.net.slice1.2.bias', 'loss.perceptual_loss.net.slice2.5.weight', 'loss.perceptual_loss.net.slice2.5.bias', 'loss.perceptual_loss.net.slice2.7.weight', 'loss.perceptual_loss.net.slice2.7.bias', 'loss.perceptual_loss.net.slice3.10.weight', 'loss.perceptual_loss.net.slice3.10.bias', 'loss.perceptual_loss.net.slice3.12.weight', 'loss.perceptual_loss.net.slice3.12.bias', 'loss.perceptual_loss.net.slice3.14.weight', 'loss.perceptual_loss.net.slice3.14.bias', 'loss.perceptual_loss.net.slice4.17.weight', 'loss.perceptual_loss.net.slice4.17.bias', 'loss.perceptual_loss.net.slice4.19.weight', 'loss.perceptual_loss.net.slice4.19.bias', 'loss.perceptual_loss.net.slice4.21.weight', 'loss.perceptual_loss.net.slice4.21.bias', 'loss.perceptual_loss.net.slice5.24.weight', 'loss.perceptual_loss.net.slice5.24.bias', 'loss.perceptual_loss.net.slice5.26.weight', 'loss.perceptual_loss.net.slice5.26.bias', 'loss.perceptual_loss.net.slice5.28.weight', 'loss.perceptual_loss.net.slice5.28.bias', 'loss.perceptual_loss.lin0.model.1.weight', 'loss.perceptual_loss.lin1.model.1.weight', 'loss.perceptual_loss.lin2.model.1.weight', 'loss.perceptual_loss.lin3.model.1.weight', 'loss.perceptual_loss.lin4.model.1.weight', 'loss.discriminator.main.0.weight', 'loss.discriminator.main.0.bias', 'loss.discriminator.main.2.weight', 'loss.discriminator.main.3.weight', 'loss.discriminator.main.3.bias', 'loss.discriminator.main.3.running_mean', 'loss.discriminator.main.3.running_var', 'loss.discriminator.main.3.num_batches_tracked', 'loss.discriminator.main.5.weight', 'loss.discriminator.main.6.weight', 'loss.discriminator.main.6.bias', 'loss.discriminator.main.6.running_mean', 'loss.discriminator.main.6.running_var', 'loss.discriminator.main.6.num_batches_tracked', 'loss.discriminator.main.8.weight', 'loss.discriminator.main.9.weight', 'loss.discriminator.main.9.bias', 'loss.discriminator.main.9.running_mean', 'loss.discriminator.main.9.running_var', 'loss.discriminator.main.9.num_batches_tracked', 'loss.discriminator.main.11.weight', 'loss.discriminator.main.11.bias'])
Training LatentDiffusion as an unconditional model.
Monitoring val/loss_simple_ema as checkpoint metric.
Merged modelckpt-cfg: 
{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2025-03-26T05-08-45_ffhq-ldm-vq-4/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
#### Data #####
train, FFHQTrain, 60000
validation, FFHQValidation, 10000
accumulate_grad_batches = 1
Setting learning rate to 5.88e-04 = 1 (accumulate_grad_batches) * 7 (num_gpus) * 42 (batchsize) * 2.00e-06 (base_lr)
Running on GPUs 0,1,2,3,4,5,6
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 274.06 M params.
Keeping EMAs of 370.
making attention of type 'vanilla' with 512 in_channels
Working with z of shape (1, 3, 64, 64) = 12288 dimensions.
making attention of type 'vanilla' with 512 in_channels
Restored from models/first_stage_models/vq-f4/model.ckpt with 0 missing and 55 unexpected keys (['loss.perceptual_loss.scaling_layer.shift', 'loss.perceptual_loss.scaling_layer.scale', 'loss.perceptual_loss.net.slice1.0.weight', 'loss.perceptual_loss.net.slice1.0.bias', 'loss.perceptual_loss.net.slice1.2.weight', 'loss.perceptual_loss.net.slice1.2.bias', 'loss.perceptual_loss.net.slice2.5.weight', 'loss.perceptual_loss.net.slice2.5.bias', 'loss.perceptual_loss.net.slice2.7.weight', 'loss.perceptual_loss.net.slice2.7.bias', 'loss.perceptual_loss.net.slice3.10.weight', 'loss.perceptual_loss.net.slice3.10.bias', 'loss.perceptual_loss.net.slice3.12.weight', 'loss.perceptual_loss.net.slice3.12.bias', 'loss.perceptual_loss.net.slice3.14.weight', 'loss.perceptual_loss.net.slice3.14.bias', 'loss.perceptual_loss.net.slice4.17.weight', 'loss.perceptual_loss.net.slice4.17.bias', 'loss.perceptual_loss.net.slice4.19.weight', 'loss.perceptual_loss.net.slice4.19.bias', 'loss.perceptual_loss.net.slice4.21.weight', 'loss.perceptual_loss.net.slice4.21.bias', 'loss.perceptual_loss.net.slice5.24.weight', 'loss.perceptual_loss.net.slice5.24.bias', 'loss.perceptual_loss.net.slice5.26.weight', 'loss.perceptual_loss.net.slice5.26.bias', 'loss.perceptual_loss.net.slice5.28.weight', 'loss.perceptual_loss.net.slice5.28.bias', 'loss.perceptual_loss.lin0.model.1.weight', 'loss.perceptual_loss.lin1.model.1.weight', 'loss.perceptual_loss.lin2.model.1.weight', 'loss.perceptual_loss.lin3.model.1.weight', 'loss.perceptual_loss.lin4.model.1.weight', 'loss.discriminator.main.0.weight', 'loss.discriminator.main.0.bias', 'loss.discriminator.main.2.weight', 'loss.discriminator.main.3.weight', 'loss.discriminator.main.3.bias', 'loss.discriminator.main.3.running_mean', 'loss.discriminator.main.3.running_var', 'loss.discriminator.main.3.num_batches_tracked', 'loss.discriminator.main.5.weight', 'loss.discriminator.main.6.weight', 'loss.discriminator.main.6.bias', 'loss.discriminator.main.6.running_mean', 'loss.discriminator.main.6.running_var', 'loss.discriminator.main.6.num_batches_tracked', 'loss.discriminator.main.8.weight', 'loss.discriminator.main.9.weight', 'loss.discriminator.main.9.bias', 'loss.discriminator.main.9.running_mean', 'loss.discriminator.main.9.running_var', 'loss.discriminator.main.9.num_batches_tracked', 'loss.discriminator.main.11.weight', 'loss.discriminator.main.11.bias'])
Training LatentDiffusion as an unconditional model.
Monitoring val/loss_simple_ema as checkpoint metric.
Merged modelckpt-cfg: 
{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2025-03-26T05-08-42_ffhq-ldm-vq-4/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
#### Data #####
train, FFHQTrain, 60000
validation, FFHQValidation, 10000
accumulate_grad_batches = 1
Setting learning rate to 5.88e-04 = 1 (accumulate_grad_batches) * 7 (num_gpus) * 42 (batchsize) * 2.00e-06 (base_lr)
Running on GPUs 0,1,2,3,4,5,6
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 274.06 M params.
Keeping EMAs of 370.
making attention of type 'vanilla' with 512 in_channels
Working with z of shape (1, 3, 64, 64) = 12288 dimensions.
making attention of type 'vanilla' with 512 in_channels
Restored from models/first_stage_models/vq-f4/model.ckpt with 0 missing and 55 unexpected keys (['loss.perceptual_loss.scaling_layer.shift', 'loss.perceptual_loss.scaling_layer.scale', 'loss.perceptual_loss.net.slice1.0.weight', 'loss.perceptual_loss.net.slice1.0.bias', 'loss.perceptual_loss.net.slice1.2.weight', 'loss.perceptual_loss.net.slice1.2.bias', 'loss.perceptual_loss.net.slice2.5.weight', 'loss.perceptual_loss.net.slice2.5.bias', 'loss.perceptual_loss.net.slice2.7.weight', 'loss.perceptual_loss.net.slice2.7.bias', 'loss.perceptual_loss.net.slice3.10.weight', 'loss.perceptual_loss.net.slice3.10.bias', 'loss.perceptual_loss.net.slice3.12.weight', 'loss.perceptual_loss.net.slice3.12.bias', 'loss.perceptual_loss.net.slice3.14.weight', 'loss.perceptual_loss.net.slice3.14.bias', 'loss.perceptual_loss.net.slice4.17.weight', 'loss.perceptual_loss.net.slice4.17.bias', 'loss.perceptual_loss.net.slice4.19.weight', 'loss.perceptual_loss.net.slice4.19.bias', 'loss.perceptual_loss.net.slice4.21.weight', 'loss.perceptual_loss.net.slice4.21.bias', 'loss.perceptual_loss.net.slice5.24.weight', 'loss.perceptual_loss.net.slice5.24.bias', 'loss.perceptual_loss.net.slice5.26.weight', 'loss.perceptual_loss.net.slice5.26.bias', 'loss.perceptual_loss.net.slice5.28.weight', 'loss.perceptual_loss.net.slice5.28.bias', 'loss.perceptual_loss.lin0.model.1.weight', 'loss.perceptual_loss.lin1.model.1.weight', 'loss.perceptual_loss.lin2.model.1.weight', 'loss.perceptual_loss.lin3.model.1.weight', 'loss.perceptual_loss.lin4.model.1.weight', 'loss.discriminator.main.0.weight', 'loss.discriminator.main.0.bias', 'loss.discriminator.main.2.weight', 'loss.discriminator.main.3.weight', 'loss.discriminator.main.3.bias', 'loss.discriminator.main.3.running_mean', 'loss.discriminator.main.3.running_var', 'loss.discriminator.main.3.num_batches_tracked', 'loss.discriminator.main.5.weight', 'loss.discriminator.main.6.weight', 'loss.discriminator.main.6.bias', 'loss.discriminator.main.6.running_mean', 'loss.discriminator.main.6.running_var', 'loss.discriminator.main.6.num_batches_tracked', 'loss.discriminator.main.8.weight', 'loss.discriminator.main.9.weight', 'loss.discriminator.main.9.bias', 'loss.discriminator.main.9.running_mean', 'loss.discriminator.main.9.running_var', 'loss.discriminator.main.9.num_batches_tracked', 'loss.discriminator.main.11.weight', 'loss.discriminator.main.11.bias'])
Training LatentDiffusion as an unconditional model.
Monitoring val/loss_simple_ema as checkpoint metric.
Merged modelckpt-cfg: 
{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2025-03-26T05-08-54_ffhq-ldm-vq-4/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
#### Data #####
train, FFHQTrain, 60000
validation, FFHQValidation, 10000
accumulate_grad_batches = 1
Setting learning rate to 5.88e-04 = 1 (accumulate_grad_batches) * 7 (num_gpus) * 42 (batchsize) * 2.00e-06 (base_lr)
Running on GPUs 0,1,2,3,4,5,6
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 274.06 M params.
Keeping EMAs of 370.
making attention of type 'vanilla' with 512 in_channels
Working with z of shape (1, 3, 64, 64) = 12288 dimensions.
making attention of type 'vanilla' with 512 in_channels
Restored from models/first_stage_models/vq-f4/model.ckpt with 0 missing and 55 unexpected keys (['loss.perceptual_loss.scaling_layer.shift', 'loss.perceptual_loss.scaling_layer.scale', 'loss.perceptual_loss.net.slice1.0.weight', 'loss.perceptual_loss.net.slice1.0.bias', 'loss.perceptual_loss.net.slice1.2.weight', 'loss.perceptual_loss.net.slice1.2.bias', 'loss.perceptual_loss.net.slice2.5.weight', 'loss.perceptual_loss.net.slice2.5.bias', 'loss.perceptual_loss.net.slice2.7.weight', 'loss.perceptual_loss.net.slice2.7.bias', 'loss.perceptual_loss.net.slice3.10.weight', 'loss.perceptual_loss.net.slice3.10.bias', 'loss.perceptual_loss.net.slice3.12.weight', 'loss.perceptual_loss.net.slice3.12.bias', 'loss.perceptual_loss.net.slice3.14.weight', 'loss.perceptual_loss.net.slice3.14.bias', 'loss.perceptual_loss.net.slice4.17.weight', 'loss.perceptual_loss.net.slice4.17.bias', 'loss.perceptual_loss.net.slice4.19.weight', 'loss.perceptual_loss.net.slice4.19.bias', 'loss.perceptual_loss.net.slice4.21.weight', 'loss.perceptual_loss.net.slice4.21.bias', 'loss.perceptual_loss.net.slice5.24.weight', 'loss.perceptual_loss.net.slice5.24.bias', 'loss.perceptual_loss.net.slice5.26.weight', 'loss.perceptual_loss.net.slice5.26.bias', 'loss.perceptual_loss.net.slice5.28.weight', 'loss.perceptual_loss.net.slice5.28.bias', 'loss.perceptual_loss.lin0.model.1.weight', 'loss.perceptual_loss.lin1.model.1.weight', 'loss.perceptual_loss.lin2.model.1.weight', 'loss.perceptual_loss.lin3.model.1.weight', 'loss.perceptual_loss.lin4.model.1.weight', 'loss.discriminator.main.0.weight', 'loss.discriminator.main.0.bias', 'loss.discriminator.main.2.weight', 'loss.discriminator.main.3.weight', 'loss.discriminator.main.3.bias', 'loss.discriminator.main.3.running_mean', 'loss.discriminator.main.3.running_var', 'loss.discriminator.main.3.num_batches_tracked', 'loss.discriminator.main.5.weight', 'loss.discriminator.main.6.weight', 'loss.discriminator.main.6.bias', 'loss.discriminator.main.6.running_mean', 'loss.discriminator.main.6.running_var', 'loss.discriminator.main.6.num_batches_tracked', 'loss.discriminator.main.8.weight', 'loss.discriminator.main.9.weight', 'loss.discriminator.main.9.bias', 'loss.discriminator.main.9.running_mean', 'loss.discriminator.main.9.running_var', 'loss.discriminator.main.9.num_batches_tracked', 'loss.discriminator.main.11.weight', 'loss.discriminator.main.11.bias'])
Training LatentDiffusion as an unconditional model.
Monitoring val/loss_simple_ema as checkpoint metric.
Merged modelckpt-cfg: 
{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2025-03-26T05-08-50_ffhq-ldm-vq-4/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
#### Data #####
train, FFHQTrain, 60000
validation, FFHQValidation, 10000
accumulate_grad_batches = 1
Setting learning rate to 5.88e-04 = 1 (accumulate_grad_batches) * 7 (num_gpus) * 42 (batchsize) * 2.00e-06 (base_lr)
Running on GPUs 0,1,2,3,4,5,6
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 274.06 M params.
Keeping EMAs of 370.
making attention of type 'vanilla' with 512 in_channels
Working with z of shape (1, 3, 64, 64) = 12288 dimensions.
making attention of type 'vanilla' with 512 in_channels
Restored from models/first_stage_models/vq-f4/model.ckpt with 0 missing and 55 unexpected keys (['loss.perceptual_loss.scaling_layer.shift', 'loss.perceptual_loss.scaling_layer.scale', 'loss.perceptual_loss.net.slice1.0.weight', 'loss.perceptual_loss.net.slice1.0.bias', 'loss.perceptual_loss.net.slice1.2.weight', 'loss.perceptual_loss.net.slice1.2.bias', 'loss.perceptual_loss.net.slice2.5.weight', 'loss.perceptual_loss.net.slice2.5.bias', 'loss.perceptual_loss.net.slice2.7.weight', 'loss.perceptual_loss.net.slice2.7.bias', 'loss.perceptual_loss.net.slice3.10.weight', 'loss.perceptual_loss.net.slice3.10.bias', 'loss.perceptual_loss.net.slice3.12.weight', 'loss.perceptual_loss.net.slice3.12.bias', 'loss.perceptual_loss.net.slice3.14.weight', 'loss.perceptual_loss.net.slice3.14.bias', 'loss.perceptual_loss.net.slice4.17.weight', 'loss.perceptual_loss.net.slice4.17.bias', 'loss.perceptual_loss.net.slice4.19.weight', 'loss.perceptual_loss.net.slice4.19.bias', 'loss.perceptual_loss.net.slice4.21.weight', 'loss.perceptual_loss.net.slice4.21.bias', 'loss.perceptual_loss.net.slice5.24.weight', 'loss.perceptual_loss.net.slice5.24.bias', 'loss.perceptual_loss.net.slice5.26.weight', 'loss.perceptual_loss.net.slice5.26.bias', 'loss.perceptual_loss.net.slice5.28.weight', 'loss.perceptual_loss.net.slice5.28.bias', 'loss.perceptual_loss.lin0.model.1.weight', 'loss.perceptual_loss.lin1.model.1.weight', 'loss.perceptual_loss.lin2.model.1.weight', 'loss.perceptual_loss.lin3.model.1.weight', 'loss.perceptual_loss.lin4.model.1.weight', 'loss.discriminator.main.0.weight', 'loss.discriminator.main.0.bias', 'loss.discriminator.main.2.weight', 'loss.discriminator.main.3.weight', 'loss.discriminator.main.3.bias', 'loss.discriminator.main.3.running_mean', 'loss.discriminator.main.3.running_var', 'loss.discriminator.main.3.num_batches_tracked', 'loss.discriminator.main.5.weight', 'loss.discriminator.main.6.weight', 'loss.discriminator.main.6.bias', 'loss.discriminator.main.6.running_mean', 'loss.discriminator.main.6.running_var', 'loss.discriminator.main.6.num_batches_tracked', 'loss.discriminator.main.8.weight', 'loss.discriminator.main.9.weight', 'loss.discriminator.main.9.bias', 'loss.discriminator.main.9.running_mean', 'loss.discriminator.main.9.running_var', 'loss.discriminator.main.9.num_batches_tracked', 'loss.discriminator.main.11.weight', 'loss.discriminator.main.11.bias'])
Training LatentDiffusion as an unconditional model.
Monitoring val/loss_simple_ema as checkpoint metric.
Merged modelckpt-cfg: 
{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2025-03-26T05-08-56_ffhq-ldm-vq-4/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
#### Data #####
train, FFHQTrain, 60000
validation, FFHQValidation, 10000
accumulate_grad_batches = 1
Setting learning rate to 5.88e-04 = 1 (accumulate_grad_batches) * 7 (num_gpus) * 42 (batchsize) * 2.00e-06 (base_lr)
Running on GPUs 0,1,2,3,4,5,6
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 274.06 M params.
Keeping EMAs of 370.
making attention of type 'vanilla' with 512 in_channels
Working with z of shape (1, 3, 64, 64) = 12288 dimensions.
making attention of type 'vanilla' with 512 in_channels
Restored from models/first_stage_models/vq-f4/model.ckpt with 0 missing and 55 unexpected keys (['loss.perceptual_loss.scaling_layer.shift', 'loss.perceptual_loss.scaling_layer.scale', 'loss.perceptual_loss.net.slice1.0.weight', 'loss.perceptual_loss.net.slice1.0.bias', 'loss.perceptual_loss.net.slice1.2.weight', 'loss.perceptual_loss.net.slice1.2.bias', 'loss.perceptual_loss.net.slice2.5.weight', 'loss.perceptual_loss.net.slice2.5.bias', 'loss.perceptual_loss.net.slice2.7.weight', 'loss.perceptual_loss.net.slice2.7.bias', 'loss.perceptual_loss.net.slice3.10.weight', 'loss.perceptual_loss.net.slice3.10.bias', 'loss.perceptual_loss.net.slice3.12.weight', 'loss.perceptual_loss.net.slice3.12.bias', 'loss.perceptual_loss.net.slice3.14.weight', 'loss.perceptual_loss.net.slice3.14.bias', 'loss.perceptual_loss.net.slice4.17.weight', 'loss.perceptual_loss.net.slice4.17.bias', 'loss.perceptual_loss.net.slice4.19.weight', 'loss.perceptual_loss.net.slice4.19.bias', 'loss.perceptual_loss.net.slice4.21.weight', 'loss.perceptual_loss.net.slice4.21.bias', 'loss.perceptual_loss.net.slice5.24.weight', 'loss.perceptual_loss.net.slice5.24.bias', 'loss.perceptual_loss.net.slice5.26.weight', 'loss.perceptual_loss.net.slice5.26.bias', 'loss.perceptual_loss.net.slice5.28.weight', 'loss.perceptual_loss.net.slice5.28.bias', 'loss.perceptual_loss.lin0.model.1.weight', 'loss.perceptual_loss.lin1.model.1.weight', 'loss.perceptual_loss.lin2.model.1.weight', 'loss.perceptual_loss.lin3.model.1.weight', 'loss.perceptual_loss.lin4.model.1.weight', 'loss.discriminator.main.0.weight', 'loss.discriminator.main.0.bias', 'loss.discriminator.main.2.weight', 'loss.discriminator.main.3.weight', 'loss.discriminator.main.3.bias', 'loss.discriminator.main.3.running_mean', 'loss.discriminator.main.3.running_var', 'loss.discriminator.main.3.num_batches_tracked', 'loss.discriminator.main.5.weight', 'loss.discriminator.main.6.weight', 'loss.discriminator.main.6.bias', 'loss.discriminator.main.6.running_mean', 'loss.discriminator.main.6.running_var', 'loss.discriminator.main.6.num_batches_tracked', 'loss.discriminator.main.8.weight', 'loss.discriminator.main.9.weight', 'loss.discriminator.main.9.bias', 'loss.discriminator.main.9.running_mean', 'loss.discriminator.main.9.running_var', 'loss.discriminator.main.9.num_batches_tracked', 'loss.discriminator.main.11.weight', 'loss.discriminator.main.11.bias'])
Training LatentDiffusion as an unconditional model.
Monitoring val/loss_simple_ema as checkpoint metric.
Merged modelckpt-cfg: 
{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2025-03-26T05-08-58_ffhq-ldm-vq-4/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
#### Data #####
train, FFHQTrain, 60000
validation, FFHQValidation, 10000
accumulate_grad_batches = 1
Setting learning rate to 5.88e-04 = 1 (accumulate_grad_batches) * 7 (num_gpus) * 42 (batchsize) * 2.00e-06 (base_lr)
Sanity Checking DataLoader 0:   0%|          | 0/2 [00:01<?, ?it/s]Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:08<00:08,  8.07s/it]Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:08<00:00,  4.49s/it]                                                                           /srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 42. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
Training: 0it [00:00, ?it/s]Epoch 0:   0%|          | 0/240 [00:00<?, ?it/s]/srv/mingyang/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2085: LightningDeprecationWarning: `Trainer.root_gpu` is deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.strategy.root_device.index` instead.
  rank_zero_deprecation(
[rank6]:[W326 05:09:31.737652442 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W326 05:09:31.740427705 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W326 05:09:31.748841527 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank4]:[W326 05:09:31.750987248 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank5]:[W326 05:09:31.761144182 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W326 05:09:31.811828388 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W326 05:09:31.831839249 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Epoch 0:   0%|          | 1/240 [00:14<59:25, 14.92s/it]Epoch 0:   0%|          | 1/240 [00:14<59:26, 14.92s/it, loss=1, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00646, train/loss_step=1.000, global_step=0.000]Epoch 0:   1%|          | 2/240 [00:15<31:11,  7.86s/it, loss=1, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00646, train/loss_step=1.000, global_step=0.000]Epoch 0:   1%|          | 2/240 [00:15<31:32,  7.95s/it, loss=0.929, v_num=0, train/loss_simple_step=0.856, train/loss_vlb_step=0.00558, train/loss_step=0.856, global_step=1.000]Epoch 0:   1%|▏         | 3/240 [00:16<22:01,  5.58s/it, loss=0.929, v_num=0, train/loss_simple_step=0.856, train/loss_vlb_step=0.00558, train/loss_step=0.856, global_step=1.000]Epoch 0:   1%|▏         | 3/240 [00:16<22:15,  5.63s/it, loss=0.882, v_num=0, train/loss_simple_step=0.789, train/loss_vlb_step=0.0071, train/loss_step=0.789, global_step=2.000] Epoch 0:   2%|▏         | 4/240 [00:17<17:27,  4.44s/it, loss=0.882, v_num=0, train/loss_simple_step=0.789, train/loss_vlb_step=0.0071, train/loss_step=0.789, global_step=2.000]Epoch 0:   2%|▏         | 4/240 [00:17<17:37,  4.48s/it, loss=0.808, v_num=0, train/loss_simple_step=0.584, train/loss_vlb_step=0.00799, train/loss_step=0.584, global_step=3.000]Epoch 0:   2%|▏         | 5/240 [00:18<14:42,  3.75s/it, loss=0.808, v_num=0, train/loss_simple_step=0.584, train/loss_vlb_step=0.00799, train/loss_step=0.584, global_step=3.000]Epoch 0:   2%|▏         | 5/240 [00:18<14:49,  3.78s/it, loss=0.761, v_num=0, train/loss_simple_step=0.572, train/loss_vlb_step=0.00434, train/loss_step=0.572, global_step=4.000]Epoch 0:   2%|▎         | 6/240 [00:19<12:51,  3.30s/it, loss=0.761, v_num=0, train/loss_simple_step=0.572, train/loss_vlb_step=0.00434, train/loss_step=0.572, global_step=4.000]Epoch 0:   2%|▎         | 6/240 [00:19<12:57,  3.32s/it, loss=0.704, v_num=0, train/loss_simple_step=0.419, train/loss_vlb_step=0.00348, train/loss_step=0.419, global_step=5.000]Epoch 0:   3%|▎         | 7/240 [00:20<11:31,  2.97s/it, loss=0.704, v_num=0, train/loss_simple_step=0.419, train/loss_vlb_step=0.00348, train/loss_step=0.419, global_step=5.000]Epoch 0:   3%|▎         | 7/240 [00:20<11:36,  2.99s/it, loss=0.73, v_num=0, train/loss_simple_step=0.884, train/loss_vlb_step=0.00497, train/loss_step=0.884, global_step=6.000] Epoch 0:   3%|▎         | 8/240 [00:21<10:31,  2.72s/it, loss=0.73, v_num=0, train/loss_simple_step=0.884, train/loss_vlb_step=0.00497, train/loss_step=0.884, global_step=6.000]Epoch 0:   3%|▎         | 8/240 [00:21<10:36,  2.74s/it, loss=0.693, v_num=0, train/loss_simple_step=0.435, train/loss_vlb_step=0.00339, train/loss_step=0.435, global_step=7.000]Epoch 0:   4%|▍         | 9/240 [00:22<09:44,  2.53s/it, loss=0.693, v_num=0, train/loss_simple_step=0.435, train/loss_vlb_step=0.00339, train/loss_step=0.435, global_step=7.000]Epoch 0:   4%|▍         | 9/240 [00:22<09:49,  2.55s/it, loss=0.683, v_num=0, train/loss_simple_step=0.607, train/loss_vlb_step=0.00449, train/loss_step=0.607, global_step=8.000]Epoch 0:   4%|▍         | 10/240 [00:23<09:07,  2.38s/it, loss=0.683, v_num=0, train/loss_simple_step=0.607, train/loss_vlb_step=0.00449, train/loss_step=0.607, global_step=8.000]Epoch 0:   4%|▍         | 10/240 [00:23<09:10,  2.39s/it, loss=0.655, v_num=0, train/loss_simple_step=0.403, train/loss_vlb_step=0.00451, train/loss_step=0.403, global_step=9.000]Epoch 0:   5%|▍         | 11/240 [00:24<08:36,  2.25s/it, loss=0.655, v_num=0, train/loss_simple_step=0.403, train/loss_vlb_step=0.00451, train/loss_step=0.403, global_step=9.000]Epoch 0:   5%|▍         | 11/240 [00:24<08:39,  2.27s/it, loss=0.648, v_num=0, train/loss_simple_step=0.575, train/loss_vlb_step=0.00473, train/loss_step=0.575, global_step=10.00]Epoch 0:   5%|▌         | 12/240 [00:25<08:10,  2.15s/it, loss=0.648, v_num=0, train/loss_simple_step=0.575, train/loss_vlb_step=0.00473, train/loss_step=0.575, global_step=10.00]Epoch 0:   5%|▌         | 12/240 [00:25<08:13,  2.16s/it, loss=0.627, v_num=0, train/loss_simple_step=0.394, train/loss_vlb_step=0.00325, train/loss_step=0.394, global_step=11.00]Epoch 0:   5%|▌         | 13/240 [00:26<07:48,  2.06s/it, loss=0.627, v_num=0, train/loss_simple_step=0.394, train/loss_vlb_step=0.00325, train/loss_step=0.394, global_step=11.00]Epoch 0:   5%|▌         | 13/240 [00:26<07:51,  2.08s/it, loss=0.609, v_num=0, train/loss_simple_step=0.394, train/loss_vlb_step=0.00406, train/loss_step=0.394, global_step=12.00]Epoch 0:   6%|▌         | 14/240 [00:27<07:29,  1.99s/it, loss=0.609, v_num=0, train/loss_simple_step=0.394, train/loss_vlb_step=0.00406, train/loss_step=0.394, global_step=12.00]Epoch 0:   6%|▌         | 14/240 [00:27<07:31,  2.00s/it, loss=0.597, v_num=0, train/loss_simple_step=0.440, train/loss_vlb_step=0.00302, train/loss_step=0.440, global_step=13.00]Epoch 0:   6%|▋         | 15/240 [00:28<07:12,  1.92s/it, loss=0.597, v_num=0, train/loss_simple_step=0.440, train/loss_vlb_step=0.00302, train/loss_step=0.440, global_step=13.00]Epoch 0:   6%|▋         | 15/240 [00:28<07:14,  1.93s/it, loss=0.609, v_num=0, train/loss_simple_step=0.777, train/loss_vlb_step=0.00538, train/loss_step=0.777, global_step=14.00]Epoch 0:   7%|▋         | 16/240 [00:29<06:57,  1.86s/it, loss=0.609, v_num=0, train/loss_simple_step=0.777, train/loss_vlb_step=0.00538, train/loss_step=0.777, global_step=14.00]Epoch 0:   7%|▋         | 16/240 [00:30<07:00,  1.88s/it, loss=0.608, v_num=0, train/loss_simple_step=0.591, train/loss_vlb_step=0.00358, train/loss_step=0.591, global_step=15.00]Epoch 0:   7%|▋         | 17/240 [00:30<06:44,  1.82s/it, loss=0.608, v_num=0, train/loss_simple_step=0.591, train/loss_vlb_step=0.00358, train/loss_step=0.591, global_step=15.00]Epoch 0:   7%|▋         | 17/240 [00:31<06:47,  1.83s/it, loss=0.607, v_num=0, train/loss_simple_step=0.602, train/loss_vlb_step=0.00684, train/loss_step=0.602, global_step=16.00]Epoch 0:   8%|▊         | 18/240 [00:31<06:32,  1.77s/it, loss=0.607, v_num=0, train/loss_simple_step=0.602, train/loss_vlb_step=0.00684, train/loss_step=0.602, global_step=16.00]Epoch 0:   8%|▊         | 18/240 [00:32<06:35,  1.78s/it, loss=0.6, v_num=0, train/loss_simple_step=0.478, train/loss_vlb_step=0.020, train/loss_step=0.478, global_step=17.00]    Epoch 0:   8%|▊         | 19/240 [00:32<06:22,  1.73s/it, loss=0.6, v_num=0, train/loss_simple_step=0.478, train/loss_vlb_step=0.020, train/loss_step=0.478, global_step=17.00]Epoch 0:   8%|▊         | 19/240 [00:33<06:24,  1.74s/it, loss=0.591, v_num=0, train/loss_simple_step=0.422, train/loss_vlb_step=0.00258, train/loss_step=0.422, global_step=18.00]Epoch 0:   8%|▊         | 20/240 [00:33<06:12,  1.69s/it, loss=0.591, v_num=0, train/loss_simple_step=0.422, train/loss_vlb_step=0.00258, train/loss_step=0.422, global_step=18.00]Epoch 0:   8%|▊         | 20/240 [00:34<06:14,  1.70s/it, loss=0.616, v_num=0, train/loss_simple_step=1.090, train/loss_vlb_step=0.00877, train/loss_step=1.090, global_step=19.00]Epoch 0:   9%|▉         | 21/240 [00:34<06:04,  1.66s/it, loss=0.616, v_num=0, train/loss_simple_step=1.090, train/loss_vlb_step=0.00877, train/loss_step=1.090, global_step=19.00]Epoch 0:   9%|▉         | 21/240 [00:35<06:05,  1.67s/it, loss=0.618, v_num=0, train/loss_simple_step=1.060, train/loss_vlb_step=0.0147, train/loss_step=1.060, global_step=20.00] Epoch 0:   9%|▉         | 22/240 [00:35<05:56,  1.63s/it, loss=0.618, v_num=0, train/loss_simple_step=1.060, train/loss_vlb_step=0.0147, train/loss_step=1.060, global_step=20.00]Epoch 0:   9%|▉         | 22/240 [00:36<05:57,  1.64s/it, loss=0.627, v_num=0, train/loss_simple_step=1.020, train/loss_vlb_step=0.00812, train/loss_step=1.020, global_step=21.00]Epoch 0:  10%|▉         | 23/240 [00:36<05:48,  1.61s/it, loss=0.627, v_num=0, train/loss_simple_step=1.020, train/loss_vlb_step=0.00812, train/loss_step=1.020, global_step=21.00]Epoch 0:  10%|▉         | 23/240 [00:37<05:49,  1.61s/it, loss=0.639, v_num=0, train/loss_simple_step=1.050, train/loss_vlb_step=0.00609, train/loss_step=1.050, global_step=22.00]Epoch 0:  10%|█         | 24/240 [00:37<05:41,  1.58s/it, loss=0.639, v_num=0, train/loss_simple_step=1.050, train/loss_vlb_step=0.00609, train/loss_step=1.050, global_step=22.00]Epoch 0:  10%|█         | 24/240 [00:38<05:42,  1.59s/it, loss=0.662, v_num=0, train/loss_simple_step=1.040, train/loss_vlb_step=0.0197, train/loss_step=1.040, global_step=23.00] Epoch 0:  10%|█         | 25/240 [00:38<05:34,  1.56s/it, loss=0.662, v_num=0, train/loss_simple_step=1.040, train/loss_vlb_step=0.0197, train/loss_step=1.040, global_step=23.00]Epoch 0:  10%|█         | 25/240 [00:39<05:36,  1.56s/it, loss=0.684, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.00724, train/loss_step=1.010, global_step=24.00]Epoch 0:  11%|█         | 26/240 [00:39<05:28,  1.54s/it, loss=0.684, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.00724, train/loss_step=1.010, global_step=24.00]Epoch 0:  11%|█         | 26/240 [00:40<05:29,  1.54s/it, loss=0.714, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.00726, train/loss_step=1.010, global_step=25.00]Epoch 0:  11%|█▏        | 27/240 [00:40<05:23,  1.52s/it, loss=0.714, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.00726, train/loss_step=1.010, global_step=25.00]Epoch 0:  11%|█▏        | 27/240 [00:41<05:24,  1.52s/it, loss=0.721, v_num=0, train/loss_simple_step=1.030, train/loss_vlb_step=0.0103, train/loss_step=1.030, global_step=26.00] Epoch 0:  12%|█▏        | 28/240 [00:41<05:17,  1.50s/it, loss=0.721, v_num=0, train/loss_simple_step=1.030, train/loss_vlb_step=0.0103, train/loss_step=1.030, global_step=26.00]Epoch 0:  12%|█▏        | 28/240 [00:42<05:18,  1.50s/it, loss=0.75, v_num=0, train/loss_simple_step=1.020, train/loss_vlb_step=0.00677, train/loss_step=1.020, global_step=27.00]Epoch 0:  12%|█▏        | 29/240 [00:42<05:12,  1.48s/it, loss=0.75, v_num=0, train/loss_simple_step=1.020, train/loss_vlb_step=0.00677, train/loss_step=1.020, global_step=27.00]Epoch 0:  12%|█▏        | 29/240 [00:43<05:13,  1.49s/it, loss=0.77, v_num=0, train/loss_simple_step=0.997, train/loss_vlb_step=0.0216, train/loss_step=0.997, global_step=28.00] Epoch 0:  12%|█▎        | 30/240 [00:43<05:07,  1.46s/it, loss=0.77, v_num=0, train/loss_simple_step=0.997, train/loss_vlb_step=0.0216, train/loss_step=0.997, global_step=28.00]Epoch 0:  12%|█▎        | 30/240 [00:44<05:08,  1.47s/it, loss=0.8, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.00641, train/loss_step=1.010, global_step=29.00]Epoch 0:  13%|█▎        | 31/240 [00:44<05:02,  1.45s/it, loss=0.8, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.00641, train/loss_step=1.010, global_step=29.00]Epoch 0:  13%|█▎        | 31/240 [00:45<05:03,  1.45s/it, loss=0.821, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00866, train/loss_step=1.000, global_step=30.00]Epoch 0:  13%|█▎        | 32/240 [00:45<04:58,  1.44s/it, loss=0.821, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00866, train/loss_step=1.000, global_step=30.00]Epoch 0:  13%|█▎        | 32/240 [00:46<04:59,  1.44s/it, loss=0.852, v_num=0, train/loss_simple_step=1.020, train/loss_vlb_step=0.00955, train/loss_step=1.020, global_step=31.00]Epoch 0:  14%|█▍        | 33/240 [00:46<04:54,  1.42s/it, loss=0.852, v_num=0, train/loss_simple_step=1.020, train/loss_vlb_step=0.00955, train/loss_step=1.020, global_step=31.00]Epoch 0:  14%|█▍        | 33/240 [00:47<04:55,  1.43s/it, loss=0.885, v_num=0, train/loss_simple_step=1.050, train/loss_vlb_step=0.00648, train/loss_step=1.050, global_step=32.00]Epoch 0:  14%|█▍        | 34/240 [00:47<04:50,  1.41s/it, loss=0.885, v_num=0, train/loss_simple_step=1.050, train/loss_vlb_step=0.00648, train/loss_step=1.050, global_step=32.00]Epoch 0:  14%|█▍        | 34/240 [00:48<04:51,  1.42s/it, loss=0.917, v_num=0, train/loss_simple_step=1.070, train/loss_vlb_step=0.0207, train/loss_step=1.070, global_step=33.00] Epoch 0:  15%|█▍        | 35/240 [00:48<04:46,  1.40s/it, loss=0.917, v_num=0, train/loss_simple_step=1.070, train/loss_vlb_step=0.0207, train/loss_step=1.070, global_step=33.00]Epoch 0:  15%|█▍        | 35/240 [00:49<04:48,  1.41s/it, loss=0.929, v_num=0, train/loss_simple_step=1.020, train/loss_vlb_step=0.00645, train/loss_step=1.020, global_step=34.00]Epoch 0:  15%|█▌        | 36/240 [00:49<04:43,  1.39s/it, loss=0.929, v_num=0, train/loss_simple_step=1.020, train/loss_vlb_step=0.00645, train/loss_step=1.020, global_step=34.00]Epoch 0:  15%|█▌        | 36/240 [00:50<04:44,  1.39s/it, loss=0.95, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.00653, train/loss_step=1.010, global_step=35.00] Epoch 0:  15%|█▌        | 37/240 [00:50<04:39,  1.38s/it, loss=0.95, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.00653, train/loss_step=1.010, global_step=35.00]Epoch 0:  15%|█▌        | 37/240 [00:51<04:40,  1.38s/it, loss=0.971, v_num=0, train/loss_simple_step=1.030, train/loss_vlb_step=0.00709, train/loss_step=1.030, global_step=36.00]Epoch 0:  16%|█▌        | 38/240 [00:51<04:36,  1.37s/it, loss=0.971, v_num=0, train/loss_simple_step=1.030, train/loss_vlb_step=0.00709, train/loss_step=1.030, global_step=36.00]Epoch 0:  16%|█▌        | 38/240 [00:52<04:37,  1.37s/it, loss=0.998, v_num=0, train/loss_simple_step=1.020, train/loss_vlb_step=0.00707, train/loss_step=1.020, global_step=37.00]Epoch 0:  16%|█▋        | 39/240 [00:53<04:33,  1.36s/it, loss=0.998, v_num=0, train/loss_simple_step=1.020, train/loss_vlb_step=0.00707, train/loss_step=1.020, global_step=37.00]Epoch 0:  16%|█▋        | 39/240 [00:53<04:34,  1.36s/it, loss=1.03, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.00686, train/loss_step=1.010, global_step=38.00] Epoch 0:  17%|█▋        | 40/240 [00:54<04:30,  1.35s/it, loss=1.03, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.00686, train/loss_step=1.010, global_step=38.00]Epoch 0:  17%|█▋        | 40/240 [00:54<04:31,  1.36s/it, loss=1.02, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.0199, train/loss_step=1.010, global_step=39.00] Epoch 0:  17%|█▋        | 41/240 [00:54<04:26,  1.34s/it, loss=1.02, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.0199, train/loss_step=1.010, global_step=39.00]Epoch 0:  17%|█▋        | 41/240 [00:55<04:27,  1.35s/it, loss=1.02, v_num=0, train/loss_simple_step=1.020, train/loss_vlb_step=0.0198, train/loss_step=1.020, global_step=40.00]Epoch 0:  18%|█▊        | 42/240 [00:56<04:24,  1.33s/it, loss=1.02, v_num=0, train/loss_simple_step=1.020, train/loss_vlb_step=0.0198, train/loss_step=1.020, global_step=40.00]Epoch 0:  18%|█▊        | 42/240 [00:56<04:25,  1.34s/it, loss=1.02, v_num=0, train/loss_simple_step=1.020, train/loss_vlb_step=0.00621, train/loss_step=1.020, global_step=41.00]Epoch 0:  18%|█▊        | 43/240 [00:57<04:21,  1.33s/it, loss=1.02, v_num=0, train/loss_simple_step=1.020, train/loss_vlb_step=0.00621, train/loss_step=1.020, global_step=41.00]Epoch 0:  18%|█▊        | 43/240 [00:57<04:22,  1.33s/it, loss=1.02, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00704, train/loss_step=1.000, global_step=42.00]Epoch 0:  18%|█▊        | 44/240 [00:58<04:18,  1.32s/it, loss=1.02, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00704, train/loss_step=1.000, global_step=42.00]Epoch 0:  18%|█▊        | 44/240 [00:58<04:19,  1.32s/it, loss=1.02, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00603, train/loss_step=1.000, global_step=43.00]Epoch 0:  19%|█▉        | 45/240 [00:59<04:15,  1.31s/it, loss=1.02, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00603, train/loss_step=1.000, global_step=43.00]Epoch 0:  19%|█▉        | 45/240 [00:59<04:16,  1.32s/it, loss=1.02, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.00768, train/loss_step=1.010, global_step=44.00]Epoch 0:  19%|█▉        | 46/240 [00:59<04:12,  1.30s/it, loss=1.02, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.00768, train/loss_step=1.010, global_step=44.00]Epoch 0:  19%|█▉        | 46/240 [01:00<04:13,  1.31s/it, loss=1.02, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.00934, train/loss_step=1.010, global_step=45.00]Epoch 0:  20%|█▉        | 47/240 [01:00<04:10,  1.30s/it, loss=1.02, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.00934, train/loss_step=1.010, global_step=45.00]Epoch 0:  20%|█▉        | 47/240 [01:01<04:11,  1.30s/it, loss=1.02, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.00643, train/loss_step=1.010, global_step=46.00]Epoch 0:  20%|██        | 48/240 [01:01<04:07,  1.29s/it, loss=1.02, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.00643, train/loss_step=1.010, global_step=46.00]Epoch 0:  20%|██        | 48/240 [01:02<04:08,  1.29s/it, loss=1.02, v_num=0, train/loss_simple_step=0.999, train/loss_vlb_step=0.00665, train/loss_step=0.999, global_step=47.00]Epoch 0:  20%|██        | 49/240 [01:02<04:05,  1.28s/it, loss=1.02, v_num=0, train/loss_simple_step=0.999, train/loss_vlb_step=0.00665, train/loss_step=0.999, global_step=47.00]Epoch 0:  20%|██        | 49/240 [01:03<04:06,  1.29s/it, loss=1.02, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.00584, train/loss_step=1.010, global_step=48.00]Epoch 0:  21%|██        | 50/240 [01:03<04:02,  1.28s/it, loss=1.02, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.00584, train/loss_step=1.010, global_step=48.00]Epoch 0:  21%|██        | 50/240 [01:04<04:03,  1.28s/it, loss=1.02, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.00714, train/loss_step=1.010, global_step=49.00]Epoch 0:  21%|██▏       | 51/240 [01:04<04:00,  1.27s/it, loss=1.02, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.00714, train/loss_step=1.010, global_step=49.00]Epoch 0:  21%|██▏       | 51/240 [01:05<04:01,  1.28s/it, loss=1.02, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.00682, train/loss_step=1.010, global_step=50.00]Epoch 0:  22%|██▏       | 52/240 [01:05<03:58,  1.27s/it, loss=1.02, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.00682, train/loss_step=1.010, global_step=50.00]Epoch 0:  22%|██▏       | 52/240 [01:06<03:58,  1.27s/it, loss=1.02, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.0191, train/loss_step=1.000, global_step=51.00] Epoch 0:  22%|██▏       | 53/240 [01:06<03:55,  1.26s/it, loss=1.02, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.0191, train/loss_step=1.000, global_step=51.00]Epoch 0:  22%|██▏       | 53/240 [01:07<03:56,  1.26s/it, loss=1.01, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00837, train/loss_step=1.000, global_step=52.00]Epoch 0:  22%|██▎       | 54/240 [01:07<03:53,  1.26s/it, loss=1.01, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00837, train/loss_step=1.000, global_step=52.00]Epoch 0:  22%|██▎       | 54/240 [01:08<03:54,  1.26s/it, loss=1.01, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00633, train/loss_step=1.000, global_step=53.00]Epoch 0:  23%|██▎       | 55/240 [01:08<03:51,  1.25s/it, loss=1.01, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00633, train/loss_step=1.000, global_step=53.00]Epoch 0:  23%|██▎       | 55/240 [01:09<03:52,  1.25s/it, loss=1.01, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.0242, train/loss_step=1.010, global_step=54.00] Epoch 0:  23%|██▎       | 56/240 [01:09<03:49,  1.25s/it, loss=1.01, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.0242, train/loss_step=1.010, global_step=54.00]Epoch 0:  23%|██▎       | 56/240 [01:10<03:50,  1.25s/it, loss=1.01, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00668, train/loss_step=1.000, global_step=55.00]